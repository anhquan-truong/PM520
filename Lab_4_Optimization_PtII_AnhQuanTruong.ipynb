{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhquan-truong/PM520/blob/main/Lab_4_Optimization_PtII_AnhQuanTruong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7LCHBjOhm0i"
      },
      "source": [
        "# Ain't no mountain high enough, or: Optimization Pt II\n",
        "Outline for today:\n",
        "1. Newton's Method & Quasi-Newton Methods\n",
        "2. Poisson Regression Lab\n",
        "3. Automatic differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-mle-optimization-pt2"
      },
      "source": [
        "Before we _climb_ into second-order methods, keep the inferential target in view. We assume some parametric model\n",
        "$\n",
        "x_1,\\dots,x_n \\sim p(\\cdot \\mid \\theta),\n",
        "$\n",
        "and estimate $\\theta$ via [maximum likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation):\n",
        "$$\n",
        "\\hat{\\theta}_{\\mathrm{MLE}} \\in \\arg\\max_{\\theta\\in\\Theta} \\ell(\\theta \\mid x_{1:n}),\n",
        "$$\n",
        "where $\\ell(\\theta \\mid x_{1:n})$ is the log-likelihood of the data. Last lecture, when closed-form solutions don't exist, we used [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent), an iterative procedure which only uses first-order (i.e. the gradient) information to improve upon our initial guess for $\\theta$.\n",
        "\n",
        "Today we'll cover a class of approaches that move beyond only first-order information to improve the [convergence rate](https://en.wikipedia.org/wiki/Rate_of_convergence) (roughly can think of this as the number of iterations needed to stop inference)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b5I2Q37z4_1"
      },
      "source": [
        "## Newton's Method for Optimization\n",
        "Let $f(\\beta)$ be the function we wish to optimize (e.g., log likelihood, a loss function, etc). Can we do better than gradient descent, by considering higher-order information (ie geometry) of the function $f$? Here, \"better\" is wrt convergence rate.\n",
        "\n",
        "Let's consider a 2nd-order [Taylor-series approximation](https://en.wikipedia.org/wiki/Taylor_series) to $f$ around $\\beta_t$ as,\n",
        "\n",
        "$$f(\\beta) \\approx f(\\beta_t) + \\nabla f(\\beta_t)^T (\\beta - \\beta_t) + \\frac{1}{2} (\\beta - \\beta_t)^T H(\\beta_t)(\\beta - \\beta_t),$$ where $H(\\beta_t) = \\nabla^2 f(\\beta_t)$ (i.e. the [Hessian](https://en.wikipedia.org/wiki/Hessian_matrix) of $f$ at $\\beta_t$). If we minimize this _local_ approximation, we see\n",
        "\n",
        "$\\nabla_\\beta f(\\beta) \\approx \\nabla f(\\beta_t) + H(\\beta_t)(\\beta - \\beta_t) = \\nabla f(\\beta_t) + H(\\beta_t)\\beta - H(\\beta_t)\\beta_t ⇒$\n",
        "$$ H(\\beta_t)\\beta = H(\\beta_t)\\beta_t - \\nabla f(\\beta_t).$$\n",
        "\n",
        "We can recognize that this is a [system of linear equations](https://en.wikipedia.org/wiki/System_of_linear_equations) $A x = b$ where $A = H(\\beta_t)$, $x = \\beta$, and $b = H(\\beta_t)\\beta_t - \\nabla f(\\beta_t)$. The solution is given by, $\\hat{x} = A^{-1}b$, which in this case implies,\n",
        "$$ \\beta = H(\\beta_t)^{-1}\\left(H(\\beta_t)\\beta_t - \\nabla f(\\beta_t)\\right) = \\beta_t - H(\\beta_t)^{-1}\\nabla f(\\beta_t).$$\n",
        "\n",
        "\n",
        "### Caveats\n",
        "[Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is only guaranteed to converge _locally_ for convex/concave functions, and can diverge for some [convex functions](https://en.wikipedia.org/wiki/Convex_function) (e.g., $f(\\beta) = \\sqrt{\\beta^2 + 1}$). To address this limitation, we can add a damping parameter, $\\rho_t \\in (0,1]$, which gives us,\n",
        "$$ \\beta_{t+1} = \\beta_t - \\rho_t H(\\beta_t)^{-1}\\nabla f(\\beta_t).$$\n",
        "\n",
        "## Quasi-Newton Methods for Optimization\n",
        "What if computing $H(\\beta_t)$ is prohibitive or too costly? Do we need _exact_ second order information to improve on gradient descent's convergence? Given an approximation of $H$, called $B$, i.e. $B(\\beta_t) \\approx H(\\beta_t)$, [_quasi_-Newton methods](https://en.wikipedia.org/wiki/Quasi-Newton_method) optimize for the form\n",
        "$$f(\\beta) \\approx f(\\beta_t) + \\nabla f(\\beta_t)^T (\\beta - \\beta_t) + \\frac{1}{2} (\\beta - \\beta_t)^T B(\\beta_t)(\\beta - \\beta_t),$$ where $B(\\beta_t) \\approx H(\\beta_t)$. Optimizing this statement gives us our update rule,\n",
        "$$ \\beta_{t+1} = \\beta_t - \\rho_t B(\\beta_t)^{-1}\\nabla f(\\beta_t).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crQSiZATzm9D"
      },
      "source": [
        "## Poisson Regression\n",
        "Assume $y_i | x_i \\sim \\text{Poi}(\\lambda_i)$ where $\\lambda_i := \\exp(x_i^T \\beta)$, and $\\text{Poi}(k | \\lambda) := \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$ is the [PMF](https://en.wikipedia.org/wiki/Probability_mass_function) of the [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution). Given $\\{(y_i, x_i)\\}_{i=1}^n$, we would like to identify the [maximum likelihood parameter estimate](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) for $\\beta$. In other words, we would to find a value for $\\beta$ such that we maximize the log-likelihood given by,\n",
        "$$\\begin{align*}\n",
        "\\ell(\\beta) &= \\sum_i \\log \\text{Poi}(y_i | \\exp(x_i^T \\beta)) \\\\\n",
        "&= \\sum_i \\log \\left[ \\frac{\\exp(y_i \\cdot x_i^T \\beta) \\exp(-\\exp(x_i^T \\beta))}{y_i!} \\right] \\\\\n",
        "&= \\sum_i \\log \\left[ \\frac{\\exp(y_i \\cdot x_i^T \\beta - \\exp(x_i^T \\beta))}{y_i!} \\right] \\\\\n",
        "&= \\sum_i \\log \\left[\\exp(y_i \\cdot x_i^T \\beta - \\exp(x_i^T \\beta))\\right] - \\log(y_i!) \\\\\n",
        "&= \\sum_i \\left[y_i \\cdot x_i^T \\beta - \\exp(x_i^T \\beta) - \\log(y_i!)\\right] \\\\\n",
        "&= y^T X\\beta - \\exp(X\\beta)^T 1_n - O(1) \\\\\n",
        "&= y^T X\\beta - \\lambda^T 1_n - O(1),\n",
        "\\end{align*}$$\n",
        "where $\\lambda = \\{\\lambda_1, \\dotsc, \\lambda_n\\}.$\n",
        "\n",
        "\n",
        "$$ \\begin{align*}\n",
        "\\nabla_\\beta \\ell &= \\nabla_\\beta \\left[ y^T X\\beta - \\lambda^T 1_n \\right] \\\\\n",
        "&= \\nabla_\\beta [ y^T X\\beta ] - \\nabla_\\beta [\\lambda^T 1_n] \\\\\n",
        "&= \\nabla_\\beta [ y^T X\\beta ] - \\nabla_\\beta [\\exp(X\\beta)^T 1_n] \\\\\n",
        "&= X^T y - X^T \\exp(X\\beta)  \\\\\n",
        "&= X^T y - X^T \\lambda  \\\\\n",
        "&= X^T(y - \\lambda) \\\\\n",
        "\\nabla^2_{\\beta \\beta} \\ell &= \\nabla_{\\beta} X^T(y - \\lambda) \\\\\n",
        "&= \\nabla_{\\beta} \\left[X^T y - X^T \\lambda \\right] \\\\\n",
        "&= - X^T \\nabla_{\\beta}  \\lambda \\\\\n",
        "&= -X^T \\nabla_{\\beta}  \\exp(X\\beta) \\\\\n",
        "&= -X^T \\Lambda X,\n",
        "\\end{align*}$$\n",
        "where $\\Lambda = \\text{diag}(\\lambda)$, i.e. $\\Lambda_{ii} = \\lambda_i$ and $\\Lambda_{ij} = 0$ for $i \\neq j$.\n",
        "\n",
        "To illustrate how $\\nabla_{\\beta}  \\exp(X\\beta) = \\Lambda X$ (i.e. last step in Hessian calculation), recall that the [Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) of a function $f : \\mathbb{R}^n → \\mathbb{R}^m$ is the $m \\times n$ matrix $J$ such that $J_{ij} = \\frac{∂f_i}{∂j}$. In this case we are computing the Jacobian for $\\exp(X\\beta)$, which is $\\mathbb{R}^p → \\mathbb{R}^n$, so our final Jacobian for $\\exp(X\\beta)$ should have shape $n \\times p$. Notice that $J_{i,j} = \\frac{\\partial}{\\partial \\beta_j} \\exp(x_i^T \\beta) = x_{ij}\\exp(x_i^T \\beta)$, thus $J_{i, .} = \\exp(x_i^T \\beta) x_i^T$. Repeating this for each $i$ we have $$∇_\\beta \\exp(X \\beta) = J(\\exp(X \\beta)) = \\begin{bmatrix} J_{1,.} \\\\ ⋮ \\\\ J_{n,.} \\end{bmatrix} =\n",
        "\\begin{bmatrix} \\exp(x_1^T \\beta) x_1^T \\\\ ⋮ \\\\ \\exp(x_n^T \\beta) x_n^T \\end{bmatrix}  =\n",
        "\\begin{bmatrix} \\lambda_1 x_1^T \\\\ ⋮ \\\\ \\lambda_n x_n^T\\end{bmatrix} = \\Lambda X.$$\n",
        "\n",
        "We can fit using Newton's method. =>\n",
        "$$\\begin{align*}\n",
        "\\beta_{t+1} &= \\beta_t - H(\\beta_t)^{-1}\\nabla \\ell(\\beta_t) \\\\\n",
        "&= \\beta_t + (X^T \\Lambda_t X)^{-1} X^T (y - \\lambda_t) ⇒ \\\\\n",
        "&= (X^T \\Lambda_t X)^{-1} X^T \\Lambda_t X\\beta_t + (X^T \\Lambda_t X)^{-1} X^T (y - \\lambda_t)\\\\\n",
        "&= (X^T \\Lambda_t X)^{-1} X^T \\Lambda_t X\\beta_t + (X^T \\Lambda_t X)^{-1} X^T \\Lambda_t\\Lambda_t^{-1}(y - \\lambda_t)\\\\\n",
        "&= (X^T \\Lambda_t X)^{-1} X^T \\Lambda_t X\\beta_t + (X^T \\Lambda_t X)^{-1} X^T \\Lambda_t(\\Lambda_t^{-1}y - \\Lambda_t^{-1}\\lambda_t)\\\\\n",
        "&= (X^T \\Lambda_t X)^{-1} X^T \\Lambda_t X\\beta_t + (X^T \\Lambda_t X)^{-1} X^T \\Lambda_t(\\Lambda_t^{-1}y - 1_n)\\\\\n",
        "&= (X^T \\Lambda_t X)^{-1} X^T \\Lambda_t (\\Lambda_t^{-1}y + X\\beta_t - 1_n)\n",
        "\\end{align*}$$\n",
        "where $\\Lambda_t := \\text{diag}(\\lambda_1, \\dotsc, \\lambda_n)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "NrxzG7-0r2x_",
        "outputId": "ac7b9988-42f1-43f5-df39-dfee421c08d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lineax in /usr/local/lib/python3.12/dist-packages (0.1.0)\n",
            "Requirement already satisfied: equinox>=0.11.10 in /usr/local/lib/python3.12/dist-packages (from lineax) (0.13.4)\n",
            "Requirement already satisfied: jax>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from lineax) (0.7.2)\n",
            "Requirement already satisfied: jaxtyping>=0.2.24 in /usr/local/lib/python3.12/dist-packages (from lineax) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from lineax) (4.15.0)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from equinox>=0.11.10->lineax) (0.1.7)\n",
            "Requirement already satisfied: jaxlib<=0.7.2,>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.1->lineax) (0.7.2)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.1->lineax) (0.5.4)\n",
            "Requirement already satisfied: numpy>=2.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.1->lineax) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.1->lineax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.1->lineax) (1.16.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install lineax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "DKK1oqZMztkU",
        "outputId": "8f949e72-f8d3-4ab8-e8b6-7bf56d7d708c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:11: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:28: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:58: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:11: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:28: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:58: SyntaxWarning: invalid escape sequence '\\e'\n",
            "/tmp/ipython-input-1615811953.py:11: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  Our loglikelihood function for $y_i | x_i ~ \\text{Poi}(\\exp(eta_i))$.\n",
            "/tmp/ipython-input-1615811953.py:28: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  $y_i | x_i ~ \\text{Poi}(\\exp(x_i^T \\beta))$.\n",
            "/tmp/ipython-input-1615811953.py:58: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  $y_i | x_i ~ \\text{Poi}(\\exp(x_i^T \\beta))$.\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as rdm\n",
        "import jax.scipy.stats as stats\n",
        "\n",
        "import lineax as lx\n",
        "\n",
        "@jax.jit\n",
        "def loglikelihood(beta, y, X):\n",
        "  \"\"\"\n",
        "  Our loglikelihood function for $y_i | x_i ~ \\text{Poi}(\\exp(eta_i))$.\n",
        "\n",
        "  beta: beta\n",
        "  y: poisson-distributed observations\n",
        "  X: our design matrix as lx.MatrixLinearOperator\n",
        "\n",
        "  returns: sum of the logliklihoods of each sample\n",
        "  \"\"\"\n",
        "  rate = jnp.exp(X.mv(beta))\n",
        "  return jnp.sum(stats.poisson.logpmf(y, rate))\n",
        "\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def irwls_fit(beta, y, X, step_size):\n",
        "  \"\"\"\n",
        "  Perform MLE estimation for $\\beta$ under the model\n",
        "     $y_i | x_i ~ \\text{Poi}(\\exp(x_i^T \\beta))$.\n",
        "\n",
        "  beta: beta\n",
        "  y: poisson-distributed observations\n",
        "  X: our design matrix as lx.MatrixLinearOperator\n",
        "\n",
        "  returns: updated estimate of $\\beta$\n",
        "  \"\"\"\n",
        "  # compute lambda_i := exp(x_i @ beta)\n",
        "  eta = X.mv(beta)\n",
        "  d_i = jnp.exp(eta)\n",
        "  d_sqrt = jnp.sqrt(d_i)\n",
        "\n",
        "  # compute z_i := Lambda^{1/2}(Lambda^-1 y + X @beta - 1)\n",
        "  z = (y / d_i + eta - 1) * d_sqrt\n",
        "\n",
        "  # X* := Lambda^{1/2} X\n",
        "  # we use linear operators to postpone any computation\n",
        "  X_star = lx.DiagonalLinearOperator(d_sqrt) @ X\n",
        "\n",
        "  # lineax can solve normal equations iteratively as (t(X*) @ (X* @ guess)) - z\n",
        "  solution = lx.linear_solve(X_star, z, solver=lx.Normal(lx.CG(atol=1e-4, rtol=1e-3)))\n",
        "  beta = solution.value\n",
        "\n",
        "  return beta\n",
        "\n",
        "\n",
        "def poiss_reg(y, X, fit_func, step_size = 1.0, max_iter=100, tol=1e-3):\n",
        "  \"\"\"\n",
        "  Perform MLE estimation for $\\beta$ under the model\n",
        "     $y_i | x_i ~ \\text{Poi}(\\exp(x_i^T \\beta))$.\n",
        "\n",
        "  y: poisson-distributed observations\n",
        "  X: our design matrix\n",
        "  max_iter: the maximum number of iterations to perform optimization\n",
        "  tol:\n",
        "\n",
        "  returns: updated estimate of $\\beta$\n",
        "  \"\"\"\n",
        "  # intialize eta := X @ beta\n",
        "  n, p = X.shape\n",
        "\n",
        "  # fake bookkeeping\n",
        "  loglike = -100000\n",
        "  delta = 10000\n",
        "\n",
        "  # convert to a linear operator for lineax\n",
        "  X = lx.MatrixLinearOperator(X)\n",
        "\n",
        "  # initialize using OLS estimate and normalizing for downstream stability\n",
        "  sol = lx.linear_solve(X, (y - jnp.mean(y))/2, solver=lx.Normal(lx.CG(atol=1e-4, rtol=1e-3)))\n",
        "  beta = sol.value\n",
        "  beta = beta / jnp.linalg.norm(beta)\n",
        "\n",
        "  for epoch in range(max_iter):\n",
        "\n",
        "    # fit using our function\n",
        "    beta = fit_func(beta, y, X, step_size)\n",
        "\n",
        "    # evaluate log likelihood\n",
        "    newll = loglikelihood(beta, y, X)\n",
        "\n",
        "    # take delta and check if we can stop\n",
        "    delta = jnp.fabs(newll - loglike)\n",
        "    print(f\"Log likelihood[{epoch}] = {newll}\")\n",
        "    if delta < tol:\n",
        "      break\n",
        "\n",
        "    # replace old value\n",
        "    loglike = newll\n",
        "\n",
        "  return beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "nRai4qWiz1_R",
        "outputId": "6bb0cb12-3258-4fc3-ee51-5aaadd359793",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log likelihood[0] = -39173304.0\n",
            "Log likelihood[1] = -14537620.0\n",
            "Log likelihood[2] = -5402229.5\n",
            "Log likelihood[3] = -2006548.625\n",
            "Log likelihood[4] = -741087.25\n",
            "Log likelihood[5] = -268893.0\n",
            "Log likelihood[6] = -93487.3125\n",
            "Log likelihood[7] = -29938.27734375\n",
            "Log likelihood[8] = -8535.572265625\n",
            "Log likelihood[9] = -2576.5654296875\n",
            "Log likelihood[10] = -1502.05126953125\n",
            "Log likelihood[11] = -1427.998779296875\n",
            "Log likelihood[12] = -1427.3929443359375\n",
            "Log likelihood[13] = -1427.393310546875\n",
            "beta = [ 1.2956359   1.3550105  -0.40960556 -0.77188545  0.38094172]\n",
            "hat(beta) = [ 1.2962788   1.3400885  -0.40938386 -0.77929157  0.38347557]\n"
          ]
        }
      ],
      "source": [
        "# Let's simulate a poisson regression model with N samples and P variables\n",
        "# we need X (N,P), beta (P,) and y (N,)\n",
        "N = 1000\n",
        "P = 5\n",
        "\n",
        "# initialize PRNG env\n",
        "seed = 0\n",
        "key = rdm.PRNGKey(seed)\n",
        "\n",
        "# TODO: split key for each random call\n",
        "key, x_key, b_key, y_key = rdm.split(key, 4)\n",
        "X = rdm.normal(x_key, shape=(N, P))\n",
        "beta = rdm.normal(b_key, shape=(P,))\n",
        "\n",
        "\n",
        "# TODO: compute lambda_i = exp(x_i' \\beta)\n",
        "lam = jnp.exp(X @ beta)\n",
        "\n",
        "\n",
        "# TODO: sample y from Poi(lambda_i)\n",
        "y = rdm.poisson(y_key, lam)\n",
        "\n",
        "# estimate beta using our irwls function\n",
        "# fit_func has signature (eta, y, X, step_size)\n",
        "beta_hat = poiss_reg(y, X, irwls_fit) # 2nd order methods\n",
        "print(f\"beta = {beta}\")\n",
        "print(f\"hat(beta) = {beta_hat}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "ZalxS2NOOfiV",
        "outputId": "7b5a8bec-afe6-4b7c-cd0f-1b79c0df4165",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log likelihood[0] = -10699.7275390625\n",
            "Log likelihood[1] = -8428.5107421875\n",
            "Log likelihood[2] = -6478.630859375\n",
            "Log likelihood[3] = -4908.4990234375\n",
            "Log likelihood[4] = -3744.06103515625\n",
            "Log likelihood[5] = -2956.5849609375\n",
            "Log likelihood[6] = -2464.587646484375\n",
            "Log likelihood[7] = -2166.8525390625\n",
            "Log likelihood[8] = -1980.2850341796875\n",
            "Log likelihood[9] = -1854.0355224609375\n",
            "Log likelihood[10] = -1762.221435546875\n",
            "Log likelihood[11] = -1692.367919921875\n",
            "Log likelihood[12] = -1637.998291015625\n",
            "Log likelihood[13] = -1595.2332763671875\n",
            "Log likelihood[14] = -1561.4241943359375\n",
            "Log likelihood[15] = -1534.61962890625\n",
            "Log likelihood[16] = -1513.324462890625\n",
            "Log likelihood[17] = -1496.37548828125\n",
            "Log likelihood[18] = -1482.861083984375\n",
            "Log likelihood[19] = -1472.06689453125\n",
            "Log likelihood[20] = -1463.42919921875\n",
            "Log likelihood[21] = -1456.504638671875\n",
            "Log likelihood[22] = -1450.94384765625\n",
            "Log likelihood[23] = -1446.469970703125\n",
            "Log likelihood[24] = -1442.864990234375\n",
            "Log likelihood[25] = -1439.955078125\n",
            "Log likelihood[26] = -1437.603515625\n",
            "Log likelihood[27] = -1435.699951171875\n",
            "Log likelihood[28] = -1434.156982421875\n",
            "Log likelihood[29] = -1432.905029296875\n",
            "Log likelihood[30] = -1431.887939453125\n",
            "Log likelihood[31] = -1431.06103515625\n",
            "Log likelihood[32] = -1430.387939453125\n",
            "Log likelihood[33] = -1429.8399658203125\n",
            "Log likelihood[34] = -1429.3929443359375\n",
            "Log likelihood[35] = -1429.028564453125\n",
            "Log likelihood[36] = -1428.73095703125\n",
            "Log likelihood[37] = -1428.4876708984375\n",
            "Log likelihood[38] = -1428.289306640625\n",
            "Log likelihood[39] = -1428.1268310546875\n",
            "Log likelihood[40] = -1427.994140625\n",
            "Log likelihood[41] = -1427.885498046875\n",
            "Log likelihood[42] = -1427.7967529296875\n",
            "Log likelihood[43] = -1427.7237548828125\n",
            "Log likelihood[44] = -1427.66455078125\n",
            "Log likelihood[45] = -1427.615966796875\n",
            "Log likelihood[46] = -1427.5760498046875\n",
            "Log likelihood[47] = -1427.542724609375\n",
            "Log likelihood[48] = -1427.515869140625\n",
            "Log likelihood[49] = -1427.4940185546875\n",
            "Log likelihood[50] = -1427.476318359375\n",
            "Log likelihood[51] = -1427.46142578125\n",
            "Log likelihood[52] = -1427.44921875\n",
            "Log likelihood[53] = -1427.439453125\n",
            "Log likelihood[54] = -1427.43115234375\n",
            "Log likelihood[55] = -1427.423828125\n",
            "Log likelihood[56] = -1427.419189453125\n",
            "Log likelihood[57] = -1427.4141845703125\n",
            "Log likelihood[58] = -1427.4105224609375\n",
            "Log likelihood[59] = -1427.407470703125\n",
            "Log likelihood[60] = -1427.405029296875\n",
            "Log likelihood[61] = -1427.402587890625\n",
            "Log likelihood[62] = -1427.4013671875\n",
            "Log likelihood[63] = -1427.399658203125\n",
            "Log likelihood[64] = -1427.3984375\n",
            "Log likelihood[65] = -1427.39794921875\n",
            "beta = [ 1.2956359   1.3550105  -0.40960556 -0.77188545  0.38094172]\n",
            "hat(beta) = [ 1.2965652   1.3394053  -0.40923366 -0.7797785   0.38307476]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:6: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:6: SyntaxWarning: invalid escape sequence '\\e'\n",
            "/tmp/ipython-input-3621341321.py:6: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  $y_i | x_i ~ \\text{Poi}(\\exp(x_i^T \\beta))$.\n"
          ]
        }
      ],
      "source": [
        "# let's implement poisson regression using _only_ gradient information to perform inference\n",
        "# and measure how quickly it converges compared with the Newton method\n",
        "def grad_fit(beta, y, X, step_size):\n",
        "  \"\"\"\n",
        "  Update MLE estimate $\\beta$ under the model\n",
        "     $y_i | x_i ~ \\text{Poi}(\\exp(x_i^T \\beta))$.\n",
        "\n",
        "  Should perform a gradient update step.\n",
        "\n",
        "  beta: beta\n",
        "  y: poisson-distributed observations\n",
        "  X: our design matrix as lx.MatrixLinearOperator\n",
        "\n",
        "  returns: updated estimate of $\\beta$\n",
        "  \"\"\"\n",
        "  pass\n",
        "  eta = X.mv(beta)\n",
        "  d_i = jnp.exp(eta)\n",
        "  return beta + step_size * X.transpose().mv(y - d_i)\n",
        "# this is gradient asse\n",
        "\n",
        "\n",
        "# NB: we can transpose a lx.MatrixLinearOperator (say X) as X.transpose()\n",
        "# NB: we compute matrix-vector produces using a lx.MatrixLinearOperator as X.mv(v)\n",
        "step_size = 1e-5\n",
        "beta_hat = poiss_reg(y, X, grad_fit, step_size, max_iter=1000)\n",
        "print(f\"beta = {beta}\")\n",
        "print(f\"hat(beta) = {beta_hat}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbJ1nlWE0R7T"
      },
      "source": [
        "## Automatic differentiation\n",
        "Automatic differentiation (AD) applies the chain rule to the exact computational graph of a function, so we get derivatives of implemented code without hand-deriving every intermediate expression. For the Poisson objective,\n",
        "$$\n",
        "\\ell(\\beta)=\\sum_{i=1}^n \\left[y_i x_i^T\\beta-\\exp(x_i^T\\beta)-\\log(y_i!)\\right],\n",
        "$$\n",
        "AD gives gradient and Hessian operators directly:\n",
        "$$\n",
        "\\nabla_\\beta \\ell(\\beta)=\\mathrm{AD}(\\ell)(\\beta), \\qquad \\nabla_{\\beta\\beta}^2 \\ell(\\beta)=\\nabla_\\beta[\\mathrm{AD}(\\ell)](\\beta).\n",
        "$$\n",
        "In JAX this corresponds to `jax.grad` for first derivatives and `jax.hessian` (or nested `jax.grad`) for second-order structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSPl_smqUq9y"
      },
      "outputs": [],
      "source": [
        "# let's not worry and use autodiff\n",
        "auto_grad_ll = jax.grad(loglikelihood)\n",
        "\n",
        "def jax_grad_step(beta, y, X, step_size):\n",
        "  pass\n",
        "\n",
        "# NB: we can transpose a lx.MatrixLinearOperator (say X) as X.transpose()\n",
        "# NB: we compute matrix-vector produces using a lx.MatrixLinearOperator as X.mv(v)\n",
        "step_size = 1e-6\n",
        "beta_hat = poiss_reg(y, X, jax_grad_step, step_size, max_iter=1000)\n",
        "print(f\"beta = {beta}\")\n",
        "print(f\"hat(beta) = {beta_hat}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtbqZkAlWPuv"
      },
      "outputs": [],
      "source": [
        "import jax.scipy.linalg as spla\n",
        "\n",
        "# Great! But can we use 2nd order information?\n",
        "auto_hess_ll = jax.hessian(loglikelihood)\n",
        "\n",
        "def jax_newton_step(beta, y, X, step_size):\n",
        "  pass\n",
        "\n",
        "step_size = 1.\n",
        "beta_hat = poiss_reg(y, X, jax_newton_step, step_size, max_iter=1000)\n",
        "print(f\"beta = {beta}\")\n",
        "print(f\"hat(beta) = {beta_hat}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}