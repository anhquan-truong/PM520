{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhquan-truong/PM520/blob/main/Lab_3_Optimization_PtI_AnhQuanTruong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Move on Up, or: Maximum likelihood Estimation & Optimization Pt I\n",
        "\n",
        "TBD: move notes from slides to here.\n"
      ],
      "metadata": {
        "id": "q1bg914lpQpN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VAK5-ADoNq_I"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as rdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLE for iid Normal data\n",
        "Let $x_1, \\dotsc, x_n \\overset{\\mathrm{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2)$ where $\\mathcal{N}(\\mu, \\sigma^2)$ refers to the [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) with mean parameter $\\mu$ and variance parameter $\\sigma^2$. The likelihood of our data is given by,\n",
        "$$\\begin{align*}\n",
        "\\mathcal{L}(\\mu, \\sigma^2 | x_1, \\dots, x_n) &=\n",
        "  \\prod_{i=1}^n \\mathcal{N}(x_i | \\mu, \\sigma^2) \\\\\n",
        "  &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\\\\\n",
        "  &= \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\\right).\n",
        "\\end{align*}\\\\\n",
        "$$\n",
        "Thus, our _log_-likelihood is given by,\n",
        "The likelihood of our data is given by,\n",
        "(maximize the log mean maximize the LLH function because of monotone\n",
        "$$\\begin{align*}\n",
        "\\ell(\\mu, \\sigma^2 | x_1, \\dots, x_n) &=\n",
        "  \\log \\left[\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\\right)\\right]\\\\\n",
        "  &= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2.\n",
        "\\end{align*}\\\\\n",
        "$$"
      ],
      "metadata": {
        "id": "MbaYn27uwi28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm_rv(key, n: int, mu: float, sigma_sq: float):\n",
        "  r\"\"\"\n",
        "  Samples $n$ observations from $x_i \\sim N(\\mu, \\sigma^2)$\n",
        "\n",
        "  n: the number of observations\n",
        "  mu: the mean parameter\n",
        "  sigma_sq: the variance parameter\n",
        "\n",
        "  returns: x, Array of observations\n",
        "  \"\"\"\n",
        "  x = mu + jnp.sqrt(sigma_sq) * rdm.normal(key, shape=(n,))\n",
        "  return x\n",
        "\n",
        "\n",
        "def norm_mle(x):\n",
        "  r\"\"\"\n",
        "  Computes $\\hat{\\mu}_{MLE}$ and $\\hat{\\sigma^2}_{MLE}$.\n",
        "\n",
        "  x: Array of observations\n",
        "\n",
        "  returns:  Tuple of $\\hat{\\mu}_{MLE}$ and $\\hat{\\sigma^2}_{MLE}$.\n",
        "  \"\"\"\n",
        "  mu_hat = jnp.mean(x)\n",
        "  # mu_hat = jnp.sum(x) / len(x)\n",
        "  #ssq_hat = jnp.mean(jnp.sum(x - mu_hat)**2)\n",
        "  ssq_hat = jnp.var(x)\n",
        "\n",
        "  return mu_hat, ssq_hat\n",
        "\n",
        "seed = 0\n",
        "key = rdm.PRNGKey(seed)\n",
        "key, x_key = rdm.split(key)\n",
        "\n",
        "N = 1000 # how many obs we want\n",
        "\n",
        "mu = 58.\n",
        "sigma_sq = 100.\n",
        "x = norm_rv(x_key, N, mu, sigma_sq)\n",
        "#print(f\"x = {x}\")\n",
        "mu_hat, ssq_hat = norm_mle(x)\n",
        "print(fr\"MLE[\\mu, \\sigma^2] = {mu_hat}, {ssq_hat}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWoIiwnVwn6O",
        "outputId": "5de7dd3f-e610-4d98-e554-16fc75c666ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLE[\\mu, \\sigma^2] = 57.703948974609375, 110.18893432617188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sq_diff(param, estimate): #square difference - l2norm\n",
        "  return (param - estimate) ** 2\n",
        "\n",
        "mu = 58.\n",
        "sigma_sq = 10.\n",
        "for N in [50, 100, 1000, 10000]:\n",
        "  key, x_key = rdm.split(key)\n",
        "  # generate N observations\n",
        "  x_n = norm_rv(x_key, N, mu, sigma_sq)\n",
        "  # estimate mu, and sigma_sq\n",
        "  mu_hat, ssq_hat = norm_mle(x_n)\n",
        "  # compute the sq-diff for both and report\n",
        "  mu_err = sq_diff(mu, mu_hat)\n",
        "  ssq_err = sq_diff(sigma_sq, ssq_hat)\n",
        "  print(f\"MSE[{N} | mu, sigma^2] = {mu_err}, {ssq_err}\")\n",
        "\n",
        "  # the error is descreasing as a function of N"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z8awgX6GXXv",
        "outputId": "a0efcfac-bb4b-4f6c-d269-cf4b82086bf9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE[50 | mu, sigma^2] = 0.5217084288597107, 3.3595786094665527\n",
            "MSE[100 | mu, sigma^2] = 0.1484147012233734, 1.515868067741394\n",
            "MSE[1000 | mu, sigma^2] = 0.0020461762323975563, 0.027577972039580345\n",
            "MSE[10000 | mu, sigma^2] = 0.000588434049859643, 0.003656691173091531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLE for iid Exponential data\n",
        "TBD: Add notes for Exponential PDF and MLE estimator"
      ],
      "metadata": {
        "id": "RTHf96slw__S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exp_rv(key, n: int, rate: float):\n",
        "  \"\"\"\n",
        "  Samples $n$ observations from $x_i \\sim Exp(\\lambda)$\n",
        "\n",
        "  n: the number of observations\n",
        "  rate: the $\\lambda$ parameter\n",
        "\n",
        "  returns: x, Array of observations\n",
        "  \"\"\"\n",
        "  mean = 1 / rate\n",
        "  x = mean * rdm.exponential(key, shape=(n,))\n",
        "  return x\n",
        "\n",
        "\n",
        "def exp_mle(x):\n",
        "  \"\"\"\n",
        "  Computes $\\hat{\\lambda}_{MLE}$.\n",
        "\n",
        "  x: Array of observations\n",
        "\n",
        "  returns: $\\hat{\\lambda}_{MLE}$.\n",
        "  \"\"\"\n",
        "  rate_hat = 1. / jnp.mean(x)\n",
        "  return rate_hat\n",
        "\n",
        "key, x_key = rdm.split(key)\n",
        "N = 100\n",
        "rate = 1 / 500.\n",
        "x = exp_rv(x_key, N, rate)\n",
        "print(f\"x = {x}\")\n",
        "rate_hat = exp_mle(x)\n",
        "print(f\"MLE[\\lambda = {rate}] = {rate_hat}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3kQz53WA8YS",
        "outputId": "836e95c2-4a66-4443-9a82-22523096c59a",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:17: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:32: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:17: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:32: SyntaxWarning: invalid escape sequence '\\l'\n",
            "/tmp/ipython-input-2234527571.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  Samples $n$ observations from $x_i \\sim Exp(\\lambda)$\n",
            "/tmp/ipython-input-2234527571.py:17: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  Computes $\\hat{\\lambda}_{MLE}$.\n",
            "/tmp/ipython-input-2234527571.py:32: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  print(f\"MLE[\\lambda = {rate}] = {rate_hat}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = [8.5540985e+02 2.5296268e+02 6.9233221e+02 7.4306616e+02 2.0023449e+01\n",
            " 3.0325012e+02 4.9858099e+02 4.4559082e+02 2.2487761e+02 1.7512365e+01\n",
            " 1.8983838e+02 7.1858414e+01 5.3631573e+00 1.2744092e+02 4.3240015e+02\n",
            " 2.5281116e+02 3.0460519e+02 9.8529327e+02 3.5058794e+03 9.7805145e+02\n",
            " 5.8293109e+02 9.4777515e+02 3.6097495e+02 2.9653098e+02 5.9593036e+02\n",
            " 2.6948223e+00 1.1319576e+02 1.3489890e+01 2.3469034e+02 8.8526413e+01\n",
            " 5.4663782e+02 2.1881448e+02 3.5277534e+01 5.4674384e+02 2.1851385e+01\n",
            " 4.5251007e+02 8.2026758e+02 6.7611023e+01 6.8925110e+01 2.2094575e+03\n",
            " 7.8212195e+02 2.3713325e+03 1.7880037e+01 3.8670126e+02 1.2078967e+03\n",
            " 1.9894528e+02 2.5183937e+02 6.4554199e+02 8.7256927e+01 1.3923959e+03\n",
            " 1.8955074e+02 1.1624373e+03 6.1272675e+02 3.5862617e+01 1.7658134e+02\n",
            " 4.1262427e+02 4.7592819e+02 5.8816719e+01 2.2342406e+02 2.8205704e+02\n",
            " 1.3506290e+03 2.4395990e+02 5.0029422e+02 4.5187828e+01 3.4501726e+03\n",
            " 9.8308083e+01 6.2813635e+02 9.8704968e+02 1.7907946e+03 5.0491989e+02\n",
            " 6.2792657e+02 4.9416483e+02 1.0475122e+03 2.7700937e+02 4.7595087e+02\n",
            " 5.9154523e+02 3.1593936e+02 4.3901651e+02 1.8166298e+03 9.3849976e+01\n",
            " 4.3905609e+02 1.7099644e+02 6.3440790e+02 8.9417584e+02 2.2738796e+01\n",
            " 3.4480920e+02 2.5444427e+02 8.2163538e+02 7.4380914e+02 5.1450897e+02\n",
            " 4.8511685e+02 4.8822217e+02 7.9101028e+01 1.6658121e+03 4.1248688e+02\n",
            " 3.0288719e+01 2.9055600e+02 1.0507615e+02 1.6017800e+03 1.5604137e+02]\n",
            "MLE[\\lambda = 0.002] = 0.0017845045076683164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 1 / 50.\n",
        "for N in [50, 100, 1000, 10000]:\n",
        "  key, x_key = rdm.split(key)\n",
        "  # generate N observations\n",
        "  x_n = exp_rv(x_key, N, rate)\n",
        "  # estimate rate\n",
        "  rate_hat = exp_mle(x_n)\n",
        "  # compute the sq-diff for rate\n",
        "  rate_err = sq_diff(rate, rate_hat)\n",
        "  print(f\"MSE[{N} | \\lambda = {rate}] = {rate_err}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-amDd6YTLuNI",
        "outputId": "5c17897c-4ba8-42e9-8831-424244458156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE[50 | \\lambda = 0.02] = 9.133363164437469e-06\n",
            "MSE[100 | \\lambda = 0.02] = 6.241853043320589e-07\n",
            "MSE[1000 | \\lambda = 0.02] = 2.641813523496239e-07\n",
            "MSE[10000 | \\lambda = 0.02] = 4.547181120528876e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient descent\n",
        "[Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) seeks to iteratively optimize a function $f(\\beta)$ by taking steps in the steepest direction,\n",
        "$$ \\hat{\\beta} = \\beta_t - \\rho_t \\nabla f(\\beta_t),$$\n",
        "where that direction is provided by the [gradient](https://en.wikipedia.org/wiki/Gradient) of (f).\n",
        "\n",
        "A helpful way to recast gradient descent is that we seek to perform a series of _local_ optimizations,\n",
        "\n",
        "$$\\hat{\\beta} = \\min_\\beta \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t}\\|\\beta - \\beta_t\\|_2^2.$$\n",
        "\n",
        "To see how these are equivalent let's solve the local problem. but using inner product notation,\n",
        "$$m(\\beta) = \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t} (\\beta - \\beta_t)^T(\\beta - \\beta_t).$$\n",
        "Now, using calculus again,\n",
        "$$\\begin{align*}\n",
        "\\nabla m(\\beta) &= \\nabla [ \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t} (\\beta - \\beta_t)^T(\\beta - \\beta_t)] \\\\\n",
        "&= \\nabla [\\nabla f(\\beta_t)^T \\beta] + \\frac{1}{2\\rho_t} \\nabla [(\\beta - \\beta_t)^T(\\beta - \\beta_t)] \\\\\n",
        "&= \\nabla f(\\beta_t) + \\frac{1}{\\rho_t}(\\beta - \\beta_t) \\Rightarrow \\\\\n",
        "\\hat{\\beta} &= \\beta_t - \\rho_t \\nabla f(\\beta_t).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Neat! However, notice that the original local objective can be thought of as minimizing the directional derivative, but with a distance penalty, where that distance is defined by the geometry of the parameter space.\n",
        "\n",
        "$$\\hat{\\beta} = \\min_\\beta \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t}\\text{dist}(\\beta, \\beta_t).$$\n",
        "\n",
        "When the natural geometry is $\\mathbb{R}^p$ then $\\text{dist}(\\cdot) = \\| \\cdot \\|_2^2$, however there are many  geometries that can describe the natural parameter space (for future class ðŸ˜‰)"
      ],
      "metadata": {
        "id": "I5mFAyAINs-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sim_linear_reg(key, N, P, r2=0.5):\n",
        "  key, x_key = rdm.split(key)\n",
        "  X = rdm.normal(x_key, shape=(N, P))\n",
        "\n",
        "  key, b_key = rdm.split(key)\n",
        "  beta = rdm.normal(b_key, shape=(P,))\n",
        "\n",
        "  # g = jnp.dot(X, beta)\n",
        "  g = X @ beta\n",
        "  s2g = jnp.var(g)\n",
        "\n",
        "  # back out what s2e is, such that s2g / (s2g + s2e) == h2\n",
        "  s2e = (1 - r2) / r2 * s2g\n",
        "  key, y_key = rdm.split(key)\n",
        "\n",
        "  # add env noise to g, but scale such that var(e) == s2e\n",
        "  y = g + jnp.sqrt(s2e) * rdm.normal(y_key, shape=(N,))\n",
        "  return y, X, beta\n",
        "\n",
        "key, sim_key = rdm.split(key)\n",
        "\n",
        "N = 1000\n",
        "P = 5\n",
        "y, X, beta = sim_linear_reg(sim_key, N, P)\n",
        "\n",
        "def linreg_loss(beta_hat, y, X):\n",
        "  #loss = jnp.square(jnp.linalg.norm(X @ beta_hat - y))\n",
        "  loss = 0.5 * jnp.sum((y - X @ beta_hat)**2)\n",
        "  return loss\n",
        "\n",
        "def gradient(beta_hat, y, X):\n",
        "  y_hat = X @ beta_hat\n",
        "  return  - X.T @ (y-y_hat)\n",
        "  #gradient = 1/2 * ((X.T @ X) @ beta_hat - (X.T @ y_hat))\n",
        "  #return gradient\n",
        "\n",
        "step_size = 1 / N\n",
        "diff = 10.\n",
        "last_loss = 1000.\n",
        "idx = 0\n",
        "beta_hat = jnp.zeros((P,))\n",
        "points = []\n",
        "# while delta in loss is large, continue\n",
        "print(f\"true beta = {beta}\")\n",
        "while jnp.fabs(diff) > 1e-3:\n",
        "\n",
        "  # take a step in the direction of the gradient using step_size\n",
        "  beta_hat = beta_hat - step_size * gradient(beta_hat, y, X)\n",
        "\n",
        "  # update our current loss and compute delta\n",
        "  cur_loss = linreg_loss(beta_hat, y, X)\n",
        "  diff = last_loss - cur_loss\n",
        "  last_loss = cur_loss\n",
        "\n",
        "  # wave to the crowd\n",
        "  print(f\"Loss[{idx}]({beta_hat}) = {last_loss}\")\n",
        "  idx += 1\n",
        "\n",
        "# OLS solution\n",
        "beta_hat_ols = jnp.linalg.solve(X.T @ X, X.T @ y)\n",
        "print(f\"ols beta = {beta_hat_ols}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6LpP-pxNy8y",
        "outputId": "bf034d2b-af40-4cb4-cd41-b6b38b157689"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true beta = [ 0.31772447  0.5006509  -0.1564306   1.3150108   1.0354018 ]\n",
            "Loss[0]([ 0.36728072  0.48481023 -0.21765856  1.3456678   1.0935663 ]) = 1538.877197265625\n",
            "Loss[1]([ 0.36483306  0.49489394 -0.24046142  1.2939041   1.0634594 ]) = 1536.942138671875\n",
            "Loss[2]([ 0.36696675  0.49218005 -0.23600028  1.297412    1.06437   ]) = 1536.9228515625\n",
            "Loss[3]([ 0.3667601   0.49263078 -0.2366552   1.2968911   1.0644364 ]) = 1536.9224853515625\n",
            "ols beta = [ 0.36679658  0.49257872 -0.23656675  1.296956    1.0644212 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key, sim_key = rdm.split(key)\n",
        "\n",
        "N = 1000\n",
        "P = 5\n",
        "y, X, beta = sim_linear_reg(sim_key, N, P)\n",
        "\n",
        "step_size = 1 / N\n",
        "diff = 10.\n",
        "last_loss = 1000.\n",
        "idx = 0\n",
        "beta_hat = jnp.zeros((P,))\n",
        "# while delta in loss is large, continue\n",
        "print(\"Using JAX to compute gradient\")\n",
        "print(f\"true beta = {beta}\")\n",
        "while jnp.fabs(diff) > 1e-3:\n",
        "  # take a step in the direction of the gradient using step_size\n",
        "  jax_gradient = jax.grad(linreg_loss)\n",
        "  vandg = jax.value_and_grad(linreg_loss)\n",
        "  cur_loss, g = vandg(beta_hat, y, X) # like last round loss - computational efficicency\n",
        "  beta_hat = beta_hat - step_size * jax.grad(linreg_loss)(beta_hat, y, X)\n",
        "\n",
        "  # update our current loss and compute delta\n",
        "  cur_loss = linreg_loss(beta_hat, y, X)\n",
        "  diff = -last_loss + cur_loss\n",
        "  last_loss = cur_loss\n",
        "\n",
        "  # wave to the crowd\n",
        "  print(f\"Loss[{idx}]({beta_hat}) = {last_loss}\")\n",
        "  idx += 1\n",
        "\n",
        "# OLS solution\n",
        "beta_hat_ols = jnp.linalg.solve(X.T @ X, X.T @ y)\n",
        "print(f\"ols beta = {beta_hat_ols}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyZh3Msjuncp",
        "outputId": "818c8a29-c55a-40cc-85c8-e20d2eac9d6c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using JAX to compute gradient\n",
            "true beta = [ 0.23336126  0.60773426 -0.15450643  3.041217    0.262714  ]\n",
            "Loss[0]([ 0.2519462   0.6916359  -0.16346951  2.903662    0.03171175]) = 5254.517578125\n",
            "Loss[1]([ 0.22414456  0.61645883 -0.1074512   2.8807147   0.15100583]) = 5242.431640625\n",
            "Loss[2]([ 0.22019097  0.6169246  -0.11199652  2.8890462   0.15287852]) = 5242.3759765625\n",
            "Loss[3]([ 0.21977223  0.6164485  -0.1117428   2.8891547   0.1533    ]) = 5242.37548828125\n",
            "ols beta = [ 0.21971165  0.61643153 -0.11175382  2.8892024   0.15331192]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression\n"
      ],
      "metadata": {
        "id": "qzMgpaYMTagt"
      }
    }
  ]
}