{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhquan-truong/PM520/blob/main/Lab_3_Optimization_PtI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Move on Up, or: Maximum likelihood Estimation & Optimization Pt I\n",
        "\n",
        "TBD: move notes from slides to here.\n"
      ],
      "metadata": {
        "id": "q1bg914lpQpN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VAK5-ADoNq_I"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as rdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLE for iid Normal data\n",
        "Let $x_1, \\dotsc, x_n \\overset{\\mathrm{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2)$ where $\\mathcal{N}(\\mu, \\sigma^2)$ refers to the [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) with mean parameter $\\mu$ and variance parameter $\\sigma^2$. The likelihood of our data is given by,\n",
        "$$\\begin{align*}\n",
        "\\mathcal{L}(\\mu, \\sigma^2 | x_1, \\dots, x_n) &=\n",
        "  \\prod_{i=1}^n \\mathcal{N}(x_i | \\mu, \\sigma^2) \\\\\n",
        "  &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\\\\\n",
        "  &= \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\\right).\n",
        "\\end{align*}\\\\\n",
        "$$\n",
        "Thus, our _log_-likelihood is given by,\n",
        "The likelihood of our data is given by,\n",
        "(maximize the log mean maximize the LLH function because of monotone\n",
        "$$\\begin{align*}\n",
        "\\ell(\\mu, \\sigma^2 | x_1, \\dots, x_n) &=\n",
        "  \\log \\left[\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\\right)\\right]\\\\\n",
        "  &= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2.\n",
        "\\end{align*}\\\\\n",
        "$$"
      ],
      "metadata": {
        "id": "MbaYn27uwi28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm_rv(key, n: int, mu: float, sigma_sq: float):\n",
        "  r\"\"\"\n",
        "  Samples $n$ observations from $x_i \\sim N(\\mu, \\sigma^2)$\n",
        "\n",
        "  n: the number of observations\n",
        "  mu: the mean parameter\n",
        "  sigma_sq: the variance parameter\n",
        "\n",
        "  returns: x, Array of observations\n",
        "  \"\"\"\n",
        "  x = mu + jnp.sqrt(sigma_sq) * rdm.normal(key, shape=(n,))\n",
        "  return x\n",
        "\n",
        "\n",
        "def norm_mle(x):\n",
        "  r\"\"\"\n",
        "  Computes $\\hat{\\mu}_{MLE}$ and $\\hat{\\sigma^2}_{MLE}$.\n",
        "\n",
        "  x: Array of observations\n",
        "\n",
        "  returns:  Tuple of $\\hat{\\mu}_{MLE}$ and $\\hat{\\sigma^2}_{MLE}$.\n",
        "  \"\"\"\n",
        "  mu_hat = jnp.mean(x)\n",
        "  # mu_hat = jnp.sum(x) / len(x)\n",
        "  #ssq_hat = jnp.mean(jnp.sum(x - mu_hat)**2)\n",
        "  ssq_hat = jnp.var(x)\n",
        "\n",
        "  return mu_hat, ssq_hat\n",
        "\n",
        "seed = 0\n",
        "key = rdm.PRNGKey(seed)\n",
        "key, x_key = rdm.split(key)\n",
        "\n",
        "N = 1000 # how many obs we want\n",
        "\n",
        "mu = 58.\n",
        "sigma_sq = 100.\n",
        "x = norm_rv(x_key, N, mu, sigma_sq)\n",
        "#print(f\"x = {x}\")\n",
        "mu_hat, ssq_hat = norm_mle(x)\n",
        "print(fr\"MLE[\\mu, \\sigma^2] = {mu_hat}, {ssq_hat}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWoIiwnVwn6O",
        "outputId": "5de7dd3f-e610-4d98-e554-16fc75c666ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLE[\\mu, \\sigma^2] = 57.703948974609375, 110.18893432617188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sq_diff(param, estimate): #square difference - l2norm\n",
        "  return (param - estimate) ** 2\n",
        "\n",
        "mu = 58.\n",
        "sigma_sq = 10.\n",
        "for N in [50, 100, 1000, 10000]:\n",
        "  key, x_key = rdm.split(key)\n",
        "  # generate N observations\n",
        "  x_n = norm_rv(x_key, N, mu, sigma_sq)\n",
        "  # estimate mu, and sigma_sq\n",
        "  mu_hat, ssq_hat = norm_mle(x_n)\n",
        "  # compute the sq-diff for both and report\n",
        "  mu_err = sq_diff(mu, mu_hat)\n",
        "  ssq_err = sq_diff(sigma_sq, ssq_hat)\n",
        "  print(f\"MSE[{N} | mu, sigma^2] = {mu_err}, {ssq_err}\")\n",
        "\n",
        "  # the error is descreasing as a function of N"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z8awgX6GXXv",
        "outputId": "a0efcfac-bb4b-4f6c-d269-cf4b82086bf9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE[50 | mu, sigma^2] = 0.5217084288597107, 3.3595786094665527\n",
            "MSE[100 | mu, sigma^2] = 0.1484147012233734, 1.515868067741394\n",
            "MSE[1000 | mu, sigma^2] = 0.0020461762323975563, 0.027577972039580345\n",
            "MSE[10000 | mu, sigma^2] = 0.000588434049859643, 0.003656691173091531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLE for iid Exponential data\n",
        "TBD: Add notes for Exponential PDF and MLE estimator"
      ],
      "metadata": {
        "id": "RTHf96slw__S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exp_rv(key, n: int, rate: float):\n",
        "  \"\"\"\n",
        "  Samples $n$ observations from $x_i \\sim Exp(\\lambda)$\n",
        "\n",
        "  n: the number of observations\n",
        "  rate: the $\\lambda$ parameter\n",
        "\n",
        "  returns: x, Array of observations\n",
        "  \"\"\"\n",
        "  mean = 1 / rate\n",
        "  x = mean * rdm.exponential(key, shape=(n,))\n",
        "  return x\n",
        "\n",
        "\n",
        "def exp_mle(x):\n",
        "  \"\"\"\n",
        "  Computes $\\hat{\\lambda}_{MLE}$.\n",
        "\n",
        "  x: Array of observations\n",
        "\n",
        "  returns: $\\hat{\\lambda}_{MLE}$.\n",
        "  \"\"\"\n",
        "  rate_hat = 1. / jnp.mean(x)\n",
        "  return rate_hat\n",
        "\n",
        "key, x_key = rdm.split(key)\n",
        "N = 100\n",
        "rate = 1 / 500.\n",
        "x = exp_rv(x_key, N, rate)\n",
        "print(f\"x = {x}\")\n",
        "rate_hat = exp_mle(x)\n",
        "print(f\"MLE[\\lambda = {rate}] = {rate_hat}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3kQz53WA8YS",
        "outputId": "836e95c2-4a66-4443-9a82-22523096c59a",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:17: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:32: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:17: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:32: SyntaxWarning: invalid escape sequence '\\l'\n",
            "/tmp/ipython-input-2234527571.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  Samples $n$ observations from $x_i \\sim Exp(\\lambda)$\n",
            "/tmp/ipython-input-2234527571.py:17: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  Computes $\\hat{\\lambda}_{MLE}$.\n",
            "/tmp/ipython-input-2234527571.py:32: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  print(f\"MLE[\\lambda = {rate}] = {rate_hat}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = [8.5540985e+02 2.5296268e+02 6.9233221e+02 7.4306616e+02 2.0023449e+01\n",
            " 3.0325012e+02 4.9858099e+02 4.4559082e+02 2.2487761e+02 1.7512365e+01\n",
            " 1.8983838e+02 7.1858414e+01 5.3631573e+00 1.2744092e+02 4.3240015e+02\n",
            " 2.5281116e+02 3.0460519e+02 9.8529327e+02 3.5058794e+03 9.7805145e+02\n",
            " 5.8293109e+02 9.4777515e+02 3.6097495e+02 2.9653098e+02 5.9593036e+02\n",
            " 2.6948223e+00 1.1319576e+02 1.3489890e+01 2.3469034e+02 8.8526413e+01\n",
            " 5.4663782e+02 2.1881448e+02 3.5277534e+01 5.4674384e+02 2.1851385e+01\n",
            " 4.5251007e+02 8.2026758e+02 6.7611023e+01 6.8925110e+01 2.2094575e+03\n",
            " 7.8212195e+02 2.3713325e+03 1.7880037e+01 3.8670126e+02 1.2078967e+03\n",
            " 1.9894528e+02 2.5183937e+02 6.4554199e+02 8.7256927e+01 1.3923959e+03\n",
            " 1.8955074e+02 1.1624373e+03 6.1272675e+02 3.5862617e+01 1.7658134e+02\n",
            " 4.1262427e+02 4.7592819e+02 5.8816719e+01 2.2342406e+02 2.8205704e+02\n",
            " 1.3506290e+03 2.4395990e+02 5.0029422e+02 4.5187828e+01 3.4501726e+03\n",
            " 9.8308083e+01 6.2813635e+02 9.8704968e+02 1.7907946e+03 5.0491989e+02\n",
            " 6.2792657e+02 4.9416483e+02 1.0475122e+03 2.7700937e+02 4.7595087e+02\n",
            " 5.9154523e+02 3.1593936e+02 4.3901651e+02 1.8166298e+03 9.3849976e+01\n",
            " 4.3905609e+02 1.7099644e+02 6.3440790e+02 8.9417584e+02 2.2738796e+01\n",
            " 3.4480920e+02 2.5444427e+02 8.2163538e+02 7.4380914e+02 5.1450897e+02\n",
            " 4.8511685e+02 4.8822217e+02 7.9101028e+01 1.6658121e+03 4.1248688e+02\n",
            " 3.0288719e+01 2.9055600e+02 1.0507615e+02 1.6017800e+03 1.5604137e+02]\n",
            "MLE[\\lambda = 0.002] = 0.0017845045076683164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 1 / 50.\n",
        "for N in [50, 100, 1000, 10000]:\n",
        "  key, x_key = rdm.split(key)\n",
        "  # generate N observations\n",
        "  x_n = exp_rv(x_key, N, rate)\n",
        "  # estimate rate\n",
        "  rate_hat = exp_mle(x_n)\n",
        "  # compute the sq-diff for rate\n",
        "  rate_err = sq_diff(rate, rate_hat)\n",
        "  print(f\"MSE[{N} | \\lambda = {rate}] = {rate_err}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-amDd6YTLuNI",
        "outputId": "5c17897c-4ba8-42e9-8831-424244458156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE[50 | \\lambda = 0.02] = 9.133363164437469e-06\n",
            "MSE[100 | \\lambda = 0.02] = 6.241853043320589e-07\n",
            "MSE[1000 | \\lambda = 0.02] = 2.641813523496239e-07\n",
            "MSE[10000 | \\lambda = 0.02] = 4.547181120528876e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient descent\n",
        "[Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) seeks to iteratively optimize a function $f(\\beta)$ by taking steps in the steepest direction,\n",
        "$$ \\hat{\\beta} = \\beta_t - \\rho_t \\nabla f(\\beta_t),$$\n",
        "where that direction is provided by the [gradient](https://en.wikipedia.org/wiki/Gradient) of (f).\n",
        "\n",
        "A helpful way to recast gradient descent is that we seek to perform a series of _local_ optimizations,\n",
        "\n",
        "$$\\hat{\\beta} = \\min_\\beta \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t}\\|\\beta - \\beta_t\\|_2^2.$$\n",
        "\n",
        "To see how these are equivalent let's solve the local problem. but using inner product notation,\n",
        "$$m(\\beta) = \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t} (\\beta - \\beta_t)^T(\\beta - \\beta_t).$$\n",
        "Now, using calculus again,\n",
        "$$\\begin{align*}\n",
        "\\nabla m(\\beta) &= \\nabla [ \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t} (\\beta - \\beta_t)^T(\\beta - \\beta_t)] \\\\\n",
        "&= \\nabla [\\nabla f(\\beta_t)^T \\beta] + \\frac{1}{2\\rho_t} \\nabla [(\\beta - \\beta_t)^T(\\beta - \\beta_t)] \\\\\n",
        "&= \\nabla f(\\beta_t) + \\frac{1}{\\rho_t}(\\beta - \\beta_t) \\Rightarrow \\\\\n",
        "\\hat{\\beta} &= \\beta_t - \\rho_t \\nabla f(\\beta_t).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Neat! However, notice that the original local objective can be thought of as minimizing the directional derivative, but with a distance penalty, where that distance is defined by the geometry of the parameter space.\n",
        "\n",
        "$$\\hat{\\beta} = \\min_\\beta \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t}\\text{dist}(\\beta, \\beta_t).$$\n",
        "\n",
        "When the natural geometry is $\\mathbb{R}^p$ then $\\text{dist}(\\cdot) = \\| \\cdot \\|_2^2$, however there are many  geometries that can describe the natural parameter space (for future class ðŸ˜‰)"
      ],
      "metadata": {
        "id": "I5mFAyAINs-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sim_linear_reg(key, N, P, r2=0.5):\n",
        "  key, x_key = rdm.split(key)\n",
        "  X = rdm.normal(x_key, shape=(N, P))\n",
        "\n",
        "  key, b_key = rdm.split(key)\n",
        "  beta = rdm.normal(b_key, shape=(P,))\n",
        "\n",
        "  # g = jnp.dot(X, beta)\n",
        "  g = X @ beta\n",
        "  s2g = jnp.var(g)\n",
        "\n",
        "  # back out what s2e is, such that s2g / (s2g + s2e) == h2\n",
        "  s2e = (1 - r2) / r2 * s2g\n",
        "  key, y_key = rdm.split(key)\n",
        "\n",
        "  # add env noise to g, but scale such that var(e) == s2e\n",
        "  y = g + jnp.sqrt(s2e) * rdm.normal(y_key, shape=(N,))\n",
        "  return y, X, beta\n",
        "\n",
        "key, sim_key = rdm.split(key)\n",
        "\n",
        "N = 1000\n",
        "P = 5\n",
        "y, X, beta = sim_linear_reg(sim_key, N, P)\n",
        "\n",
        "def linreg_loss(beta_hat, y, X):\n",
        "  loss = jnp.square(jnp.linalg.norm(X @ beta_hat - y))\n",
        "  return loss\n",
        "\n",
        "def gradient(beta_hat, y, X):\n",
        "  gradient = 2 * (X.T @ X) @ beta_hat - (X.T @ y)\n",
        "  return gradient\n",
        "\n",
        "step_size = 1 / N\n",
        "diff = 10.\n",
        "last_loss = 1000.\n",
        "idx = 0\n",
        "beta_hat = jnp.zeros((P,))\n",
        "points = []\n",
        "# while delta in loss is large, continue\n",
        "print(f\"true beta = {beta}\")\n",
        "while jnp.fabs(diff) > 1e-3:\n",
        "\n",
        "  # take a step in the direction of the gradient using step_size\n",
        "  beta_hat = beta_hat - step_size * gradient(beta_hat, y, X)\n",
        "\n",
        "  # update our current loss and compute delta\n",
        "  cur_loss = linreg_loss(beta_hat, y, X)\n",
        "  diff = last_loss - cur_loss\n",
        "  last_loss = cur_loss\n",
        "  points.append = last_loss\n",
        "\n",
        "  # wave to the crowd\n",
        "  print(f\"Loss[{idx}]({beta_hat}) = {last_loss}\")\n",
        "  idx += 1\n",
        "\n",
        "# OLS solution\n",
        "beta_hat_ols = jnp.linalg.solve(X.T @ X, X.T @ y)\n",
        "print(f\"ols beta = {beta_hat_ols}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6LpP-pxNy8y",
        "outputId": "543d8e25-fec6-4d82-dae1-7ff6f345497c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true beta = [-1.1528285  -0.26166388  0.8598455  -0.3606247   0.16862099]\n",
            "Loss[0]([-1.2251306  -0.19976614  0.9057299  -0.3435196   0.1651888 ]) = 2392.6494140625\n",
            "Loss[1]([ 1.3157523e-01  6.3808396e-02  1.5654147e-02 -2.2888184e-05\n",
            " -1.0015623e-01]) = 5199.1416015625\n",
            "Loss[2]([-1.3951987  -0.269184    0.86823845 -0.34303877  0.2854225 ]) = 2488.356201171875\n",
            "Loss[3]([ 0.35156953  0.13813311  0.08297706 -0.00149933 -0.24854171]) = 5898.51220703125\n",
            "Loss[4]([-1.6804284  -0.34725785  0.76012653 -0.34007064  0.4724877 ]) = 2889.731689453125\n",
            "Loss[5]([ 0.7225126   0.2181369   0.2467997  -0.00643733 -0.48814696]) = 7375.13427734375\n",
            "Loss[6]([-2.1644664  -0.4264331   0.52035826 -0.3326807   0.78299457]) = 4287.68896484375\n",
            "Loss[7]([ 1.3562579   0.29241168  0.5898761  -0.01678658 -0.8939776 ]) = 11017.5\n",
            "Loss[8]([-2.9968805  -0.48991752  0.03704488 -0.31881705  1.3167174 ]) = 8926.7421875\n",
            "Loss[9]([ 2.4528356   0.33671653  1.2631971  -0.03479189 -1.5990975 ]) = 21119.1015625\n",
            "Loss[10]([-4.445257   -0.50323534 -0.8932923  -0.29594234  2.2514126 ]) = 24091.91796875\n",
            "Loss[11]([ 4.370306    0.30258316  2.540699   -0.06339832 -2.8412292 ]) = 51238.25390625\n",
            "Loss[12]([-6.9888544  -0.3988617  -2.6391644  -0.26056486  3.905237  ]) = 73406.71875\n",
            "Loss[13]([ 7.7503166   0.0966095   4.9178267  -0.10681693 -5.0463953 ]) = 144677.171875\n",
            "Loss[14]([-11.486942    -0.04833585  -5.8663654   -0.20753515   6.8488493 ]) = 233414.28125\n",
            "Loss[15]([13.743809   -0.45707655  9.288964   -0.1714178  -8.979195  ]) = 440560.25\n",
            "Loss[16]([-19.481411     0.788236   -11.7759905   -0.12889503  12.106897  ]) = 751968.9375\n",
            "Loss[17]([ 24.416763    -1.6847161   17.266712    -0.26722884 -16.012953  ]) = 1387273.625\n",
            "Loss[18]([-3.3740833e+01  2.5534396e+00 -2.2532700e+01 -1.1911571e-02\n",
            "  2.1520235e+01]) = 2431293.0\n",
            "Loss[19]([ 43.479687   -4.1855335  31.756294   -0.4105288 -28.615438 ]) = 4432297.5\n",
            "Loss[20]([-59.23862      6.0572453  -42.035133     0.16435412  38.397247  ]) = 7867145.5\n",
            "Loss[21]([ 77.59926    -9.052885   57.988853   -0.6283994 -51.222237 ]) = 14252413.0\n",
            "Loss[22]([-104.911835     12.773903    -77.301674      0.43509996   68.685036  ]) = 25457036.0\n",
            "Loss[23]([138.75685   -18.272776  105.38       -0.9667829 -91.807274 ]) = 45965084.0\n",
            "Loss[24]([-186.82393      25.376787   -140.96313       0.86052775  123.07542   ]) = 82364504.0\n",
            "Loss[25]([ 248.48958    -35.44159    190.87259     -1.5048344 -164.70703  ]) = 148448592.0\n",
            "Loss[26]([-333.85187     48.701416  -255.74554      1.5450075  220.79233  ]) = 266449280.0\n",
            "Loss[27]([ 445.51678   -67.05805   344.94885    -2.380516 -295.69943 ]) = 479758272.0\n",
            "Loss[28]([-597.91315     91.47866   -462.53302      2.6713152  396.4025   ]) = 861878656.0\n",
            "Loss[29]([ 799.45465   -124.848366   622.4428      -3.8364305 -531.1371   ]) = 1551023744.0\n",
            "Loss[30]([-1072.3577     169.45358   -834.868        4.561961   712.0636  ]) = 2787720704.0\n",
            "Loss[31]([1435.4792    -229.94946   1121.9866      -6.3019085 -954.3721   ]) = 5015234560.0\n",
            "Loss[32]([-1925.041      310.9967   -1505.0295       7.789172  1279.5498  ]) = 9016425472.0\n",
            "Loss[33]([ 2578.6802    -420.43512   2020.9822     -10.540372 -1715.294   ]) = 16218261504.0\n",
            "Loss[34]([-3457.8022     567.1974   -2710.9324      13.372254  2299.8638  ]) = 29161406464.0\n",
            "Loss[35]([ 4633.8184    -764.8533    3638.4949     -17.913742 -3083.4482  ]) = 52449095680.0\n",
            "Loss[36]([-6213.424    1030.02    -4880.4688     23.13198  4134.469  ]) = 94313865216.0\n",
            "Loss[37]([ 8328.766    -1386.5747    6548.3604     -30.857422 -5543.564   ]) = 169622224896.0\n",
            "Loss[38]([-11167.99       1864.9587    -8783.194        40.327263   7433.397   ]) = 305027481600.0\n",
            "Loss[39]([14972.459    -2507.589    11782.603      -53.733887 -9967.346   ]) = 548571873280.0\n",
            "Loss[40]([-20076.795      3369.7708   -15803.106        70.799644  13365.621   ]) = 986506067968.0\n",
            "Loss[41]([ 26918.787     -4527.2666    21197.227       -94.366905 -17922.416   ]) = 1774138032128.0\n",
            "Loss[42]([-36096.453     6080.1016  -28429.21       125.02952  24033.365  ]) = 3190503768064.0\n",
            "Loss[43]([ 48400.836    -8164.0127   38130.125     -166.79895 -32227.916  ]) = 5737768550400.0\n",
            "Loss[44]([-64903.633    10959.456   -51137.758      221.83539  43217.188  ]) = 10318535196672.0\n",
            "Loss[45]([ 87031.13    -14710.03     68584.12      -296.24988 -57953.67   ]) = 18556645277696.0\n",
            "Loss[46]([-116706.96      19740.84     -91978.77        395.01892   77715.84   ]) = 33371539374080.0\n",
            "Loss[47]([ 156499.78     -26489.473    123354.664      -528.02997 -104217.05   ]) = 60014576271360.0\n",
            "Loss[48]([-209865.22     35541.188  -165429.4        705.3233  139756.03  ]) = 107928057020416.0\n",
            "Loss[49]([ 281426.4      -47682.42     221856.1        -943.57355 -187414.38   ]) = 194094890483712.0\n",
            "Loss[50]([-377394.03    63966.29  -297525.       1261.926  251325.75 ]) = 349053669867520.0\n",
            "Loss[51]([ 506086.03    -85806.65    399002.62     -1689.2605 -337032.3   ]) = 627728193683456.0\n",
            "Loss[52]([-678667.75    115098.01   -535086.7       2261.0847  451967.56  ]) = 1128886385508352.0\n",
            "Loss[53]([ 910101.25   -154382.53    717583.56     -3028.2297 -606098.7   ]) = 2030155207802880.0\n",
            "Loss[54]([-1220462.5      207067.88    -962317.44       4055.6467   812793.3   ]) = 3650968356913152.0\n",
            "Loss[55]([ 1636662.8     -277725.44    1290517.8       -5433.6416 -1089976.8   ]) = 6565792204193792.0\n",
            "Loss[56]([-2194802.       372484.06   -1730644.5        7280.1016  1461688.5   ]) = 1.1807717647712256e+16\n",
            "Loss[57]([ 2943279.5    -499564.3    2320873.8      -9756.281 -1960165.   ]) = 2.12346404339712e+16\n",
            "Loss[58]([-3947013.      669989.06  -3112390.2      13075.258  2628638.   ]) = 3.818771386990592e+16\n",
            "Loss[59]([ 5293047.     -898541.2    4173845.2     -17525.908 -3525081.   ]) = 6.867562082494054e+16\n",
            "Loss[60]([-7098123.     1205044.5   -5597292.       23492.486  4727239.5  ]) = 1.2350411246875443e+17\n",
            "Loss[61]([ 9518782.    -1616084.8    7506187.      -31493.424 -6339372.5  ]) = 2.2210598737543168e+17\n",
            "Loss[62]([-12764966.      2167312.5   -10066081.        42220.766   8501294.   ]) = 3.994286256333783e+17\n",
            "Loss[63]([ 17118198.     -2906537.5    13498987.       -56605.844 -11400502.   ]) = 7.183202235554202e+17\n",
            "Loss[64]([-22956018.     3897874.5  -18102632.       75894.06  15288434.  ]) = 1.2918046666019308e+18\n",
            "Loss[65]([ 30784730.     -5227304.5    24276276.      -101759.125 -20502274.   ]) = 2.3231421754093076e+18\n",
            "Loss[66]([-41283272.     7010128.5  -32555344.      136441.95  27494202.  ]) = 4.1778662839848796e+18\n",
            "Loss[67]([ 55362176.    -9400972.    43657848.     -182951.48 -36870608.  ]) = 7.513348881174757e+18\n",
            "Loss[68]([-74242432.    12607192.   -58546688.      245318.23  49444672.  ]) = 1.3511773049413698e+19\n",
            "Loss[69]([ 99561456.  -16906858.   78513120.    -328952.2 -66306912. ]) = 2.4299158595337978e+19\n",
            "Loss[70]([-1.3351509e+08  2.2672878e+07 -1.0528877e+08  4.4110406e+05\n",
            "  8.8919712e+07]) = 4.369886259098052e+19\n",
            "Loss[71]([ 1.7904798e+08 -3.0405330e+07  1.4119579e+08 -5.9150019e+05\n",
            " -1.1924422e+08]) = 7.858667000552227e+19\n",
            "Loss[72]([-2.4010914e+08  4.0774824e+07 -1.8934830e+08  7.9318019e+05\n",
            "  1.5991037e+08]) = 1.4132782934242047e+20\n",
            "Loss[73]([ 3.2199418e+08 -5.4680688e+07  2.5392245e+08 -1.0636368e+06\n",
            " -2.1444502e+08]) = 2.5415965980823545e+20\n",
            "Loss[74]([-4.3180467e+08  7.3328952e+07 -3.4051853e+08  1.4263215e+06\n",
            "  2.8757779e+08]) = 4.570729650097383e+20\n",
            "Loss[75]([ 5.7906413e+08 -9.8336936e+07  4.5664672e+08 -1.9126905e+06\n",
            " -3.8565120e+08]) = 8.219860264635712e+20\n",
            "Loss[76]([-7.7654381e+08  1.3187351e+08 -6.1237843e+08  2.5649210e+06\n",
            "  5.1717088e+08]) = 1.478233957140374e+21\n",
            "Loss[77]([ 1.0413708e+09 -1.7684720e+08  8.2121978e+08 -3.4395745e+06\n",
            " -6.9354310e+08]) = 2.6584109652170905e+21\n",
            "Loss[78]([-1.3965123e+09  2.3715843e+08 -1.1012828e+09  4.6125020e+06\n",
            "  9.3006406e+08]) = 4.780803502784545e+21\n",
            "Loss[79]([ 1.8727685e+09 -3.1803786e+08  1.4768566e+09 -6.1854200e+06\n",
            " -1.2472463e+09]) = 8.597650911827174e+21\n",
            "Loss[80]([-2.5114440e+09  4.2649987e+08 -1.9805133e+09  8.2947450e+06\n",
            "  1.6725983e+09]) = 1.5461745855789412e+22\n",
            "Loss[81]([ 3.3679288e+09 -5.7195110e+08  2.6559340e+09 -1.1123397e+07\n",
            " -2.2430093e+09]) = 2.780593791992638e+22\n",
            "Loss[82]([-4.5165030e+09  7.6700608e+08 -3.5616952e+09  1.4916715e+07\n",
            "  3.0079491e+09]) = 5.000532664934072e+22\n",
            "Loss[83]([ 6.0567793e+09 -1.0285814e+09  4.7763507e+09 -2.0003656e+07\n",
            " -4.0337585e+09]) = 8.99280484961199e+22\n",
            "Loss[84]([-8.1223409e+09  1.3793626e+09 -6.4052439e+09  2.6825396e+07\n",
            "  5.4094029e+09]) = 1.617238843165058e+23\n",
            "Loss[85]([ 1.0892325e+10 -1.8497720e+09  8.5896428e+09 -3.5973568e+07\n",
            " -7.2541870e+09]) = 2.908391853150579e+23\n",
            "Loss[86]([-1.4606963e+10  2.4806062e+09 -1.1518995e+10  4.8241504e+07\n",
            "  9.7281034e+09]) = 5.230363153349812e+23\n",
            "Loss[87]([ 1.9588415e+10 -3.3265761e+09  1.5447354e+10 -6.4693280e+07\n",
            " -1.3045706e+10]) = 9.406122345125948e+23\n",
            "Loss[88]([-2.6268713e+10  4.4610504e+09 -2.0715409e+10  8.6755664e+07\n",
            "  1.7494721e+10]) = 1.691568737402175e+24\n",
            "Loss[89]([ 3.5227206e+10 -5.9824174e+09  2.7780051e+10 -1.1634195e+08\n",
            " -2.3460995e+10]) = 3.0420649591016167e+24\n",
            "Loss[90]([-4.7240831e+10  8.0226217e+09 -3.7253964e+10  1.5601834e+08\n",
            "  3.1461966e+10]) = 5.470754348704556e+24\n",
            "Loss[91]([ 6.3351513e+10 -1.0758601e+10  4.9958789e+10 -2.0922554e+08\n",
            " -4.2191536e+10]) = 9.83843836573996e+24\n",
            "Loss[92]([-8.4956471e+10  1.4427642e+10 -6.6996380e+10  2.8057770e+08\n",
            "  5.6580235e+10]) = 1.7693147308516828e+25\n",
            "Loss[93]([ 1.1392941e+11 -1.9347948e+10  8.9844359e+10 -3.7626333e+08\n",
            " -7.5875959e+10]) = 3.181880729948568e+25\n",
            "Loss[94]([-1.5278309e+11  2.5946239e+10 -1.2048424e+11  5.0458134e+08\n",
            "  1.0175216e+11]) = 5.722197074902971e+25\n",
            "Loss[95]([ 2.0488716e+11 -3.4794758e+10  1.6157337e+11 -6.7665933e+08\n",
            " -1.3645300e+11]) = 1.0290624094971705e+26\n",
            "Loss[96]([-2.7476043e+11  4.6660919e+10 -2.1667522e+11  9.0742362e+08\n",
            "  1.8298795e+11]) = 1.85063436591373e+26\n",
            "Loss[97]([ 3.6846282e+11 -6.2573847e+10  2.9056860e+11 -1.2168870e+09\n",
            " -2.4539287e+11]) = 3.32812467229053e+26\n",
            "Loss[98]([-4.9412076e+11  8.3913597e+10 -3.8966205e+11  1.6318858e+09\n",
            "  3.2907991e+11]) = 5.9851959022769425e+26\n",
            "Loss[99]([ 6.6263227e+11 -1.1253089e+11  5.2254959e+11 -2.1884127e+09\n",
            " -4.4130697e+11]) = 1.0763594001335599e+27\n",
            "Loss[100]([-8.8861167e+11  1.5090765e+11 -7.0075626e+11  2.9347315e+09\n",
            "  5.9180718e+11]) = 1.9356916756483062e+27\n",
            "Loss[101]([ 1.1916576e+12 -2.0237212e+11  9.3973722e+11 -3.9355712e+09\n",
            " -7.9363290e+11]) = 3.481088911072497e+27\n",
            "Loss[102]([-1.5980518e+12  2.7138766e+11 -1.2602185e+12  5.2777329e+09\n",
            "  1.0642879e+12]) = 6.260281276175681e+27\n",
            "Loss[103]([ 2.1430400e+12 -3.6393982e+11  1.6899944e+12 -7.0776125e+09\n",
            " -1.4272454e+12]) = 1.1258298783904342e+28\n",
            "Loss[104]([-2.8738876e+12  4.8805531e+11 -2.2663382e+12  9.4912860e+09\n",
            "  1.9139831e+12]) = 2.02465819725089e+28\n",
            "Loss[105]([ 3.8539779e+12 -6.5449827e+11  3.0392336e+12 -1.2728107e+10\n",
            " -2.5667143e+12]) = 3.6410829236304445e+28\n",
            "Loss[106]([-5.1683095e+12  8.7770379e+11 -4.0757127e+12  1.7068765e+10\n",
            "  3.4420482e+12]) = 6.548009753314636e+28\n",
            "Loss[107]([ 6.9308724e+12 -1.1770297e+12  5.4656641e+12 -2.2889775e+10\n",
            " -4.6158997e+12]) = 1.1775738396401983e+29\n",
            "Loss[108]([-9.2945284e+12  1.5784353e+12 -7.3296348e+12  3.0695887e+10\n",
            "  6.1900734e+12]) = 2.117712003782853e+29\n",
            "Loss[109]([ 1.2464269e+13 -2.1167339e+12  9.8292801e+12 -4.1164149e+10\n",
            " -8.3010917e+12]) = 3.80842838576327e+29\n",
            "Loss[110]([-1.6714995e+13  2.8386098e+12 -1.3181389e+13  5.5202345e+10\n",
            "  1.1132037e+13]) = 6.848960691322798e+29\n",
            "Loss[111]([ 2.2415359e+13 -3.8066693e+12  1.7676674e+13 -7.4028122e+10\n",
            " -1.4928428e+13]) = 1.2316961641006619e+30\n",
            "Loss[112]([-3.0059730e+13  5.1048675e+12 -2.3705000e+13  9.9273998e+10\n",
            "  2.0019512e+13]) = 2.2150440571194376e+30\n",
            "Loss[113]([ 4.0311087e+13 -6.8457951e+12  3.1789186e+13 -1.3312965e+11\n",
            " -2.6846823e+13]) = 3.983467395143725e+30\n",
            "Loss[114]([-5.4058488e+13  9.1804349e+12 -4.2630352e+13  1.7853129e+11\n",
            "  3.6002471e+13]) = 7.163745027474424e+30\n",
            "Loss[115]([ 7.2494216e+13 -1.2311267e+13  5.7168707e+13 -2.3941692e+11\n",
            " -4.8280491e+13]) = 1.288306064997001e+31\n",
            "Loss[116]([-9.7217122e+13  1.6509813e+13 -7.6665116e+13  3.2106670e+11\n",
            "  6.4745726e+13]) = 2.316849271992751e+31\n",
            "Loss[117]([ 1.3037135e+14 -2.2140204e+13  1.0281044e+14 -4.3056189e+11\n",
            " -8.6826153e+13]) = 4.1665502644732955e+31\n",
            "Loss[118]([-1.74832281e+14  2.96907440e+13 -1.37872175e+14  5.77397850e+11\n",
            "  1.16436731e+14]) = 7.49299669980196e+31\n",
            "Loss[119]([ 2.3445586e+14 -3.9816264e+13  1.8489116e+14 -7.7430915e+11\n",
            " -1.5614548e+14]) = 1.3475175591455781e+32\n",
            "Loss[120]([-3.14413014e+14  5.33949202e+13 -2.47945107e+14  1.03837585e+12\n",
            "  2.09396232e+14]) = 2.4233341661782435e+32\n",
            "Loss[121]([ 4.2163815e+14 -7.1604353e+13  3.3250254e+14 -1.3924957e+12\n",
            " -2.8080724e+14]) = 4.3580487565754e+32\n",
            "Loss[122]([-5.6543063e+14  9.6023775e+13 -4.4589686e+14  1.8673813e+12\n",
            "  3.7657173e+14]) = 7.837380593327678e+32\n",
            "Loss[123]([ 7.58261108e+14 -1.28771055e+14  5.97962325e+14 -2.50422179e+12\n",
            " -5.04995074e+14]) = 1.4094504420281733e+33\n",
            "Loss[124]([-1.0168529e+15  1.7268622e+14 -8.0188710e+14  3.3582434e+12\n",
            "  6.7721494e+14]) = 2.534711631512749e+33\n",
            "Loss[125]([ 1.3636331e+15 -2.3157788e+14  1.0753571e+15 -4.5035165e+12\n",
            " -9.0816755e+14]) = 4.558347145446765e+33\n",
            "Loss[126]([-1.8286771e+15  3.1055352e+14 -1.4420890e+15  6.0393678e+12\n",
            "  1.2178825e+15]) = 8.197592517396315e+33\n",
            "Loss[127]([ 2.4523163e+15 -4.1646251e+14  1.9338885e+15 -8.0989819e+12\n",
            " -1.6332206e+15]) = 1.4742296159919065e+34\n",
            "Loss[128]([-3.2886371e+15  5.5848983e+14 -2.5934081e+15  1.0860986e+13\n",
            "  2.1902025e+15]) = 2.651209526334749e+34\n",
            "Loss[129]([ 4.4101707e+15 -7.4895324e+14  3.4778455e+15 -1.4564929e+13\n",
            " -2.9371338e+15]) = 4.767853765609348e+34\n",
            "Loss[130]([-5.9141834e+15  1.0043708e+15 -4.6639055e+15  1.9532039e+13\n",
            "  3.9387926e+15]) = 8.574359939734832e+34\n",
            "Loss[131]([ 7.9311151e+15 -1.3468942e+15  6.2544511e+15 -2.6193110e+13\n",
            " -5.2820501e+15]) = 1.5419871251373557e+35\n",
            "Loss[132]([-1.0635885e+16  1.8062294e+15 -8.3874253e+15  3.5125883e+13\n",
            "  7.0834034e+15]) = 2.773061350894453e+35\n",
            "Loss[133]([ 1.4263073e+16 -2.4222131e+15  1.1247815e+16 -4.7104999e+13\n",
            " -9.4990761e+15]) = 4.986989751885114e+35\n",
            "Loss[134]([-1.9127250e+16  3.2482679e+15 -1.5083689e+16  6.3169544e+13\n",
            "  1.2738574e+16]) = 8.96844735640419e+35\n",
            "Loss[135]([ 2.5650275e+16 -4.3560345e+15  2.0227722e+16 -8.4712492e+13\n",
            " -1.7082848e+16]) = 1.612858003964006e+36\n",
            "Loss[136]([-3.4397860e+16  5.8415861e+15 -2.7126041e+16  1.1360222e+14\n",
            "  2.2908662e+16]) = 2.9005148244213623e+36\n",
            "Loss[137]([ 4.6128670e+16 -7.8337594e+15  3.6376913e+16 -1.5234418e+14\n",
            " -3.0721268e+16]) = 5.2161968260388806e+36\n",
            "Loss[138]([-6.1860062e+16  1.0505328e+16 -4.8782634e+16  2.0429849e+14\n",
            "  4.1198228e+16]) = 9.380646766779203e+36\n",
            "Loss[139]([ 8.2956374e+16 -1.4087990e+16  6.5419112e+16 -2.7397095e+14\n",
            " -5.5248183e+16]) = 1.6869867567174672e+37\n",
            "Loss[140]([-1.1124722e+17  1.8892453e+16 -8.7729182e+16  3.6740398e+14\n",
            "  7.4089638e+16]) = 3.0338248278756936e+37\n",
            "Loss[141]([ 1.4918617e+17 -2.5335402e+16  1.1764772e+17 -4.9270060e+14\n",
            " -9.9356638e+16]) = 5.455937252707654e+37\n",
            "Loss[142]([-2.0006356e+17  3.3975605e+16 -1.5776944e+17  6.6072696e+14\n",
            "  1.3324051e+17]) = 9.811787032127646e+37\n",
            "Loss[143]([ 2.6829183e+17 -4.5562405e+16  2.1157397e+17 -8.8605571e+14\n",
            " -1.7867992e+17]) = 1.7645215662069347e+38\n",
            "Loss[144]([-3.5978817e+17  6.1100677e+16 -2.8372767e+17  1.1882274e+15\n",
            "  2.3961564e+17]) = 3.17326147541647e+38\n",
            "Loss[145]([ 4.8248776e+17 -8.1938029e+16  3.8048820e+17 -1.5934514e+15\n",
            " -3.2133245e+17]) = inf\n",
            "Loss[146]([-6.4703194e+17  1.0988160e+17 -5.1024720e+17  2.1368727e+15\n",
            "  4.3091733e+17]) = inf\n",
            "ols beta = [-1.1677448  -0.16932918  0.9089383  -0.34348717  0.11925054]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key, sim_key = rdm.split(key)\n",
        "\n",
        "N = 1000\n",
        "P = 5\n",
        "y, X, beta = sim_linear_reg(sim_key, N, P)\n",
        "\n",
        "step_size = 1 / N\n",
        "diff = 10.\n",
        "last_loss = 1000.\n",
        "idx = 0\n",
        "beta_hat = jnp.zeros((P,))\n",
        "# while delta in loss is large, continue\n",
        "print(\"Using JAX to compute gradient\")\n",
        "print(f\"true beta = {beta}\")\n",
        "while jnp.fabs(diff) > 1e-3:\n",
        "  # take a step in the direction of the gradient using step_size\n",
        "\n",
        "  beta_hat = beta_hat - step_size * jax.grad(linreg_loss)(beta_hat, y, X)\n",
        "\n",
        "  # update our current loss and compute delta\n",
        "  cur_loss = linreg_loss(beta_hat, y, X)\n",
        "  diff = last_loss - cur_loss\n",
        "  last_loss = cur_loss\n",
        "\n",
        "  # wave to the crowd\n",
        "  print(f\"Loss[{idx}]({beta_hat}) = {last_loss}\")\n",
        "  idx += 1\n",
        "\n",
        "# OLS solution\n",
        "beta_hat_ols = jnp.linalg.solve(X.T @ X, X.T @ y)\n",
        "print(f\"ols beta = {beta_hat_ols}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyZh3Msjuncp",
        "outputId": "d84627f8-b200-4f56-dbea-97d4fa45e621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using JAX to compute gradient\n",
            "true beta = [-0.49276644  0.36893874  0.8708915  -0.22680327  0.31571558]\n",
            "Loss[0]([-0.5512781   0.26551786  0.9118643  -0.22698347  0.35736504]) = 699.7918701171875\n",
            "Loss[1]([-0.5188158   0.29075423  0.8899959  -0.22931792  0.3277857 ]) = 698.3126220703125\n",
            "Loss[2]([-0.5198399   0.29171753  0.8909349  -0.23179689  0.33005244]) = 698.305419921875\n",
            "Loss[3]([-0.5196871   0.2918358   0.89090306 -0.23202236  0.32988426]) = 698.305419921875\n",
            "ols beta = [-0.5196884   0.29184714  0.8909022  -0.23206532  0.3298884 ]\n"
          ]
        }
      ]
    }
  ]
}