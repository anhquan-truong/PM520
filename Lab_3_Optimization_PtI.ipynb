{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhquan-truong/PM520/blob/main/Lab_3_Optimization_PtI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Move on Up, or: Maximum likelihood Estimation & Optimization Pt I\n",
        "\n",
        "TBD: move notes from slides to here.\n"
      ],
      "metadata": {
        "id": "q1bg914lpQpN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VAK5-ADoNq_I"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as rdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLE for iid Normal data\n",
        "Let $x_1, \\dotsc, x_n \\overset{\\mathrm{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2)$ where $\\mathcal{N}(\\mu, \\sigma^2)$ refers to the [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) with mean parameter $\\mu$ and variance parameter $\\sigma^2$. The likelihood of our data is given by,\n",
        "$$\\begin{align*}\n",
        "\\mathcal{L}(\\mu, \\sigma^2 | x_1, \\dots, x_n) &=\n",
        "  \\prod_{i=1}^n \\mathcal{N}(x_i | \\mu, \\sigma^2) \\\\\n",
        "  &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\\\\\n",
        "  &= \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\\right).\n",
        "\\end{align*}\\\\\n",
        "$$\n",
        "Thus, our _log_-likelihood is given by,\n",
        "The likelihood of our data is given by,\n",
        "(maximize the log mean maximize the LLH function because of monotone\n",
        "$$\\begin{align*}\n",
        "\\ell(\\mu, \\sigma^2 | x_1, \\dots, x_n) &=\n",
        "  \\log \\left[\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\\right)\\right]\\\\\n",
        "  &= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2.\n",
        "\\end{align*}\\\\\n",
        "$$"
      ],
      "metadata": {
        "id": "MbaYn27uwi28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm_rv(key, n: int, mu: float, sigma_sq: float):\n",
        "  r\"\"\"\n",
        "  Samples $n$ observations from $x_i \\sim N(\\mu, \\sigma^2)$\n",
        "\n",
        "  n: the number of observations\n",
        "  mu: the mean parameter\n",
        "  sigma_sq: the variance parameter\n",
        "\n",
        "  returns: x, Array of observations\n",
        "  \"\"\"\n",
        "  x = mu + jnp.sqrt(sigma_sq) * rdm.normal(key, shape=(n,))\n",
        "  return x\n",
        "\n",
        "\n",
        "def norm_mle(x):\n",
        "  r\"\"\"\n",
        "  Computes $\\hat{\\mu}_{MLE}$ and $\\hat{\\sigma^2}_{MLE}$.\n",
        "\n",
        "  x: Array of observations\n",
        "\n",
        "  returns:  Tuple of $\\hat{\\mu}_{MLE}$ and $\\hat{\\sigma^2}_{MLE}$.\n",
        "  \"\"\"\n",
        "  mu_hat = jnp.mean(x)\n",
        "  # mu_hat = jnp.sum(x) / len(x)\n",
        "  #ssq_hat = jnp.mean(jnp.sum(x - mu_hat)**2)\n",
        "  ssq_hat = jnp.var(x)\n",
        "\n",
        "  return mu_hat, ssq_hat\n",
        "\n",
        "seed = 0\n",
        "key = rdm.PRNGKey(seed)\n",
        "key, x_key = rdm.split(key)\n",
        "\n",
        "N = 1000 # how many obs we want\n",
        "\n",
        "mu = 58.\n",
        "sigma_sq = 100.\n",
        "x = norm_rv(x_key, N, mu, sigma_sq)\n",
        "#print(f\"x = {x}\")\n",
        "mu_hat, ssq_hat = norm_mle(x)\n",
        "print(fr\"MLE[\\mu, \\sigma^2] = {mu_hat}, {ssq_hat}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWoIiwnVwn6O",
        "outputId": "5de7dd3f-e610-4d98-e554-16fc75c666ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLE[\\mu, \\sigma^2] = 57.703948974609375, 110.18893432617188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sq_diff(param, estimate): #square difference - l2norm\n",
        "  return (param - estimate) ** 2\n",
        "\n",
        "mu = 58.\n",
        "sigma_sq = 10.\n",
        "for N in [50, 100, 1000, 10000]:\n",
        "  key, x_key = rdm.split(key)\n",
        "  # generate N observations\n",
        "  x_n = norm_rv(x_key, N, mu, sigma_sq)\n",
        "  # estimate mu, and sigma_sq\n",
        "  mu_hat, ssq_hat = norm_mle(x_n)\n",
        "  # compute the sq-diff for both and report\n",
        "  mu_err = sq_diff(mu, mu_hat)\n",
        "  ssq_err = sq_diff(sigma_sq, ssq_hat)\n",
        "  print(f\"MSE[{N} | mu, sigma^2] = {mu_err}, {ssq_err}\")\n",
        "\n",
        "  # the error is descreasing as a function of N"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z8awgX6GXXv",
        "outputId": "a0efcfac-bb4b-4f6c-d269-cf4b82086bf9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE[50 | mu, sigma^2] = 0.5217084288597107, 3.3595786094665527\n",
            "MSE[100 | mu, sigma^2] = 0.1484147012233734, 1.515868067741394\n",
            "MSE[1000 | mu, sigma^2] = 0.0020461762323975563, 0.027577972039580345\n",
            "MSE[10000 | mu, sigma^2] = 0.000588434049859643, 0.003656691173091531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLE for iid Exponential data\n",
        "TBD: Add notes for Exponential PDF and MLE estimator"
      ],
      "metadata": {
        "id": "RTHf96slw__S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exp_rv(key, n: int, rate: float):\n",
        "  \"\"\"\n",
        "  Samples $n$ observations from $x_i \\sim Exp(\\lambda)$\n",
        "\n",
        "  n: the number of observations\n",
        "  rate: the $\\lambda$ parameter\n",
        "\n",
        "  returns: x, Array of observations\n",
        "  \"\"\"\n",
        "  mean = 1 / rate\n",
        "  x = mean * rdm.exponential(key, shape=(n,))\n",
        "  return x\n",
        "\n",
        "\n",
        "def exp_mle(x):\n",
        "  \"\"\"\n",
        "  Computes $\\hat{\\lambda}_{MLE}$.\n",
        "\n",
        "  x: Array of observations\n",
        "\n",
        "  returns: $\\hat{\\lambda}_{MLE}$.\n",
        "  \"\"\"\n",
        "  rate_hat = 1. / jnp.mean(x)\n",
        "  return rate_hat\n",
        "\n",
        "key, x_key = rdm.split(key)\n",
        "N = 100\n",
        "rate = 1 / 500.\n",
        "x = exp_rv(x_key, N, rate)\n",
        "print(f\"x = {x}\")\n",
        "rate_hat = exp_mle(x)\n",
        "print(f\"MLE[\\lambda = {rate}] = {rate_hat}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3kQz53WA8YS",
        "outputId": "836e95c2-4a66-4443-9a82-22523096c59a",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:17: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:32: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:17: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:32: SyntaxWarning: invalid escape sequence '\\l'\n",
            "/tmp/ipython-input-2234527571.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  Samples $n$ observations from $x_i \\sim Exp(\\lambda)$\n",
            "/tmp/ipython-input-2234527571.py:17: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  Computes $\\hat{\\lambda}_{MLE}$.\n",
            "/tmp/ipython-input-2234527571.py:32: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  print(f\"MLE[\\lambda = {rate}] = {rate_hat}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = [8.5540985e+02 2.5296268e+02 6.9233221e+02 7.4306616e+02 2.0023449e+01\n",
            " 3.0325012e+02 4.9858099e+02 4.4559082e+02 2.2487761e+02 1.7512365e+01\n",
            " 1.8983838e+02 7.1858414e+01 5.3631573e+00 1.2744092e+02 4.3240015e+02\n",
            " 2.5281116e+02 3.0460519e+02 9.8529327e+02 3.5058794e+03 9.7805145e+02\n",
            " 5.8293109e+02 9.4777515e+02 3.6097495e+02 2.9653098e+02 5.9593036e+02\n",
            " 2.6948223e+00 1.1319576e+02 1.3489890e+01 2.3469034e+02 8.8526413e+01\n",
            " 5.4663782e+02 2.1881448e+02 3.5277534e+01 5.4674384e+02 2.1851385e+01\n",
            " 4.5251007e+02 8.2026758e+02 6.7611023e+01 6.8925110e+01 2.2094575e+03\n",
            " 7.8212195e+02 2.3713325e+03 1.7880037e+01 3.8670126e+02 1.2078967e+03\n",
            " 1.9894528e+02 2.5183937e+02 6.4554199e+02 8.7256927e+01 1.3923959e+03\n",
            " 1.8955074e+02 1.1624373e+03 6.1272675e+02 3.5862617e+01 1.7658134e+02\n",
            " 4.1262427e+02 4.7592819e+02 5.8816719e+01 2.2342406e+02 2.8205704e+02\n",
            " 1.3506290e+03 2.4395990e+02 5.0029422e+02 4.5187828e+01 3.4501726e+03\n",
            " 9.8308083e+01 6.2813635e+02 9.8704968e+02 1.7907946e+03 5.0491989e+02\n",
            " 6.2792657e+02 4.9416483e+02 1.0475122e+03 2.7700937e+02 4.7595087e+02\n",
            " 5.9154523e+02 3.1593936e+02 4.3901651e+02 1.8166298e+03 9.3849976e+01\n",
            " 4.3905609e+02 1.7099644e+02 6.3440790e+02 8.9417584e+02 2.2738796e+01\n",
            " 3.4480920e+02 2.5444427e+02 8.2163538e+02 7.4380914e+02 5.1450897e+02\n",
            " 4.8511685e+02 4.8822217e+02 7.9101028e+01 1.6658121e+03 4.1248688e+02\n",
            " 3.0288719e+01 2.9055600e+02 1.0507615e+02 1.6017800e+03 1.5604137e+02]\n",
            "MLE[\\lambda = 0.002] = 0.0017845045076683164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rate = 1 / 50.\n",
        "for N in [50, 100, 1000, 10000]:\n",
        "  key, x_key = rdm.split(key)\n",
        "  # generate N observations\n",
        "  x_n = exp_rv(x_key, N, rate)\n",
        "  # estimate rate\n",
        "  rate_hat = exp_mle(x_n)\n",
        "  # compute the sq-diff for rate\n",
        "  rate_err = sq_diff(rate, rate_hat)\n",
        "  print(f\"MSE[{N} | \\lambda = {rate}] = {rate_err}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-amDd6YTLuNI",
        "outputId": "5c17897c-4ba8-42e9-8831-424244458156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE[50 | \\lambda = 0.02] = 9.133363164437469e-06\n",
            "MSE[100 | \\lambda = 0.02] = 6.241853043320589e-07\n",
            "MSE[1000 | \\lambda = 0.02] = 2.641813523496239e-07\n",
            "MSE[10000 | \\lambda = 0.02] = 4.547181120528876e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient descent\n",
        "[Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) seeks to iteratively optimize a function $f(\\beta)$ by taking steps in the steepest direction,\n",
        "$$ \\hat{\\beta} = \\beta_t - \\rho_t \\nabla f(\\beta_t),$$\n",
        "where that direction is provided by the [gradient](https://en.wikipedia.org/wiki/Gradient) of (f).\n",
        "\n",
        "A helpful way to recast gradient descent is that we seek to perform a series of _local_ optimizations,\n",
        "\n",
        "$$\\hat{\\beta} = \\min_\\beta \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t}\\|\\beta - \\beta_t\\|_2^2.$$\n",
        "\n",
        "To see how these are equivalent let's solve the local problem. but using inner product notation,\n",
        "$$m(\\beta) = \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t} (\\beta - \\beta_t)^T(\\beta - \\beta_t).$$\n",
        "Now, using calculus again,\n",
        "$$\\begin{align*}\n",
        "\\nabla m(\\beta) &= \\nabla [ \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t} (\\beta - \\beta_t)^T(\\beta - \\beta_t)] \\\\\n",
        "&= \\nabla [\\nabla f(\\beta_t)^T \\beta] + \\frac{1}{2\\rho_t} \\nabla [(\\beta - \\beta_t)^T(\\beta - \\beta_t)] \\\\\n",
        "&= \\nabla f(\\beta_t) + \\frac{1}{\\rho_t}(\\beta - \\beta_t) \\Rightarrow \\\\\n",
        "\\hat{\\beta} &= \\beta_t - \\rho_t \\nabla f(\\beta_t).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Neat! However, notice that the original local objective can be thought of as minimizing the directional derivative, but with a distance penalty, where that distance is defined by the geometry of the parameter space.\n",
        "\n",
        "$$\\hat{\\beta} = \\min_\\beta \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t}\\text{dist}(\\beta, \\beta_t).$$\n",
        "\n",
        "When the natural geometry is $\\mathbb{R}^p$ then $\\text{dist}(\\cdot) = \\| \\cdot \\|_2^2$, however there are many  geometries that can describe the natural parameter space (for future class ðŸ˜‰)"
      ],
      "metadata": {
        "id": "I5mFAyAINs-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sim_linear_reg(key, N, P, r2=0.5):\n",
        "  key, x_key = rdm.split(key)\n",
        "  X = rdm.normal(x_key, shape=(N, P))\n",
        "\n",
        "  key, b_key = rdm.split(key)\n",
        "  beta = rdm.normal(b_key, shape=(P,))\n",
        "\n",
        "  # g = jnp.dot(X, beta)\n",
        "  g = X @ beta\n",
        "  s2g = jnp.var(g)\n",
        "\n",
        "  # back out what s2e is, such that s2g / (s2g + s2e) == h2\n",
        "  s2e = (1 - r2) / r2 * s2g\n",
        "  key, y_key = rdm.split(key)\n",
        "\n",
        "  # add env noise to g, but scale such that var(e) == s2e\n",
        "  y = g + jnp.sqrt(s2e) * rdm.normal(y_key, shape=(N,))\n",
        "  return y, X, beta\n",
        "\n",
        "key, sim_key = rdm.split(key)\n",
        "\n",
        "N = 1000\n",
        "P = 5\n",
        "y, X, beta = sim_linear_reg(sim_key, N, P)\n",
        "\n",
        "def linreg_loss(beta_hat, y, X):\n",
        "  #loss = jnp.square(jnp.linalg.norm(X @ beta_hat - y))\n",
        "  loss = jnp.sum((y - X @ beta_hat)**2)\n",
        "  return loss\n",
        "\n",
        "def gradient(beta_hat, y, X):\n",
        "  y_hat = X @ beta_hat\n",
        "  return -1/2 * X.T @ (y-y_hat)\n",
        "  #gradient = 1/2 * ((X.T @ X) @ beta_hat - (X.T @ y_hat))\n",
        "  #return gradient\n",
        "\n",
        "step_size = 1 / N\n",
        "diff = 10.\n",
        "last_loss = 1000.\n",
        "idx = 0\n",
        "beta_hat = jnp.zeros((P,))\n",
        "points = []\n",
        "# while delta in loss is large, continue\n",
        "print(f\"true beta = {beta}\")\n",
        "while jnp.fabs(diff) > 1e-3:\n",
        "\n",
        "  # take a step in the direction of the gradient using step_size\n",
        "  beta_hat = beta_hat - step_size * gradient(beta_hat, y, X)\n",
        "\n",
        "  # update our current loss and compute delta\n",
        "  cur_loss = linreg_loss(beta_hat, y, X)\n",
        "  diff = last_loss - cur_loss\n",
        "  last_loss = cur_loss\n",
        "\n",
        "  # wave to the crowd\n",
        "  print(f\"Loss[{idx}]({beta_hat}) = {last_loss}\")\n",
        "  idx += 1\n",
        "\n",
        "# OLS solution\n",
        "beta_hat_ols = jnp.linalg.solve(X.T @ X, X.T @ y)\n",
        "print(f\"ols beta = {beta_hat_ols}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6LpP-pxNy8y",
        "outputId": "67351fef-2118-42b5-bbcb-f09b81ae7a04"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true beta = [ 0.27317098  0.35787642 -0.43727624 -0.11257944  1.0787896 ]\n",
            "Loss[0]([-0.2426016  -0.27000135  0.39343295  0.06244757 -1.087016  ]) = 3777.68359375\n",
            "Loss[1]([-0.7305606 -0.7884005  1.1795309  0.1473572 -3.2879128]) = 12789.728515625\n",
            "Loss[2]([-1.7137765  -1.7815676   2.7505023   0.23630705 -7.7456527 ]) = 49456.31640625\n",
            "Loss[3]([ -3.6985135  -3.67969     5.890618    0.2505736 -16.777561 ]) = 198885.125\n",
            "Loss[4]([ -7.712308    -7.2974014   12.168433    -0.05214828 -35.08352   ]) = 808855.125\n",
            "Loss[5]([-15.844563  -14.171099   24.72171    -1.3284883 -72.19871  ]) = 3302788.5\n",
            "Loss[6]([ -32.351723  -27.184803   49.828514   -5.24057  -147.47455 ]) = 13515814.0\n",
            "Loss[7]([ -65.92068   -51.722427  100.05257   -15.820463 -300.19678 ]) = 55405400.0\n",
            "Loss[8]([-134.3121    -97.76929   200.54146   -42.569073 -610.1455  ]) = 227484320.0\n",
            "Loss[9]([ -273.9022   -183.7002    401.64087  -107.40582 -1239.386  ]) = 935440512.0\n",
            "Loss[10]([ -559.322    -343.0067    804.16345  -260.09686 -2517.2383 ]) = 3852382976.0\n",
            "Loss[11]([-1143.9431   -636.01294  1610.0183   -612.221   -5113.091  ]) = 15888249856.0\n",
            "Loss[12]([ -2343.4604  -1169.7424   3223.6748  -1411.4292 -10387.973 ]) = 65620631552.0\n",
            "Loss[13]([ -4808.6973  -2130.3357   6455.5435  -3202.8428 -21110.    ]) = 271397928960.0\n",
            "Loss[14]([ -9883.355   -3832.8535  12929.732   -7178.078  -42910.71  ]) = 1123979100160.0\n",
            "Loss[15]([-20345.6     -6789.9688  25901.734  -15926.723  -87250.32  ]) = 4660996866048.0\n",
            "Loss[16]([ -41947.15   -11785.708   51898.508  -35048.168 -177457.   ]) = 19353147801600.0\n",
            "Loss[17]([ -86611.16   -19891.809  104008.84   -76597.21  -361030.16 ]) = 80456280178688.0\n",
            "Loss[18]([-179083.75   -32229.926  208486.06  -166427.84  -734711.4  ]) = 334878432296960.0\n",
            "Loss[19]([ -370782.8     -48937.715   418000.5    -359804.06  -1495586.2  ]) = 1395458765225984.0\n",
            "Loss[20]([ -768658.25   -65947.19   838245.4   -774505.4  -3045269.  ]) = 5821474908667904.0\n",
            "Loss[21]([-1595387.2    -66155.51  1681364.6  -1660883.2  -6202368.  ]) = 2.4311937256914944e+16\n",
            "Loss[22]([-3.3150250e+06  3.1718359e+03  3.3732675e+06 -3.5498250e+06\n",
            " -1.2635870e+07]) = 1.0163918987906253e+17\n",
            "Loss[23]([ -6895476.      285824.66   6769237.    -7564719.   -25749372.  ]) = 4.253466916804362e+17\n",
            "Loss[24]([-14357136.   1148700.  13587230. -16078145. -52485592.]) = 1.7817668391482163e+18\n",
            "Loss[25]([-2.9920400e+07  3.4899010e+06  2.7278888e+07 -3.4092208e+07\n",
            " -1.0700982e+08]) = 7.470862652365865e+18\n",
            "Loss[26]([-6.2407060e+07  9.4462660e+06  5.4780940e+07 -7.2136056e+07\n",
            " -2.1823045e+08]) = 3.1353739548358083e+19\n",
            "Loss[27]([-1.3026822e+08  2.3998164e+07  1.1003786e+08 -1.5234074e+08\n",
            " -4.4515722e+08]) = 1.3170254622319496e+20\n",
            "Loss[28]([-2.7211626e+08  5.8573552e+07  2.2108918e+08 -3.2116122e+08\n",
            " -9.0827226e+08]) = 5.536982227805536e+20\n",
            "Loss[29]([-5.6879488e+08  1.3907653e+08  4.4433328e+08 -6.7599130e+08\n",
            " -1.8536227e+09]) = 2.329779219020703e+21\n",
            "Loss[30]([-1.1896443e+09  3.2365235e+08  8.9324288e+08 -1.4207885e+09\n",
            " -3.7837947e+09]) = 9.81088687444358e+21\n",
            "Loss[31]([-2.4895181e+09  7.4177395e+08  1.7961946e+09 -2.9822275e+09\n",
            " -7.7256131e+09]) = 4.134694469653846e+22\n",
            "Loss[32]([-5.2122813e+09  1.6798268e+09  3.6129684e+09 -6.2520433e+09\n",
            " -1.5777413e+10]) = 1.743856285680684e+23\n",
            "Loss[33]([-1.0917766e+10  3.7677292e+09  7.2695276e+09 -1.3092270e+10\n",
            " -3.2228078e+10]) = 7.360390677142545e+23\n",
            "Loss[34]([-2.2877753e+10  8.3844019e+09  1.4631330e+10 -2.7387798e+10\n",
            " -6.5845658e+10]) = 3.1088920365035195e+24\n",
            "Loss[35]([-4.7956582e+10  1.8535963e+10  2.9457900e+10 -5.7237721e+10\n",
            " -1.3455887e+11]) = 1.314065112721445e+25\n",
            "Loss[36]([-1.00559012e+11  4.07525417e+10  5.93287496e+10 -1.19514644e+11\n",
            " -2.75035259e+11]) = 5.558110980524315e+25\n",
            "Loss[37]([-2.10919064e+11  8.91751793e+10  1.19530635e+11 -2.49345229e+11\n",
            " -5.62281447e+11]) = 2.352499990064446e+26\n",
            "Loss[38]([-4.4250458e+11  1.9434244e+11  2.4090701e+11 -5.1981323e+11\n",
            " -1.1497591e+12]) = 9.963629546356059e+26\n",
            "Loss[39]([-9.28566608e+11  4.22044566e+11  4.85715149e+11 -1.08288606e+12\n",
            " -2.35150718e+12]) = 4.222650384018862e+27\n",
            "Loss[40]([-1.9488979e+12  9.1371025e+11  9.7967486e+11 -2.2543860e+12\n",
            " -4.8102821e+12]) = 1.7907149951094176e+28\n",
            "Loss[41]([-4.0910436e+12  1.9727825e+12  1.9767726e+12 -4.6903250e+12\n",
            " -9.8418819e+12]) = 7.598647515263254e+28\n",
            "Loss[42]([-8.5888976e+12  4.2491935e+12  3.9903641e+12 -9.7526879e+12\n",
            " -2.0140385e+13]) = 3.226337403826561e+29\n",
            "Loss[43]([-1.8033884e+13  9.1328484e+12  8.0585383e+12 -2.0267892e+13\n",
            " -4.1222849e+13]) = 1.3706994370921804e+30\n",
            "Loss[44]([-3.7868714e+13  1.9591959e+13  1.6281526e+13 -4.2098871e+13\n",
            " -8.4389329e+13]) = 5.826790336785147e+30\n",
            "Loss[45]([-7.9524910e+13  4.1957275e+13  3.2910640e+13 -8.7402467e+13\n",
            " -1.7278861e+14]) = 2.478375785032773e+31\n",
            "Loss[46]([-1.6701276e+14  8.9715869e+13  6.6556046e+13 -1.8137647e+14\n",
            " -3.5385028e+14]) = 1.0547558619721261e+32\n",
            "Loss[47]([-3.5076250e+14  1.9157074e+14  1.3466553e+14 -3.7623044e+14\n",
            " -7.2476875e+14]) = 4.4913818622191565e+32\n",
            "Loss[48]([-7.3669480e+14  4.0854930e+14  2.7261577e+14 -7.8010578e+14\n",
            " -1.4847514e+15]) = 1.9135862035191582e+33\n",
            "Loss[49]([-1.5472749e+15  8.7029352e+14  5.5217814e+14 -1.6169245e+15\n",
            " -3.0421549e+15]) = 8.157423220031563e+33\n",
            "Loss[50]([-3.24973385e+15  1.85198695e+15  1.11904984e+15 -3.35021005e+15\n",
            " -6.23420409e+15]) = 3.4792998050537612e+34\n",
            "Loss[51]([-6.8253365e+15  3.9373230e+15  2.2691848e+15 -6.9391951e+15\n",
            " -1.2777674e+16]) = 1.4847791429362902e+35\n",
            "Loss[52]([-1.4334823e+16  8.3635522e+15  4.6041437e+15 -1.4368420e+16\n",
            " -2.6193438e+16]) = 6.339574526891885e+35\n",
            "Loss[53]([-3.0105719e+16  1.7751665e+16  9.3474895e+15 -2.9742633e+16\n",
            " -5.3703451e+16]) = 2.7082220526588917e+36\n",
            "Loss[54]([-6.3225303e+16  3.7650843e+16  1.8989627e+16 -6.1549905e+16\n",
            " -1.1012348e+17]) = 1.157528524935803e+37\n",
            "Loss[55]([-1.3277469e+17  7.9803774e+16  3.8603054e+16 -1.2733843e+17\n",
            " -2.2585240e+17]) = 4.949942346180794e+37\n",
            "Loss[56]([-2.7881725e+17  1.6904747e+17  7.8526974e+16 -2.6337942e+17\n",
            " -4.6327153e+17]) = inf\n",
            "Loss[57]([-5.8546616e+17  3.5789165e+17  1.5985131e+17 -5.4462838e+17\n",
            " -9.5041167e+17]) = inf\n",
            "ols beta = [ 0.24150695  0.29127392 -0.3945152  -0.10162834  1.0622745 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key, sim_key = rdm.split(key)\n",
        "\n",
        "N = 1000\n",
        "P = 5\n",
        "y, X, beta = sim_linear_reg(sim_key, N, P)\n",
        "\n",
        "step_size = 1 / N\n",
        "diff = 10.\n",
        "last_loss = 1000.\n",
        "idx = 0\n",
        "beta_hat = jnp.zeros((P,))\n",
        "# while delta in loss is large, continue\n",
        "print(\"Using JAX to compute gradient\")\n",
        "print(f\"true beta = {beta}\")\n",
        "while jnp.fabs(diff) > 1e-3:\n",
        "  # take a step in the direction of the gradient using step_size\n",
        "  jax_gradient = jax.grad(linreg_loss)\n",
        "  beta_hat = beta_hat - step_size * jax.grad(linreg_loss)(beta_hat, y, X)\n",
        "\n",
        "  # update our current loss and compute delta\n",
        "  cur_loss = linreg_loss(beta_hat, y, X)\n",
        "  diff = -last_loss + cur_loss\n",
        "  last_loss = cur_loss\n",
        "\n",
        "  # wave to the crowd\n",
        "  print(f\"Loss[{idx}]({beta_hat}) = {last_loss}\")\n",
        "  idx += 1\n",
        "\n",
        "# OLS solution\n",
        "beta_hat_ols = jnp.linalg.solve(X.T @ X, X.T @ y)\n",
        "print(f\"ols beta = {beta_hat_ols}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyZh3Msjuncp",
        "outputId": "9bba3555-781c-48f6-a648-a88fb81a99c0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using JAX to compute gradient\n",
            "true beta = [-0.10399994 -0.05746136  1.5482496   1.9442106   1.4066578 ]\n",
            "Loss[0]([0.0387072  0.00539958 3.3333583  3.5295694  2.7882462 ]) = 16051.1689453125\n",
            "Loss[1]([-0.43933755 -0.17723572 -0.60123205  0.34447336 -0.07998443]) = 17306.716796875\n",
            "Loss[2]([0.5971826 0.0774616 4.015812  3.2639012 2.9249098]) = 19508.658203125\n",
            "Loss[3]([-1.1464326  -0.13437687 -1.4028525   0.5519347  -0.27514887]) = 23072.908203125\n",
            "Loss[4]([ 1.4915488  -0.09753431  4.9817643   3.0972779   3.1823533 ]) = 28727.400390625\n",
            "Loss[5]([-2.2781854  0.1986772 -2.5882707  0.6929574 -0.6008816]) = 37690.5234375\n",
            "Loss[6]([ 2.9252787  -0.62503654  6.455108    2.9677818   3.585001  ]) = 51970.7734375\n",
            "Loss[7]([-4.096849   0.9698879 -4.4354835  0.8247254 -1.0922008]) = 74862.578125\n",
            "Loss[8]([ 5.2352157 -1.7052794  8.784864   2.8193264  4.180567 ]) = 111761.7890625\n",
            "Loss[9]([-7.0343165  2.444725  -7.385849   1.005825  -1.8122978]) = 171505.8125\n",
            "Loss[10]([ 8.974659  -3.6859303 12.531748   2.587059   5.051365 ]) = 268569.59375\n",
            "Loss[11]([-11.799002    5.075078  -12.153769    1.3115399  -2.8673778]) = 426669.21875\n",
            "Loss[12]([15.05043   -7.151575  18.60761    2.1804452  6.333653 ]) = 684672.9375\n",
            "Loss[13]([-19.551708    9.614885  -19.90447     1.8534263  -4.4316306]) = 1106288.0\n",
            "Loss[14]([ 24.948357  -13.072584   28.502577    1.4598136   8.249725 ]) = 1795959.5\n",
            "Loss[15]([-32.194416   17.311378  -32.544434    2.8081095  -6.788575 ]) = 2924929.5\n",
            "Loss[16]([ 41.103397   -23.050617    44.656487     0.20045304  11.161073  ]) = 4773989.0\n",
            "Loss[17]([-52.84428    30.220097  -53.19655     4.4624977 -10.399112 ]) = 7803527.5\n",
            "Loss[18]([ 67.506195  -39.722355   71.067375   -1.9646969  15.655522 ]) = 12768646.0\n",
            "Loss[19]([-86.61119   51.721973 -86.980095   7.286731 -16.013203]) = 20907550.0\n",
            "Loss[20]([110.69969  -67.42224  114.28957   -5.638158  22.690262]) = 34250780.0\n",
            "Loss[21]([-141.87196    87.372604 -142.2869     12.053222  -24.853123]) = 56128404.0\n",
            "Loss[22]([ 181.40947  -113.26973   185.069     -11.810201   33.826527]) = 92001688.0\n",
            "Loss[23]([-232.36156   146.29457  -232.87726    20.0313    -38.913578]) = 150827184.0\n",
            "Loss[24]([ 297.2235   -188.95268   301.02625   -22.107872   51.613873]) = 247293248.0\n",
            "Loss[25]([-380.601     243.46214  -381.31586    33.306583  -61.454365]) = 405490272.0\n",
            "Loss[26]([ 486.98267  -313.65515   491.05646   -39.203938   80.2209  ]) = 664926080.0\n",
            "Loss[27]([-623.5222    403.44928  -624.6043     55.304127  -97.80713 ]) = 1090391040.0\n",
            "Loss[28]([ 797.9768  -518.85565  802.5445   -67.48755  126.46837]) = 1788152704.0\n",
            "Loss[29]([-1021.68713   666.57806 -1023.4267     91.6474   -156.69902]) = 2932488704.0\n",
            "Loss[30]([1307.764   -856.1977  1313.2024  -114.16289  201.52397]) = 4809205760.0\n",
            "Loss[31]([-1674.4127   1098.9944  -1677.294     151.5659   -252.42278]) = 7887055360.0\n",
            "Loss[32]([ 2143.5312  -1410.4053   2150.466    -191.05348   323.6819 ]) = 12934811648.0\n",
            "Loss[33]([-2744.5728   1809.2112  -2749.4172    250.20473  -408.3955 ]) = 21213261824.0\n",
            "Loss[34]([ 3513.8555  -2320.4504   3523.3455   -317.55817   522.91754]) = 34790174720.0\n",
            "Loss[35]([-4499.289    2975.2122  -4507.4434    412.41382  -662.99115]) = 57056755712.0\n",
            "Loss[36]([ 5760.8154 -3814.2908  5774.5996  -525.508    848.3619]) = 93574692864.0\n",
            "Loss[37]([-7376.626   4888.9434 -7390.329    678.9587 -1079.1147]) = 153465421824.0\n",
            "Loss[38]([ 9445.419   -6265.8096   9466.358    -867.11237  1380.5557 ]) = 251688222720.0\n",
            "Loss[39]([-12095.015    8029.24   -12117.95     1116.7073  -1759.8921]) = 412776857600.0\n",
            "Loss[40]([ 15487.708  -10288.23    15520.538   -1428.0164   2251.5457]) = 676968398848.0\n",
            "Loss[41]([-19832.688   13181.43   -19870.977    1835.3535  -2874.4036]) = 1110251012096.0\n",
            "Loss[42]([ 25396.527  -16887.348   25449.121   -2348.7012   3677.848 ]) = 1820849995776.0\n",
            "Loss[43]([-32521.984   21633.594  -32585.629    3014.7979  -4699.9033]) = 2986258202624.0\n",
            "Loss[44]([ 41646.547  -27712.746   41731.684   -3859.564    6014.4766]) = 4897567145984.0\n",
            "Loss[45]([-53331.977   35498.324  -53437.395    4950.1304  -7690.999 ]) = 8032183910400.0\n",
            "Loss[46]([ 68296.13   -45469.848   68434.97    -6338.53     9843.6045]) = 13173061058560.0\n",
            "Loss[47]([-87460.01   58240.34  -87634.36    8125.361 -12593.224]) = 21604312547328.0\n",
            "Loss[48]([112001.27  -74595.78  112228.41  -10405.422  16119.946]) = 35431789887488.0\n",
            "Loss[49]([-143429.97    95542.016 -143717.58    13334.227  -20629.18 ]) = 58109502423040.0\n",
            "Loss[50]([ 183678.22  -122367.64   184050.3    -17076.805   26409.117]) = 95301641699328.0\n",
            "Loss[51]([-235221.38   156722.39  -235694.89    21878.625  -33803.785]) = 156298054205440.0\n",
            "Loss[52]([ 301228.44  -200720.14   301839.3    -28019.992   43278.81 ]) = 256333966213120.0\n",
            "Loss[53]([-385760.2    257066.02  -386539.8     35893.676  -55405.215]) = 420397136740352.0\n",
            "Loss[54]([ 494014.    -329227.     495016.62   -45969.105   70939.67 ]) = 689466469515264.0\n",
            "Loss[55]([-632646.9   421640.5  -633929.6    58881.13  -90825.91]) = 1130749596008448.0\n",
            "Loss[56]([ 810185.5  -539990.7   811833.    -75408.92  116297.64]) = 1854470241648640.0\n",
            "Loss[57]([-1037547.25   691555.2  -1039655.9     96584.06  -148909.52]) = 3041394654445568.0\n",
            "Loss[58]([1328715.   -885657.06 1331420.6  -123694.05  190677.67]) = 4987992481464320.0\n",
            "Loss[59]([-1701595.    1134233.8  -1705057.6    158421.36  -244159.98]) = 8180478716674048.0\n",
            "Loss[60]([ 2179120.   -1452571.5   2183556.5   -202887.02   312654.62]) = 1.3416262356959232e+16\n",
            "Loss[61]([-2790658.    1860250.8  -2796339.5    259840.95  -400362.88]) = 2.200318402940109e+16\n",
            "Loss[62]([ 3573818.5  -2382345.8   3581094.5   -332770.5    512690.25]) = 3.608604893301965e+16\n",
            "Loss[63]([-4576755.5   3050960.2  -4586073.5    426177.94  -656531.6 ]) = 5.918233864647475e+16\n",
            "Loss[64]([ 5861153.5  -3907215.8   5873103.5   -545789.6    840743.25]) = 9.706114128858317e+16\n",
            "Loss[65]([-7505999.5   5003781.   -7521306.5    698978.75 -1076640.4 ]) = 1.5918366851701146e+17\n",
            "Loss[66]([ 9612452.  -6408091.   9632076.   -895151.   1378742.9]) = 2.6106694138750566e+17\n",
            "Loss[67]([-12310056.    8206506.  -12335218.    1146388.2  -1765616.4]) = 4.281596203814093e+17\n",
            "Loss[68]([ 15764694. -10509616.  15796960.  -1468125.   2261061.]) = 7.021965040093757e+17\n",
            "Loss[69]([-20188902.   13459082.  -20230184.    1880161.2  -2895534.5]) = 1.15162937228578e+18\n",
            "Loss[70]([ 25854678.  -17236290.   25907472.   -2407821.8   3708073.5]) = 1.8887090509174538e+18\n",
            "Loss[71]([-33110438.   22073486.  -33178156.    3083589.2  -4748626.5]) = 3.097549655328686e+18\n",
            "Loss[72]([ 42402496.  -28268254.   42489212.   -3948973.8   6081207.5]) = 5.080092815266939e+18\n",
            "Loss[73]([-54302224.   36201576.  -54413212.    5057241.   -7787743.5]) = 8.331525170216829e+18\n",
            "Loss[74]([ 69541496. -46361288.  69683600.  -6476530.   9973182.]) = 1.366399273769789e+19\n",
            "Loss[75]([-89057480.  59372048. -89239424.   8294135. -12771930.]) = 2.2409399373377044e+19\n",
            "Loss[76]([ 1.1405044e+08 -7.6034320e+07  1.1428339e+08 -1.0621843e+07\n",
            "  1.6356118e+07]) = 3.6752222893482443e+19\n",
            "Loss[77]([-1.4605734e+08  9.7372464e+07 -1.4635566e+08  1.3602803e+07\n",
            " -2.0946154e+07]) = 6.027489758119199e+19\n",
            "Loss[78]([ 1.8704666e+08 -1.2469912e+08  1.8742878e+08 -1.7420328e+07\n",
            "  2.6824310e+07]) = 9.885300906617786e+19\n",
            "Loss[79]([-2.3953894e+08  1.5969474e+08 -2.4002840e+08  2.2309192e+07\n",
            " -3.4352112e+07]) = 1.6212228582812942e+20\n",
            "Loss[80]([ 3.0676294e+08 -2.0451150e+08  3.0739008e+08 -2.8570092e+07\n",
            "  4.3992480e+07]) = 2.6588676935511756e+20\n",
            "Loss[81]([-3.9285286e+08  2.6190558e+08 -3.9365523e+08  3.6588024e+07\n",
            " -5.6338312e+07]) = 4.360638134855433e+20\n",
            "Loss[82]([ 5.0310285e+08 -3.3540646e+08  5.0412947e+08 -4.6856200e+07\n",
            "  7.2148880e+07]) = 7.151598692461571e+20\n",
            "Loss[83]([-6.4429248e+08  4.2953498e+08 -6.4560774e+08  6.0005936e+07\n",
            " -9.2396400e+07]) = 1.1728871580236314e+21\n",
            "Loss[84]([ 8.2510694e+08 -5.5007885e+08  8.2679046e+08 -7.6846288e+07\n",
            "  1.1832602e+08]) = 1.923578598742393e+21\n",
            "Loss[85]([-1.05666394e+09  7.04453760e+08 -1.05882054e+09  9.84122880e+07\n",
            " -1.51532800e+08]) = 3.1547414211505244e+21\n",
            "Loss[86]([ 1.3532036e+09 -9.0215258e+08  1.3559698e+09 -1.2603021e+08\n",
            "  1.9405843e+08]) = 5.173898507409718e+21\n",
            "Loss[87]([-1.7329665e+09  1.1553329e+09 -1.7365048e+09  1.6139955e+08\n",
            " -2.4851843e+08]) = 8.485375610166874e+21\n",
            "Loss[88]([ 2.2193060e+09 -1.4795643e+09  2.2238349e+09 -2.0669574e+08\n",
            "  3.1826166e+08]) = 1.3916325510558064e+22\n",
            "Loss[89]([-2.8421294e+09  1.8947896e+09 -2.8479255e+09  2.6470224e+08\n",
            " -4.0757808e+08]) = 2.2823254371395466e+22\n",
            "Loss[90]([ 3.6397404e+09 -2.4265380e+09  3.6471642e+09 -3.3898819e+08\n",
            "  5.2195965e+08]) = 3.7430934216150616e+22\n",
            "Loss[91]([-4.6611917e+09  3.1075133e+09 -4.6707036e+09  4.3412170e+08\n",
            " -6.6844173e+08]) = 6.138806662112859e+22\n",
            "Loss[92]([ 5.9693076e+09 -3.9795994e+09  5.9814871e+09 -5.5595302e+08\n",
            "  8.5603187e+08]) = 1.0067869925300707e+23\n",
            "Loss[93]([-7.6445164e+09  5.0964342e+09 -7.6601262e+09  7.1197491e+08\n",
            " -1.0962688e+09]) = 1.6511653598779655e+23\n",
            "Loss[94]([ 9.7898680e+09 -6.5266959e+09  9.8098627e+09 -9.1178394e+08\n",
            "  1.4039235e+09]) = 2.7079737433972478e+23\n",
            "Loss[95]([-1.2537278e+10  8.3583370e+09 -1.2562883e+10  1.1676667e+09\n",
            " -1.7979197e+09]) = 4.441168207320095e+23\n",
            "Loss[96]([ 1.6055724e+10 -1.0704005e+10  1.6088525e+10 -1.4953585e+09\n",
            "  2.3024832e+09]) = 7.283678162529767e+23\n",
            "Loss[97]([-2.0561586e+10  1.3707952e+10 -2.0603582e+10  1.9150088e+09\n",
            " -2.9486497e+09]) = 1.1945485281642116e+24\n",
            "Loss[98]([ 2.6331965e+10 -1.7554950e+10  2.6385705e+10 -2.4524332e+09\n",
            "  3.7761580e+09]) = 1.959100425551304e+24\n",
            "Loss[99]([-3.3721686e+10  2.2481537e+10 -3.3790540e+10  3.1406830e+09\n",
            " -4.8358902e+09]) = 3.2129887426937194e+24\n",
            "Loss[100]([ 4.3185308e+10 -2.8790733e+10  4.3273478e+10 -4.0220787e+09\n",
            "  6.1930240e+09]) = 5.2694167840513e+24\n",
            "Loss[101]([-5.5304765e+10  3.6870545e+10 -5.5417709e+10  5.1508275e+09\n",
            " -7.9310244e+09]) = 8.642032697204608e+24\n",
            "Loss[102]([ 7.0825361e+10 -4.7217877e+10  7.0970122e+10 -6.5963479e+09\n",
            "  1.0156764e+10]) = 1.417324452022849e+25\n",
            "Loss[103]([-9.0701840e+10  6.0469027e+10 -9.0887037e+10  8.4475402e+09\n",
            " -1.3007140e+10]) = 2.324464536187502e+25\n",
            "Loss[104]([ 1.1615622e+11 -7.7439050e+10  1.1639355e+11 -1.0818268e+10\n",
            "  1.6657460e+10]) = 3.8122059749472233e+25\n",
            "Loss[105]([-1.4875419e+11  9.9171328e+10 -1.4905806e+11  1.3854289e+10\n",
            " -2.1332169e+10]) = 6.252149896047758e+25\n",
            "Loss[106]([ 1.90500291e+11 -1.27002665e+11  1.90889591e+11 -1.77423606e+10\n",
            "  2.73188332e+10]) = 1.0253750898242767e+26\n",
            "Loss[107]([-2.4396213e+11  1.6264436e+11 -2.4446049e+11  2.2721516e+10\n",
            " -3.4985583e+10]) = 1.681650568749784e+26\n",
            "Loss[108]([ 3.1242728e+11 -2.0828835e+11  3.1306534e+11 -2.9098017e+10\n",
            "  4.4803924e+10]) = 2.757962929685956e+26\n",
            "Loss[109]([-4.0010619e+11  2.6674204e+11 -4.0092316e+11  3.7264065e+10\n",
            " -5.7377653e+10]) = 4.523150132375856e+26\n",
            "Loss[110]([ 5.1239164e+11 -3.4160053e+11  5.1343760e+11 -4.7721734e+10\n",
            "  7.3480061e+10]) = 7.418130865355308e+26\n",
            "Loss[111]([-6.5618955e+11  4.3746705e+11 -6.5752714e+11  6.1114327e+10\n",
            " -9.4101324e+10]) = 1.2165998865102212e+27\n",
            "Loss[112]([ 8.4034204e+11 -5.6023790e+11  8.4205502e+11 -7.8265360e+10\n",
            "  1.2050981e+11]) = 1.9952687560482845e+27\n",
            "Loss[113]([-1.07617347e+12  7.17462438e+11 -1.07836762e+12  1.00230095e+11\n",
            " -1.54329547e+11]) = 3.272308366498347e+27\n",
            "Loss[114]([ 1.3781890e+12 -9.1881039e+11  1.3810013e+12 -1.2835852e+11\n",
            "  1.9764009e+11]) = 5.366707416441553e+27\n",
            "Loss[115]([-1.7649613e+12  1.1766638e+12 -1.7685645e+12  1.6438107e+11\n",
            " -2.5310560e+11]) = 8.80158856177498e+27\n",
            "Loss[116]([ 2.2602766e+12 -1.5068834e+12  2.2648966e+12 -2.1051225e+11\n",
            "  3.2413647e+11]) = 1.4434941450192715e+28\n",
            "Loss[117]([-2.8945990e+12  1.9297728e+12 -2.9005140e+12  2.6959017e+11\n",
            " -4.1510181e+11]) = 2.3673820557985614e+28\n",
            "Loss[118]([ 3.7069366e+12 -2.4713429e+12  3.7145105e+12 -3.4524722e+11\n",
            "  5.3159566e+11]) = 3.8825937180607156e+28\n",
            "Loss[119]([-4.7472527e+12  3.1648936e+12 -4.7569416e+12  4.4213656e+11\n",
            " -6.8078207e+11]) = 6.367588908416711e+28\n",
            "Loss[120]([ 6.0795136e+12 -4.0530826e+12  6.0919393e+12 -5.6621846e+11\n",
            "  8.7183655e+11]) = 1.0443087519409466e+29\n",
            "Loss[121]([-7.7856632e+12  5.1905319e+12 -7.8015753e+12  7.2512097e+11\n",
            " -1.1165084e+12]) = 1.712704287008636e+29\n",
            "Loss[122]([ 9.9706208e+12 -6.6471907e+12  9.9909999e+12 -9.2861746e+11\n",
            "  1.4298468e+12]) = 2.808895507208289e+29\n",
            "Loss[123]([-1.2768750e+13  8.5126556e+12 -1.2794850e+13  1.1892225e+12\n",
            " -1.8311187e+12]) = 4.6066821044548096e+29\n",
            "Loss[124]([ 1.6352152e+13 -1.0901647e+13  1.6385596e+13 -1.5229638e+12\n",
            "  2.3450002e+12]) = 7.555130530334291e+29\n",
            "Loss[125]([-2.0941212e+13  1.3961082e+13 -2.0984000e+13  1.9503645e+12\n",
            " -3.0030981e+12]) = 1.2390672870543072e+30\n",
            "Loss[126]([ 2.6818145e+13 -1.7879114e+13  2.6872929e+13 -2.4977117e+12\n",
            "  3.8458795e+12]) = 2.0321166556502696e+30\n",
            "Loss[127]([-3.4344379e+13  2.2896685e+13 -3.4414522e+13  3.1986717e+12\n",
            " -4.9251830e+12]) = 3.332746147774676e+30\n",
            "Loss[128]([ 4.3982797e+13 -2.9322375e+13  4.4072555e+13 -4.0963476e+12\n",
            "  6.3073823e+12]) = 5.465825034324242e+30\n",
            "Loss[129]([-5.6326080e+13  3.7551356e+13 -5.6441012e+13  5.2459424e+12\n",
            " -8.0774730e+12]) = 8.964146871279157e+30\n",
            "Loss[130]([ 7.2133372e+13 -4.8089721e+13  7.2280625e+13 -6.7181609e+12\n",
            "  1.0344332e+13]) = 1.4701536539240892e+31\n",
            "Loss[131]([-9.2376844e+13  6.1585641e+13 -9.2565521e+13  8.6035289e+12\n",
            " -1.3247365e+13]) = 2.4111101852887593e+31\n",
            "Loss[132]([ 1.18301301e+14 -7.88689710e+13  1.18542876e+14 -1.10180423e+13\n",
            "  1.69650747e+13]) = 3.9543010926048664e+31\n",
            "Loss[133]([-1.5150123e+14  1.0100263e+14 -1.5181067e+14  1.4110123e+13\n",
            " -2.1726126e+13]) = 6.485196483804322e+31\n",
            "Loss[134]([ 1.9401812e+14 -1.2934796e+14  1.9441490e+14 -1.8069936e+13\n",
            "  2.7823335e+13]) = 1.0635959144214106e+32\n",
            "Loss[135]([-2.4846686e+14  1.6564821e+14 -2.4897508e+14  2.3140996e+13\n",
            " -3.5631623e+13]) = 1.744333145752376e+32\n",
            "Loss[136]([ 3.1819641e+14 -2.1213546e+14  3.1884703e+14 -2.9635306e+13\n",
            "  4.5631171e+13]) = 2.8607685964693715e+32\n",
            "Loss[137]([-4.0749462e+14  2.7166874e+14 -4.0832784e+14  3.7952177e+13\n",
            " -5.8436977e+13]) = 4.69176025296556e+32\n",
            "Loss[138]([ 5.2185249e+14 -3.4790926e+14  5.2292119e+14 -4.8602932e+13\n",
            "  7.4836651e+13]) = 7.694649201648172e+32\n",
            "Loss[139]([-6.683051e+14  4.455458e+14 -6.696726e+14  6.224269e+13 -9.583862e+13]) = 1.2619495768623104e+33\n",
            "Loss[140]([ 8.5585719e+14 -5.7058325e+14  8.5760746e+14 -7.9710608e+13\n",
            "  1.2273480e+14]) = 2.0696393774464242e+33\n",
            "Loss[141]([-1.09604384e+15  7.30711443e+14 -1.09828575e+15  1.02080509e+14\n",
            " -1.57178824e+14]) = 3.3942845823408476e+33\n",
            "Loss[142]([ 1.4036389e+15 -9.3577815e+14  1.4065086e+15 -1.3072806e+14\n",
            "  2.0128948e+14]) = 5.566761959988282e+33\n",
            "Loss[143]([-1.7975540e+15  1.1983939e+15 -1.8012298e+15  1.6741538e+14\n",
            " -2.5777921e+14]) = 9.129687709545869e+33\n",
            "Loss[144]([ 2.3020144e+15 -1.5347091e+15  2.3067271e+15 -2.1439890e+14\n",
            "  3.3012223e+14]) = 1.4973011045040682e+34\n",
            "Loss[145]([-2.9480505e+15  1.9654068e+15 -2.9540823e+15  2.7456746e+14\n",
            " -4.2276742e+14]) = 2.4556261415880124e+34\n",
            "Loss[146]([ 3.7753879e+15 -2.5169778e+15  3.7831135e+15 -3.5162246e+14\n",
            "  5.4141274e+14]) = 4.0273163009927784e+34\n",
            "Loss[147]([-4.8349043e+15  3.2233426e+15 -4.8448095e+15  4.5030202e+14\n",
            " -6.9335469e+14]) = 6.6049493562686165e+34\n",
            "Loss[148]([ 6.1917714e+15 -4.1279347e+15  6.2044469e+15 -5.7667224e+14\n",
            "  8.8793644e+14]) = 1.0832350687166989e+35\n",
            "Loss[149]([-7.9294336e+15  5.2863896e+15 -7.9456535e+15  7.3850808e+14\n",
            " -1.1371256e+15]) = 1.7765459626337578e+35\n",
            "Loss[150]([ 1.0154742e+16 -6.7699508e+15  1.0175510e+16 -9.4576146e+14\n",
            "  1.4562492e+15]) = 2.9135972559142864e+35\n",
            "Loss[151]([-1.3004551e+16  8.6698575e+15 -1.3031139e+16  1.2111780e+15\n",
            " -1.8649322e+15]) = 4.778398241758371e+35\n",
            "Loss[152]([ 1.6654142e+16 -1.1102965e+16  1.6688192e+16 -1.5510838e+15\n",
            "  2.3883043e+15]) = 7.836752283050439e+35\n",
            "Loss[153]([-2.1327934e+16  1.4218884e+16 -2.1371549e+16  1.9863769e+15\n",
            " -3.0585581e+15]) = 1.285255097938899e+36\n",
            "Loss[154]([ 2.7313366e+16 -1.8209257e+16  2.7369196e+16 -2.5438265e+15\n",
            "  3.9169063e+15]) = 2.107861143827552e+36\n",
            "Loss[155]([-3.4978508e+16  2.3319465e+16 -3.5050075e+16  3.2577311e+15\n",
            " -5.0161374e+15]) = 3.456963682635496e+36\n",
            "Loss[156]([ 4.4794860e+16 -2.9863771e+16  4.4886497e+16 -4.1719643e+15\n",
            "  6.4238516e+15]) = 5.669546393285852e+36\n",
            "Loss[157]([-5.7366091e+16  3.8244725e+16 -5.7483353e+16  5.3427799e+15\n",
            " -8.2266276e+15]) = 9.29826088661977e+36\n",
            "Loss[158]([ 7.3465235e+16 -4.8977737e+16  7.3615525e+16 -6.8421798e+15\n",
            "  1.0535346e+16]) = 1.524950586393894e+37\n",
            "Loss[159]([-9.4082384e+16  6.2722800e+16 -9.4274987e+16  8.7623512e+15\n",
            " -1.3491987e+16]) = 2.5009742363227585e+37\n",
            "Loss[160]([ 1.2048561e+17 -8.0325209e+16  1.2073219e+17 -1.1221386e+16\n",
            "  1.7278366e+16]) = 4.101688369375433e+37\n",
            "Loss[161]([-1.5429859e+17  1.0286777e+17 -1.5461429e+17  1.4370544e+16\n",
            " -2.2127349e+16]) = 6.726918731653043e+37\n",
            "Loss[162]([ 1.9760079e+17 -1.3173645e+17  1.9800514e+17 -1.8403499e+16\n",
            "  2.8337116e+16]) = 1.1032395625641646e+38\n",
            "Loss[163]([-2.5305501e+17  1.6870676e+17 -2.5357281e+17  2.3568214e+16\n",
            " -3.6289609e+16]) = 1.8093489366165414e+38\n",
            "Loss[164]([ 3.2407191e+17 -2.1605232e+17  3.2473519e+17 -3.0182408e+16\n",
            "  4.6473844e+16]) = 2.9673956264116936e+38\n",
            "Loss[165]([-4.1501884e+17  2.7668519e+17 -4.1586787e+17  3.8652807e+16\n",
            " -5.9516311e+16]) = inf\n",
            "Loss[166]([nan nan nan nan nan]) = nan\n",
            "ols beta = [-0.07717529 -0.05436444  1.5243394   1.86216     1.3811557 ]\n"
          ]
        }
      ]
    }
  ]
}