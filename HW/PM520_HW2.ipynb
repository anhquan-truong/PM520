{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhquan-truong/PM520/blob/main/HW/PM520_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2. Maximum likelihood & Optimization Crash Course"
      ],
      "metadata": {
        "id": "SB19uPlEpw4u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0RgUYaXfIWf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install lineax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.numpy.linalg as jnpla\n",
        "import jax.scipy as jsp\n",
        "import jax.scipy.linalg as jspla"
      ],
      "metadata": {
        "id": "0aNnRcuZfTXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Ordinary least squares (i.e. OLS)\n",
        "OLS is an approach to fit a linear regression model $$y = X \\beta + ɛ,$$\n",
        "such that $\\mathbb{E}[ɛ'ɛ]$ is minimized, where $\\mathbb{E}[ɛ_i]=0$ and\n",
        "$\\mathbb{V}[ɛ_i] = \\sigma^2$, for each $i=1,\\dotsc,n$.\n",
        "\n",
        "**1.1 Derive the OLS solution $\\hat{\\beta}$ under the above objective. Show step by step.**\n",
        "\n",
        "We have the residual sum of square:\n",
        "$$\n",
        "\\begin{align*}\n",
        "RSS(\\hat{\\beta}) &= ɛ'ɛ \\\\\n",
        "&= (y - X \\hat{\\beta})^T (y - X \\hat{\\beta})\\\\\n",
        "&= (y^T - \\hat{\\beta}^TX^T)(y - X \\hat{\\beta}) \\\\\n",
        "&= (y^Ty - y^T X \\hat{\\beta} - \\hat{\\beta}^TX^Ty +\\hat{\\beta}^TX^TX\\hat{\\beta})\\\\\n",
        "&= (y^Ty - 2 \\hat{\\beta}^TX^Ty + \\hat{\\beta}^TX^TX\\hat{\\beta})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "$\\quad$\n",
        "\n",
        "To find the OLS solution, we find$\\hat{\\beta}$ minizing the loss function, or the RSS.\n",
        "\n",
        "This equals to find the stationary points at which the derivation with respect to $\\hat{\\beta}$, or the gradient, is zero.\n",
        "\n",
        "We first express the RSS in terms of its cordinates\n",
        "$$RSS(\\hat{\\beta}) = \\sum(y_i^2-2\\hat{\\beta}_iX_{ji}y_i+\\hat{\\beta}_i^2X_{ji}X_{ij})$$\n",
        "\n",
        "Then we take the derivatives with respect to each $\\hat{\\beta}_k$\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial RSS(\\hat{\\beta})} {\\partial \\hat{\\beta}_k} &= 0-2\\sum(X_{jk} y_{k}) + 2\\sum(\\hat{\\beta}_k X_{jk}X_{kj}) \\\\\n",
        "&= -2\\sum(X_{jk} y_{k} - \\hat{\\beta}_k X_{jk}X_{kj}) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Hence, the gradient with repect to $\\hat{\\beta}$ is\n",
        "$$ \\frac{\\partial RSS(\\hat{\\beta})} {\\partial \\hat{\\beta}} = \\nabla_{\\hat{\\beta}}RSS(\\hat{\\beta}) = -2(X^Ty - X^TX\\hat{\\beta})$$\n",
        "\n",
        "Set the gradient equal to 0, we have\n",
        "\n",
        "$$\\hat{\\beta}=(X^TX)^{-1}X^Ty$$\n",
        "\n",
        "This is the solution of the OLS\n",
        "\n",
        "**1.2 Re-write the objective using a likelihood formulation assuming $ɛ_i \\sim N(0, \\sigma^2)$, for each $i=1,\\dotsc,n$.**\n",
        "\n",
        "For each $i$ observation, we have\n",
        "$$y_i = x^T_i \\beta + ɛ_i$$\n",
        "\n",
        "The expectation and variance of $y_i$ given $x_i$ is\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E[y_i|x_i]} & =\\mathbb{E[x^T_i \\beta + ɛ_i]}\\\\\n",
        "&=\\mathbb{E[x^T_i \\beta]} + \\mathbb{E[ɛ_i]}\\\\\n",
        "&=x^T_i \\beta + 0\\\\\n",
        "&=x^T_i \\beta\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{Var[y_i|x_i]} & =\\mathbb{Var[x^T_i \\beta + ɛ_i]}\\\\\n",
        "&=\\mathbb{Var[x^T_i \\beta]} + \\mathbb{Var[ɛ_i]}\\\\\n",
        "&=0 + \\sigma^2\\\\\n",
        "&=\\sigma^2\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Then we have $$y_i|x_i \\sim \\mathcal{N}(x_i^T\\beta, \\sigma^2)$$\n",
        "\n",
        "The likelihood of the data given x is,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "L(\\beta, \\sigma^2 |y_1,…,y_i) &=\n",
        "  \\prod_{i=1}^n N(y_i | x^T\\beta, \\sigma^2) \\\\\n",
        "  &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}\\right)\\\\\n",
        "  &= \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i^T\\beta)^2\\right).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Thus, our _log_-likelihood is given by,\n",
        "$$\\begin{align*}\n",
        "\\ell(\\beta, \\sigma^2 |y_1,…,y_i) &=\n",
        "  \\log \\left[\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i^T\\beta)^2\\right)\\right]\\\\\n",
        "  &= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i^T\\beta)^2.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "**1.3 Derive the OLS solution $\\hat{\\beta}_{MLE}$ using the above objective. Show step by step.**\n",
        "\n",
        "We maximize the _log_-likelihood to find $\\hat{\\beta}_{MLE}$ such that the error is minimized\n",
        "\n",
        "First, we take the derivative of the _log_-likelihood with respect to $\\beta$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\beta}\\ell(\\beta, \\sigma^2|y_1,…,y_i) =\n",
        "  -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n 2 x_i (y_i - x_i^T\\beta) =-\\frac{1}{\\sigma^2} (X^Ty - X^TX\\beta)\n",
        "$$\n",
        "\n",
        "Set the gradient to zero gives\n",
        "\n",
        "$$\n",
        "\\hat{\\beta}_{MLE} = (X^TX)^{-1}X^Ty\\\\\n",
        "$$\n",
        "**1.4 Using [lineax](https://docs.kidger.site/lineax/), implement a solver for OLS.**"
      ],
      "metadata": {
        "id": "CtLu4_XIfaC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lineax as lx\n",
        "\n",
        "from jax import Array\n",
        "from jax.typing import ArrayLike\n",
        "import jax.random as rdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "seed = 27102000\n",
        "key = rdm.PRNGKey(seed)\n",
        "\n",
        "key, y_key, x_key, b_key = rdm.split(key, num = 4)\n",
        "N = 1000\n",
        "D = 5\n",
        "\n",
        "X = rdm.normal(x_key, shape = (N,D))\n",
        "beta = rdm.normal(b_key, shape = (D,))\n",
        "s2e = 0.04\n",
        "y = X @ beta + jnp.sqrt(s2e) * rdm.normal(y_key, shape = (N,))\n",
        "\n",
        "def solve_ols(y: ArrayLike, X: ArrayLike) -> Array:\n",
        "  r\"\"\"\n",
        "  Solves ordinary least squares using lineax.\n",
        "\n",
        "  y: ArrayLike of observations\n",
        "  X: ArrayLike of covariates\n",
        "\n",
        "  returns: $\\hat{\\beta}$ for OLS\n",
        "  \"\"\"\n",
        "  X_star = lx.MatrixLinearOperator(X)\n",
        "  solution = lx.linear_solve(X_star, y, solver=lx.Normal(lx.CG(atol=1e-6, rtol=1e-6)))\n",
        "  beta_hat = solution.value\n",
        "  return solution.value\n",
        "\n",
        "solve_ols(y, X)\n",
        "print(f\"beta hat: {solve_ols(y, X)}\")\n",
        "print(f\"True beta: {beta}\")\n"
      ],
      "metadata": {
        "id": "f4f3welGhtMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9ad21d-ef91-4185-ff37-f2a145aa7ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beta hat: [-2.4730766   0.47487995  1.2987322   1.4924115   0.7521332 ]\n",
            "True beta: [-2.4790854   0.46948895  1.2997216   1.5003587   0.7489997 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Weighted least squares (i.e. WLS)\n",
        "WLS is an approach to fit a slightly more general linear model where, $$y = X \\beta + ɛ,$$ where $\\mathbb{E}[ɛ_i] = 0$ and $\\mathbb{V}[ɛ_i] = \\sigma^2_i$. We can model all variances jointly as $\\mathbb{V}[ɛ] = D$ where $D$ is a diagonal matrix such that $D_{ii} = \\sigma^2_i$.\n",
        "\n",
        "2.1 Write the WLS objective.\n",
        "\n",
        "In weighted linear regresion, each residual is weighted against it variance. We have $D$ is a diagonal matrix of the variance, then residual is weighted with a matrix $W = D^{-1}$.\n",
        "\n",
        "The objective of WLS is to minimize the weighted residual sum of square (WRSS). WRSS given by\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "WRSS(\\beta) &= (y-X\\beta)^TW(y-X\\beta) \\\\\n",
        "&= (y^TW -\\beta^TX^TW)(y-X\\beta) \\\\\n",
        "&= (y^TWy -y^TWX\\beta - \\beta^TX^TWy + \\beta^TX^TWX\\beta) \\\\\n",
        "&= (y^TWy -2\\beta^TX^TWy + (X\\beta)^TWX\\beta)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "The WLS objective is\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\hat{\\beta} = \\arg\\min_\\beta\\;WRSS(\\beta)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "\n",
        "**2.2. Derive the WLS solution $\\hat{\\beta}$ under the above objective. Show step by step.**\n",
        "\n",
        "To find $\\hat{\\beta}$, we need to differentiate the WRSS and set it to zero.\n",
        "\n",
        "By applying calculus\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "&\\frac{\\partial}{\\partial w} (w^TX) = w \\quad \\text{(derivative of a linear function is its slope}) \\\\\n",
        "&\\frac{\\partial}{\\partial w} (w^TXw) = (X + X^T)w \\quad \\text{(derivative of a quadratic function}) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "We can show that\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla_\\beta\\;WRSS(\\beta) &= \\frac{\\partial}{\\partial \\beta} \\left(y^TWy -2\\beta^TX^TWy + \\beta^TX^TWX\\beta \\right) \\\\\n",
        "& = -2X^TWy +2 X^TWX\\beta  \n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Hence,\n",
        "\n",
        "$$\\hat{\\beta} = (X^TWX)^{-1}X^TWy$$\n",
        "\n",
        "\n",
        "**2.3. Re-write the objective using a likelihood formulation assuming $ɛ \\sim N(0, D)$.**\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "L(\\beta, D|y_1,…,y_i) &=\n",
        "  \\prod_{i=1}^n N(y_i | x^T\\beta, \\sigma^2_i) \\\\\n",
        "  &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2_i}} \\exp\\left(-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2_i}\\right)\\\\\n",
        "  &= \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2_i}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2_i} \\sum_{i=1}^n (y_i - x_i^T\\beta)^2\\right)\\\\\n",
        "  &= \\left(\\frac{1}{\\sqrt{2\\pi D_{ii}}}\\right)^n \\exp\\left(-\\frac{1}{2 D_{ii}} \\sum_{i=1}^n (y_i - x_i^T\\beta)^2\\right).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "2.4 Derive the OLS solution $\\hat{\\beta}_{MLE}$ using the above objective. Show step by step.\n",
        "\n",
        "2.5 Using [lineax](https://docs.kidger.site/lineax/), implement a solver for WLS."
      ],
      "metadata": {
        "id": "4sbheXNtiYy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lineax as lx\n",
        "\n",
        "from jax import Array\n",
        "from jax.typing import ArrayLike\n",
        "\n",
        "\n",
        "def solve_wls(y: ArrayLike, X: ArrayLike, D: ArrayLike) -> Array:\n",
        "  \"\"\"\n",
        "  Solves weighted least squares using lineax.\n",
        "\n",
        "  y: ArrayLike of observations\n",
        "  X: ArrayLike of covariates\n",
        "  D: ArrayLike of weights per observation\n",
        "\n",
        "  returns: $\\hat{\\beta}$ for WLS\n",
        "  \"\"\"\n",
        "  pass"
      ],
      "metadata": {
        "id": "HELjji9HjffX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. MLE for scalar Poisson observations\n",
        "Given $x_1, \\dotsc, x_n$, assume that $x_i \\sim \\text{Poi}(\\lambda)$ for $i=1,\\dotsc,n$ where $\\text{Poi}(\\lambda)$ is the Poisson distribution with rate $\\lambda$.\n",
        "\n",
        "3.1 Write a likelihood-based formulation of the objective.\n",
        "\n",
        "3.2 Derive the MLE for the above objective. Show step by step.\n",
        "\n",
        "3.3 Implement a function that simulates Poisson distributed data with rate $\\lambda$ using JAX.\n",
        "\n",
        "3.4 Implement a function that computes the MLE $\\hat{\\lambda}$ given observations $x_1, \\dotsc, x_n$."
      ],
      "metadata": {
        "id": "VXwMNL3fkDsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lineax as lx\n",
        "import jax.random as rdm\n",
        "\n",
        "from jax import Array\n",
        "from jax.typing import ArrayLike\n",
        "\n",
        "\n",
        "def simulate_poisson(key, rate: ArrayLike, n: int) -> Array:\n",
        "  \"\"\"\n",
        "  Simulates Poisson distributed data.\n",
        "\n",
        "  key: PRNGKey to generate\n",
        "  rate: rate specifying the Poisson distribution; can be either a scalar, or\n",
        "    ArrayLike (i.e. unique to each observation)\n",
        "  n: the number of samples to generate\n",
        "\n",
        "  returns: $x_i \\sim \\text{Poi}(\\lambda_i)$\n",
        "  \"\"\"\n",
        "  pass\n",
        "\n",
        "\n",
        "def fit_poisson(x: ArrayLike) -> float:\n",
        "  \"\"\"\n",
        "  Fits Poisson distributed data.\n",
        "\n",
        "  x: ArrayLike observations\n",
        "\n",
        "  returns: estimate of $\\lambda$.\n",
        "  \"\"\"\n",
        "  pass"
      ],
      "metadata": {
        "id": "LY1UCqDBlF7l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}