{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhquan-truong/PM520/blob/main/HW/PM520_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2. Maximum likelihood & Optimization Crash Course"
      ],
      "metadata": {
        "id": "SB19uPlEpw4u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0RgUYaXfIWf",
        "collapsed": true,
        "outputId": "591b4a64-5a25-45e3-84a0-1d09b1287555",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lineax\n",
            "  Downloading lineax-0.1.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting equinox>=0.11.10 (from lineax)\n",
            "  Downloading equinox-0.13.4-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: jax>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from lineax) (0.7.2)\n",
            "Collecting jaxtyping>=0.2.24 (from lineax)\n",
            "  Downloading jaxtyping-0.3.7-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from lineax) (4.15.0)\n",
            "Collecting wadler-lindig>=0.1.0 (from equinox>=0.11.10->lineax)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jaxlib<=0.7.2,>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.1->lineax) (0.7.2)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.1->lineax) (0.5.4)\n",
            "Requirement already satisfied: numpy>=2.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.1->lineax) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.1->lineax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.1->lineax) (1.16.3)\n",
            "Downloading lineax-0.1.0-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading equinox-0.13.4-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, jaxtyping, equinox, lineax\n",
            "Successfully installed equinox-0.13.4 jaxtyping-0.3.7 lineax-0.1.0 wadler-lindig-0.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install lineax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.numpy.linalg as jnpla\n",
        "import jax.scipy as jsp\n",
        "import jax.scipy.linalg as jspla"
      ],
      "metadata": {
        "id": "0aNnRcuZfTXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Ordinary least squares (i.e. OLS)\n",
        "OLS is an approach to fit a linear regression model $$y = X \\beta + ɛ,$$\n",
        "such that $\\mathbb{E}[ɛ'ɛ]$ is minimized, where $\\mathbb{E}[ɛ_i]=0$ and\n",
        "$\\mathbb{V}[ɛ_i] = \\sigma^2$, for each $i=1,\\dotsc,n$.\n",
        "\n",
        "1.1 Derive the OLS solution $\\hat{\\beta}$ under the above objective. Show step by step.\n",
        "\n",
        "We have the residual sum of square:\n",
        "$$\n",
        "\\begin{align*}\n",
        "RSS(\\hat{\\beta}) &= ɛ'ɛ \\\\\n",
        "&= (y - X \\hat{\\beta})^2 \\\\\n",
        "&= (y - X \\hat{\\beta})^T (y - X \\hat{\\beta})\\\\\n",
        "&= (y^T - \\hat{\\beta}^TX^T)(y - X \\hat{\\beta}) \\\\\n",
        "&= (y^Ty - y^T X \\hat{\\beta} - \\hat{\\beta}^TX^Ty +\\hat{\\beta}^TX^TX\\hat{\\beta})\\\\\n",
        "&= (y^Ty - 2 \\hat{\\beta}^TX^Ty + \\hat{\\beta}^TX^TX\\hat{\\beta})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "$\\quad$\n",
        "\n",
        "To find the OLS solution, we find$\\hat{\\beta}$ minizing the loss function, or the RSS.\n",
        "\n",
        "This equals to find the stationary points at which the derivation with respect to $\\hat{\\beta}$, or the gradient, is zero.\n",
        "\n",
        "We first express the RSS in terms of its cordinates\n",
        "$$RSS(\\hat{\\beta}) = \\sum(y_i^2-2\\hat{\\beta}_iX_{ji}y_i+\\hat{\\beta}_i^2X_{ji}X_{ij})$$\n",
        "\n",
        "Then we take the derivatives with respect to each $\\hat{\\beta}_k$\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial RSS(\\hat{\\beta})} {\\partial \\hat{\\beta}_k} &= 0-2\\sum(X_{jk} y_{k}) + 2\\sum(\\hat{\\beta}_k X_{jk}X_{kj}) \\\\\n",
        "&= -2\\sum(X_{jk} y_{k} - \\hat{\\beta}_k X_{jk}X_{kj}) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Hence, the gradient with repect to $\\hat{\\beta}$ is\n",
        "$$ \\frac{\\partial RSS(\\hat{\\beta})} {\\partial \\hat{\\beta}} = \\nabla_{\\hat{\\beta}}RSS(\\hat{\\beta}) = -2(X^Ty - X^TX\\hat{\\beta})$$\n",
        "\n",
        "Set the gradient equal to 0, we have\n",
        "\n",
        "$$\\hat{\\beta}=(X^TX)^{-1}X^Ty$$\n",
        "\n",
        "This is the solution of the OLS\n",
        "\n",
        "1.2 Re-write the objective using a likelihood formulation assuming $ɛ_i \\sim N(0, \\sigma^2)$, for each $i=1,\\dotsc,n$.\n",
        "\n",
        "For each $i$ observation, we have\n",
        "$$y_i = x^T_i \\beta + ɛ_i$$\n",
        "\n",
        "The expectation and variance of $y_i$ given $x_i$ is\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E[y_i|x_i]} & =\\mathbb{E[x^T_i \\beta + ɛ_i]}\\\\\n",
        "&=\\mathbb{E[x^T_i \\beta]} + \\mathbb{E[ɛ_i]}\\\\\n",
        "&=x^T_i \\beta + 0\\\\\n",
        "&=x^T_i \\beta\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{Var[y_i|x_i]} & =\\mathbb{Var[x^T_i \\beta + ɛ_i]}\\\\\n",
        "&=\\mathbb{Var[x^T_i \\beta]} + \\mathbb{Var[ɛ_i]}\\\\\n",
        "&=0 + \\sigma^2\\\\\n",
        "&=\\sigma^2\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Then we have $$y_i|x_i \\sim \\mathcal{N}(x_i^T\\beta, \\sigma^2)$$\n",
        "\n",
        "The likelihood of the data given x is,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "L(\\beta, \\sigma^2 | y_i) &=\n",
        "  \\prod_{i=1}^n N(y_i | x_i^T\\beta, \\sigma^2) \\\\\n",
        "  &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}\\right)\\\\\n",
        "  &= \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i^T\\beta)^2\\right).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Thus, our _log_-likelihood is given by,\n",
        "$$\\begin{align*}\n",
        "\\ell(\\beta, \\sigma^2 | y_i) &=\n",
        "  \\log \\left[\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i^T\\beta)^2\\right)\\right]\\\\\n",
        "  &= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i^T\\beta)^2.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "1.3 Derive the OLS solution $\\hat{\\beta}_{MLE}$ using the above objective. Show step by step.\n",
        "\n",
        "We maximize the _log_-likelihood to find $\\hat{\\beta}_{MLE}$ such that the error is minimized\n",
        "\n",
        "First, we take the derivative of the _log_-likelihood with respect to $\\beta$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\beta}\\ell(\\beta, \\sigma^2 | y_i) =\n",
        "  -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n 2 x_i (y_i - x_i^T\\beta) =-\\frac{1}{\\sigma^2} (X^Ty - X^TX\\beta)\n",
        "$$\n",
        "\n",
        "Set the gradient to zero gives\n",
        "\n",
        "$$\n",
        "\\hat{\\beta}_{MLE} = (X^TX)^{-1}X^Ty\\\\\n",
        "$$\n",
        "1.4 Using [lineax](https://docs.kidger.site/lineax/), implement a solver for OLS."
      ],
      "metadata": {
        "id": "CtLu4_XIfaC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lineax as lx\n",
        "\n",
        "from jax import Array\n",
        "from jax.typing import ArrayLike\n",
        "\n",
        "seed = 0\n",
        "key = rdm.PRNGKey(seed)\n",
        "\n",
        "key, y_key, x_key, b_key = rdm.split(key, num = 4)\n",
        "\n",
        "def solve_ols(y: ArrayLike, X: ArrayLike) -> Array:\n",
        "  r\"\"\"\n",
        "  Solves ordinary least squares using lineax.\n",
        "\n",
        "  y: ArrayLike of observations\n",
        "  X: ArrayLike of covariates\n",
        "\n",
        "  returns: $\\hat{\\beta}$ for OLS\n",
        "  \"\"\"\n",
        "  pass"
      ],
      "metadata": {
        "id": "f4f3welGhtMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Weighted least squares (i.e. WLS)\n",
        "WLS is an approach to fit a slightly more general linear model where, $$y = X \\beta + ɛ,$$ where $\\mathbb{E}[ɛ_i] = 0$ and $\\mathbb{V}[ɛ_i] = \\sigma^2_i$. We can model all variances jointly as $\\mathbb{V}[ɛ] = D$ where $D$ is a diagonal matrix such that $D_{ii} = \\sigma^2_i$.\n",
        "\n",
        "2.1 Write the WLS objective.\n",
        "\n",
        "2.2. Derive the WLS solution $\\hat{\\beta}$ under the above objective. Show step by step.\n",
        "\n",
        "2.3. Re-write the objective using a likelihood formulation assuming $ɛ \\sim N(0, D)$.\n",
        "\n",
        "2.4 Derive the OLS solution $\\hat{\\beta}_{MLE}$ using the above objective. Show step by step.\n",
        "\n",
        "2.5 Using [lineax](https://docs.kidger.site/lineax/), implement a solver for WLS."
      ],
      "metadata": {
        "id": "4sbheXNtiYy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lineax as lx\n",
        "\n",
        "from jax import Array\n",
        "from jax.typing import ArrayLike\n",
        "\n",
        "\n",
        "def solve_wls(y: ArrayLike, X: ArrayLike, D: ArrayLike) -> Array:\n",
        "  \"\"\"\n",
        "  Solves weighted least squares using lineax.\n",
        "\n",
        "  y: ArrayLike of observations\n",
        "  X: ArrayLike of covariates\n",
        "  D: ArrayLike of weights per observation\n",
        "\n",
        "  returns: $\\hat{\\beta}$ for WLS\n",
        "  \"\"\"\n",
        "  pass"
      ],
      "metadata": {
        "id": "HELjji9HjffX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. MLE for scalar Poisson observations\n",
        "Given $x_1, \\dotsc, x_n$, assume that $x_i \\sim \\text{Poi}(\\lambda)$ for $i=1,\\dotsc,n$ where $\\text{Poi}(\\lambda)$ is the Poisson distribution with rate $\\lambda$.\n",
        "\n",
        "3.1 Write a likelihood-based formulation of the objective.\n",
        "\n",
        "3.2 Derive the MLE for the above objective. Show step by step.\n",
        "\n",
        "3.3 Implement a function that simulates Poisson distributed data with rate $\\lambda$ using JAX.\n",
        "\n",
        "3.4 Implement a function that computes the MLE $\\hat{\\lambda}$ given observations $x_1, \\dotsc, x_n$."
      ],
      "metadata": {
        "id": "VXwMNL3fkDsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lineax as lx\n",
        "import jax.random as rdm\n",
        "\n",
        "from jax import Array\n",
        "from jax.typing import ArrayLike\n",
        "\n",
        "\n",
        "def simulate_poisson(key, rate: ArrayLike, n: int) -> Array:\n",
        "  \"\"\"\n",
        "  Simulates Poisson distributed data.\n",
        "\n",
        "  key: PRNGKey to generate\n",
        "  rate: rate specifying the Poisson distribution; can be either a scalar, or\n",
        "    ArrayLike (i.e. unique to each observation)\n",
        "  n: the number of samples to generate\n",
        "\n",
        "  returns: $x_i \\sim \\text{Poi}(\\lambda_i)$\n",
        "  \"\"\"\n",
        "  pass\n",
        "\n",
        "\n",
        "def fit_poisson(x: ArrayLike) -> float:\n",
        "  \"\"\"\n",
        "  Fits Poisson distributed data.\n",
        "\n",
        "  x: ArrayLike observations\n",
        "\n",
        "  returns: estimate of $\\lambda$.\n",
        "  \"\"\"\n",
        "  pass"
      ],
      "metadata": {
        "id": "LY1UCqDBlF7l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}