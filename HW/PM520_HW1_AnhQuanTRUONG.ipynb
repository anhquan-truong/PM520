{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhquan-truong/PM520/blob/main/HW/PM520_HW1_AnhQuanTRUONG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1. Linear regression and normal equations"
      ],
      "metadata": {
        "id": "r94B4C5cpi8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as rdm\n",
        "import jax.numpy.linalg as jnpla\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "XknGDH_1Kohu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Linear model simulation\n",
        "In class we defined a Python function that simulates $N$ $P\\times 1$ variables $X$ (i.e. an $N \\times P$ matrix $X$) and outcome $y$ as a linear function of $X$. Please include its definition here and use for problem 2."
      ],
      "metadata": {
        "id": "G7TX02bf92rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given $N \\times P$ matrix $X$, a $P \\times 1$ vector $\\beta$, and $N \\times 1$ outcome vector $y$, and a random variabl $\\epsilon$, with $\\mathbb{E}[\\epsilon]=0$ and $Var(\\epsilon) = \\sigma^2$. Then, we can describe the $y$ as a linear function of $X$ as\n",
        "\n",
        "$$\n",
        "y = X\\times \\beta + \\epsilon\\\\\n",
        "$$"
      ],
      "metadata": {
        "id": "GEreYqjJNudL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sim_linear_reg(key, N, P, r2=0.5):\n",
        "  key, b_key = rdm.split(key)\n",
        "  b = rdm.normal(b_key, shape=(P,))\n",
        "\n",
        "  key, X_key = rdm.split(key)\n",
        "  X = rdm.normal(X_key, shape = (N, P))\n",
        "\n",
        "  y_hat = X @ b # this is the predicted y without error eps\n",
        "  var_pred = jnp.var(y_hat)\n",
        "  var_tot = var_pred/r2 # var_tot = var_pred + var_eps\n",
        "  var_eps =( var_tot - var_pred)\n",
        "\n",
        "  key, e_key = rdm.split(key)\n",
        "  eps = rdm.normal(e_key, shape = (N,)) * jnp.sqrt(var_eps)\n",
        "  y = y_hat + eps\n",
        "  return y, y_hat, b, X, eps\n",
        "\n",
        "seed = 91227102000\n",
        "key = rdm.PRNGKey(seed) # creating key from seed\n",
        "\n",
        "N = 100 # rows\n",
        "P = 10 # cols - number of features\n",
        "\n",
        "y, y_hat, b, X, eps = sim_linear_reg(key, N, P, r2=0.5)\n",
        "\n",
        "#Double check eps\n",
        "print(f\"Mean of epsilon: {jnp.mean(eps)}\\n Var of epsilon: {jnp.var(eps)}\")\n"
      ],
      "metadata": {
        "id": "s1CpkFZh6Y7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a385db62-9b82-427f-fc08-031b71952425",
        "collapsed": true
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of epsilon: -0.1648550033569336\n",
            " Var of epsilon: 10.337937355041504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Just-in time decorator and ordinary least squares\n",
        "Complete the definition of `ordinary_least_squares` below, that estimates the effect and its standard error. `@jit` wraps a function to perform just-in-time compilation, which boosts computational performance/speed.\n",
        "\n",
        "Compare the times of with and without JIT\n",
        "Hint: use [`block_until_ready()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.block_until_ready.html) to get correct timing estimates."
      ],
      "metadata": {
        "id": "Bq2qsE137hUK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "r3OWo_ixyh7Y",
        "outputId": "d22e7033-6a5f-43fe-fc2f-a76558769750",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:9: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:9: SyntaxWarning: invalid escape sequence '\\h'\n",
            "/tmp/ipython-input-2632854808.py:9: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  Returns a tuple of $\\hat{beta}$ and $\\text{se}(\\hat{beta})$.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.6665306   1.2053609  -0.00617147  2.3890886   0.07242881  1.4051489\n",
            " -1.3037714  -0.69175446 -0.6884035   0.88761115]\n",
            "[-1.4279191   0.85496247 -0.6770563   2.2756033  -0.29681712  0.44081363\n",
            " -1.2529446  -0.41230848 -0.6117764   1.1318142 ]\n",
            "629 µs ± 218 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
            "85.4 µs ± 19.7 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "\n",
        "from jax import jit\n",
        "\n",
        "\n",
        "def ordinary_least_squares(X, y):\n",
        "  \"\"\"\n",
        "  computes the OLS solution to linear system y ~ X.\n",
        "  Returns a tuple of $\\hat{beta}$ and $\\text{se}(\\hat{beta})$.\n",
        "  \"\"\"\n",
        "\n",
        "  b_hat = jnpla.inv(X.T @ X) @ (X.T@y)\n",
        "  return b_hat\n",
        "\n",
        "jit_ordinary_least_squares = jit(ordinary_least_squares)\n",
        "\n",
        "b_hat = jit_ordinary_least_squares(X,y)\n",
        "print(b_hat)\n",
        "print(b)\n",
        "\n",
        "\n",
        "%timeit ordinary_least_squares(X,y).block_until_ready()\n",
        "%timeit jit_ordinary_least_squares(X,y).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. OLS derivation\n",
        "Assume that $y = X \\beta + \\epsilon$ where $y$ is $N \\times 1$ vector, $X$ is an $N \\times P$ matrix where $P < N$ and $\\epsilon$ is a random variable such that $\\mathbb{E}[\\epsilon_i] = 0$ and $\\mathbb{V}[\\epsilon_i] = \\sigma^2$ for all $i = 1 \\dots n$. Derive the OLS \"normal equations\"."
      ],
      "metadata": {
        "id": "pNGJ7Yij8gBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to find $\\beta$ such that the residual sum of square is minimal. The $RSS(\\beta)$ is\n",
        "\n",
        "$$RSS(\\beta)=\\sum_{i=1}^n (y_i - x_i^T\\beta)^2$$\n",
        "\n",
        "We want to find\n",
        "\n",
        "$$\\beta^*=argmin \\ RSS(\\beta)$$\n",
        "\n",
        "**Approach**: We find stationary point, i.e. the point with zero gradients. We take the derivative of $RSS(\\beta)$ with respect to $\\beta$\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial RSS(\\beta)}{\\partial \\beta} &= 2\\sum_{i=1}^n (x_i^T\\beta-y_i)x_i \\\\\n",
        "&=2\\sum_{i=1}^n (x_i^T x_i \\beta - x_i y_i) \\\\\n",
        "&=2\\sum_{i=1}^n (x_i^T \\beta x_i - x_i y_i) \\\\\n",
        "&=2\\sum_{i=1}^n (x_i x_i^T) \\beta - 2\\sum_{i=1}^n (x_i y_i) \\\\\n",
        "&=2\\sum_{i=1}^n (x_i x_i^T) \\beta - 2\\sum_{i=1}^n (x_i y_i) \\\\\n",
        "&=2[(X^T X) \\beta - (X Y)] \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "From that we have\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla RSS(\\beta) &= 0 \\iff 2[(X^T X) \\beta - (X Y)]  = 0 \\iff \\beta = (X^T X)^{-1} (X^T Y)\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "assuming $(X^T X)^{-1}$ is invertible.\n"
      ],
      "metadata": {
        "id": "C_GI3GZNlt1q"
      }
    }
  ]
}