{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhquan-truong/PM520/blob/main/HW/PM520_HW1_AnhQuanTRUONG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r94B4C5cpi8I"
      },
      "source": [
        "# Homework 1. Linear regression and normal equations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XknGDH_1Kohu"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as rdm\n",
        "import jax.numpy.linalg as jnpla\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7TX02bf92rV"
      },
      "source": [
        "# 1. Linear model simulation\n",
        "In class we defined a Python function that simulates $N$ $P\\times 1$ variables $X$ (i.e. an $N \\times P$ matrix $X$) and outcome $y$ as a linear function of $X$. Please include its definition here and use for problem 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEreYqjJNudL"
      },
      "source": [
        "Given $N \\times P$ matrix $X$, a $P \\times 1$ vector $\\beta$, and $N \\times 1$ outcome vector $y$, and a random variabl $\\epsilon$, with $\\mathbb{E}[\\epsilon]=0$ and $Var(\\epsilon) = \\sigma^2$. Then, we can describe the $y$ as a linear function of $X$ as\n",
        "\n",
        "$$\n",
        "y = X\\times \\beta + \\epsilon\\\\\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "s1CpkFZh6Y7p",
        "outputId": "27a11afc-a8b4-4535-ccaa-32570ae93832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of epsilon: 0.9396470785140991\n",
            " Var of epsilon: 158.33473205566406\n"
          ]
        }
      ],
      "source": [
        "def sim_linear_reg(key, N, P, r2=0.5):\n",
        "  key, b_key = rdm.split(key)\n",
        "  b = rdm.normal(b_key, shape=(P,))\n",
        "\n",
        "  key, X_key = rdm.split(key)\n",
        "  X = rdm.normal(X_key, shape = (N, P))\n",
        "\n",
        "  y_hat = X @ b # this is the predicted y without error eps\n",
        "  s2pred = jnp.var(y_hat)\n",
        "  s2tot = s2pred/r2 # s2tot = s2pred + s2e\n",
        "  s2e =( s2tot - s2pred)\n",
        "  # or s2e = s2pred * (1/r2-1)\n",
        "\n",
        "  key, e_key = rdm.split(key)\n",
        "  eps = rdm.normal(e_key, shape = (N,)) * jnp.sqrt(s2e)\n",
        "  y = y_hat + eps\n",
        "  return y, y_hat, b, X, eps\n",
        "\n",
        "seed = 912\n",
        "key = rdm.PRNGKey(seed) # creating key from seed\n",
        "\n",
        "N, P = 1000, 5 # a matrix 1000 x 150\n",
        "\n",
        "y, y_hat, b, X, eps = sim_linear_reg(key, N, P, r2=0.5)\n",
        "\n",
        "#Double check eps\n",
        "print(f\"Mean of epsilon: {jnp.mean(eps)}\\n Var of epsilon: {jnp.var(eps)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq2qsE137hUK"
      },
      "source": [
        "# 2. Just-in time decorator and ordinary least squares\n",
        "Complete the definition of `ordinary_least_squares` below, that estimates the effect and its standard error. `@jit` wraps a function to perform just-in-time compilation, which boosts computational performance/speed.\n",
        "\n",
        "Compare the times of with and without JIT\n",
        "Hint: use [`block_until_ready()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.block_until_ready.html) to get correct timing estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3OWo_ixyh7Y",
        "outputId": "f23f8317-fb3a-466d-a108-c645af25d724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b_hat direct and b_hat linreg the same? True\n",
            "4.32 ms ± 371 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "2.7 ms ± 256 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "\n",
        "from jax import jit\n",
        "\n",
        "def ordinary_least_squares(X, y):\n",
        "  N, P = X.shape\n",
        "  XtX = X.T @ X\n",
        "  inv_XtX = jnpla.inv(XtX) # Calculate inverse once\n",
        "  b_hat = inv_XtX @ (X.T@y)\n",
        "\n",
        "  s2_e = jnp.sum(jnp.square(y - X @ b_hat)) / (N - P)\n",
        "  # Corrected se_b_hat calculation: sqrt of diagonal of (s2_e * (X'X)^-1)\n",
        "  se_b_hat = jnp.sqrt(jnp.diag(s2_e * inv_XtX))\n",
        "  return b_hat, se_b_hat\n",
        "\n",
        "jit_ordinary_least_squares = jit(ordinary_least_squares)\n",
        "\n",
        "N, P = 1000, 150 # a matrix 1000 x 150\n",
        "y, y_hat, b, X, eps = sim_linear_reg(key, N, P, r2=0.5)\n",
        "\n",
        "b_hat_dir, se_b_hat = jit_ordinary_least_squares(X,y)\n",
        "\n",
        "\n",
        "# Double check\n",
        "model = LinearRegression(fit_intercept=False).fit(X, y)\n",
        "b_hat_linreg = model.coef_\n",
        "print(f'b_hat direct and b_hat linreg the same? {jnp.allclose(b_hat_dir,b_hat_linreg, atol=1e-5)}')\n",
        "\n",
        "%timeit ordinary_least_squares(X,y)[0].block_until_ready()\n",
        "%timeit jit_ordinary_least_squares(X,y)[0].block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNGJ7Yij8gBt"
      },
      "source": [
        "# 3. OLS derivation\n",
        "Assume that $y = X \\beta + \\epsilon$ where $y$ is $N \\times 1$ vector, $X$ is an $N \\times P$ matrix where $P < N$ and $\\epsilon$ is a random variable such that $\\mathbb{E}[\\epsilon_i] = 0$ and $\\mathbb{V}[\\epsilon_i] = \\sigma^2$ for all $i = 1 \\dots n$. Derive the OLS \"normal equations\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_GI3GZNlt1q"
      },
      "source": [
        "The goal is to find $\\beta$ such that the residual sum of square is minimal (RSS), or the sun of square of $\\epsilon$ is minimal. The $RSS(\\beta)$ is\n",
        "\n",
        "$$RSS(\\beta)=\\sum_{i=1}^n (y_i - x_i^T\\beta)^2$$\n",
        "\n",
        "We want to find\n",
        "\n",
        "$$\\beta^*=argmin \\ RSS(\\beta)$$\n",
        "\n",
        "**Approach**: We find stationary point, i.e. the point with zero gradients. We take the derivative of $RSS(\\beta)$ with respect to $\\beta$\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial RSS(\\beta)}{\\partial \\beta} &= 2\\sum_{i=1}^n (x_i^T\\beta-y_i)x_i \\\\\n",
        "&=2\\sum_{i=1}^n (x_i (x_i^T \\beta) - x_i y_i) \\\\\n",
        "&=2\\sum_{i=1}^n (x_i x_i^T) \\beta - 2\\sum_{i=1}^n (x_i y_i) \\\\\n",
        "&=2[(X^T X) \\beta - (X^T Y)] \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "From that we have\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla RSS(\\beta) &= 0 \\iff 2[(X^T X) \\hat{\\beta} - (X^T Y)]  = 0 \\iff \\hat{\\beta} = (X^T X)^{-1} (X^T Y)\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "assuming $(X^T X)$ is invertible.\n",
        "\n",
        "Now we calculate the variance of $\\hat{\\beta}$\n",
        "\n",
        "From above, we have\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\hat{\\beta} &= (X^T X)^{-1} (X^T Y) \\\\\n",
        "&= (X^T X)^{-1} X^T (X \\beta + \\epsilon)\\\\\n",
        "&= (X^T X)^{-1} X^T X \\beta + (X^T X)^{-1} X^T \\epsilon\\\\\n",
        "&= I_n \\beta + (X^T X)^{-1} X^T \\epsilon\\\\\n",
        "\\Rightarrow \\beta - \\hat{\\beta} &= (X^T X)^{-1} X^T \\epsilon\\\\\n",
        "\\Rightarrow Var(\\hat{\\beta}) &= Var[(X^T X)^{-1} X^T \\epsilon]\\\\\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Apply variances rule for a matrix-vector mulilplication $Var(Ax) = AVar(X)A^T$, we have\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "Var(\\hat{\\beta}) &= [(X^T X)^{-1} X^T]\\ Var(\\epsilon)\\ [(X^T X)^{-1} X^T]^T\\\\\n",
        "&= [(X^T X)^{-1} X^T]\\  Var(\\epsilon)\\ X [(X^T X)^{-1}]^T\\\\\n",
        "&= [(X^T X)^{-1} X^T]\\ Var(\\epsilon)\\ X [(X^T X)^T]^{-1}\\\\\\\\\n",
        "\\Rightarrow Var(\\hat{\\beta}) &= [(X^T X)^{-1} X^T]\\ Var(\\epsilon)\\ X (X^T X)^{-1}\\\\\n",
        " &= [(X^T X)^{-1} X^T]\\ σ^2\\ X (X^T X)^{-1}\\\\\n",
        " &= σ^2\\ [(X^T X)^{-1} X^T X] (X^T X)^{-1}\\\\\n",
        "&= σ^2\\ (X^T X)^{-1}\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "Assuming errors are independent and homoskedasticity"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}