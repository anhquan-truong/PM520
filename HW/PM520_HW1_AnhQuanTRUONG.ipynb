{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhquan-truong/PM520/blob/main/HW/PM520_HW1_AnhQuanTRUONG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r94B4C5cpi8I"
      },
      "source": [
        "# Homework 1. Linear regression and normal equations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XknGDH_1Kohu"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as rdm\n",
        "import jax.numpy.linalg as jnpla\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7TX02bf92rV"
      },
      "source": [
        "# 1. Linear model simulation\n",
        "In class we defined a Python function that simulates $N$ $P\\times 1$ variables $X$ (i.e. an $N \\times P$ matrix $X$) and outcome $y$ as a linear function of $X$. Please include its definition here and use for problem 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEreYqjJNudL"
      },
      "source": [
        "Given $N \\times P$ matrix $X$, a $P \\times 1$ vector $\\beta$, and $N \\times 1$ outcome vector $y$, and a random variabl $\\epsilon$, with $\\mathbb{E}[\\epsilon]=0$ and $Var(\\epsilon) = \\sigma^2$. Then, we can describe the $y$ as a linear function of $X$ as\n",
        "\n",
        "$$\n",
        "y = X\\times \\beta + \\epsilon\\\\\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "s1CpkFZh6Y7p",
        "outputId": "d89e48ef-04dd-49e5-e432-ac0dc1732d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of epsilon: -0.3946656584739685\n",
            " Var of epsilon: 124.35021209716797\n"
          ]
        }
      ],
      "source": [
        "def sim_linear_reg(key, N, P, r2=0.5):\n",
        "  key, b_key = rdm.split(key)\n",
        "  b = rdm.normal(b_key, shape=(P,))\n",
        "\n",
        "  key, X_key = rdm.split(key)\n",
        "  X = rdm.normal(X_key, shape = (N, P))\n",
        "\n",
        "  y_hat = X @ b # this is the predicted y without error eps\n",
        "  s2pred = jnp.var(y_hat)\n",
        "  s2tot = s2pred/r2 # s2tot = s2pred + s2e\n",
        "  s2e =( s2tot - s2pred)\n",
        "\n",
        "  key, e_key = rdm.split(key)\n",
        "  eps = rdm.normal(e_key, shape = (N,)) * jnp.sqrt(s2e)\n",
        "  y = y_hat + eps\n",
        "  return y, y_hat, b, X, eps\n",
        "\n",
        "seed = 91227102000\n",
        "key = rdm.PRNGKey(seed) # creating key from seed\n",
        "\n",
        "N, P = 1000, 150 # a matrix 1000 x 150\n",
        "\n",
        "y, y_hat, b, X, eps = sim_linear_reg(key, N, P, r2=0.5)\n",
        "\n",
        "#Double check eps\n",
        "print(f\"Mean of epsilon: {jnp.mean(eps)}\\n Var of epsilon: {jnp.var(eps)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq2qsE137hUK"
      },
      "source": [
        "# 2. Just-in time decorator and ordinary least squares\n",
        "Complete the definition of `ordinary_least_squares` below, that estimates the effect and its standard error. `@jit` wraps a function to perform just-in-time compilation, which boosts computational performance/speed.\n",
        "\n",
        "Compare the times of with and without JIT\n",
        "Hint: use [`block_until_ready()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.block_until_ready.html) to get correct timing estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3OWo_ixyh7Y",
        "outputId": "d22e7033-6a5f-43fe-fc2f-a76558769750"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:7: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:7: SyntaxWarning: invalid escape sequence '\\h'\n",
            "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_20948\\204879276.py:7: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.66653067  1.2053611  -0.00617148  2.3890886   0.07242878  1.4051487\n",
            " -1.3037715  -0.69175434 -0.6884035   0.88761115]\n",
            "[-1.4279191   0.85496247 -0.6770563   2.2756033  -0.29681712  0.44081363\n",
            " -1.2529446  -0.41230848 -0.6117764   1.1318142 ]\n",
            "130 μs ± 43.7 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
            "50.7 μs ± 15.4 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
            "13.3 μs ± 2.74 μs per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "\n",
        "from jax import jit\n",
        "\n",
        "\n",
        "def ordinary_least_squares(X, y):\n",
        "  \"\"\"\n",
        "  computes the OLS solution to linear system y ~ X.\n",
        "  Returns a tuple of $\\hat{beta}$ and $\\text{se}(\\hat{beta})$.\n",
        "  \"\"\"\n",
        "  b_hat = jnpla.inv(X.T @ X) @ (X.T@y)\n",
        "  return b_hat\n",
        "\n",
        "jit_ordinary_least_squares = jit(ordinary_least_squares)\n",
        "\n",
        "b_hat = jit_ordinary_least_squares(X,y)\n",
        "print(b_hat)\n",
        "print(b)\n",
        "\n",
        "\n",
        "%timeit ordinary_least_squares(X,y).block_until_ready()\n",
        "%timeit jit_ordinary_least_squares(X,y).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNGJ7Yij8gBt"
      },
      "source": [
        "# 3. OLS derivation\n",
        "Assume that $y = X \\beta + \\epsilon$ where $y$ is $N \\times 1$ vector, $X$ is an $N \\times P$ matrix where $P < N$ and $\\epsilon$ is a random variable such that $\\mathbb{E}[\\epsilon_i] = 0$ and $\\mathbb{V}[\\epsilon_i] = \\sigma^2$ for all $i = 1 \\dots n$. Derive the OLS \"normal equations\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_GI3GZNlt1q"
      },
      "source": [
        "The goal is to find $\\beta$ such that the residual sum of square is minimal (RSS), or the sun of square of $\\epsilon$ is minimal. The $RSS(\\beta)$ is\n",
        "\n",
        "$$RSS(\\beta)=\\sum_{i=1}^n (y_i - x_i^T\\beta)^2$$\n",
        "\n",
        "We want to find\n",
        "\n",
        "$$\\beta^*=argmin \\ RSS(\\beta)$$\n",
        "\n",
        "**Approach**: We find stationary point, i.e. the point with zero gradients. We take the derivative of $RSS(\\beta)$ with respect to $\\beta$\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial RSS(\\beta)}{\\partial \\beta} &= 2\\sum_{i=1}^n (x_i^T\\beta-y_i)x_i \\\\\n",
        "&=2\\sum_{i=1}^n (x_i^T x_i \\beta - x_i y_i) \\\\\n",
        "&=2\\sum_{i=1}^n (x_i^T \\beta x_i - x_i y_i) \\\\\n",
        "&=2\\sum_{i=1}^n (x_i x_i^T) \\beta - 2\\sum_{i=1}^n (x_i y_i) \\\\\n",
        "&=2\\sum_{i=1}^n (x_i x_i^T) \\beta - 2\\sum_{i=1}^n (x_i y_i) \\\\\n",
        "&=2[(X^T X) \\beta - (X^T Y)] \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "From that we have\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla RSS(\\beta) &= 0 \\iff 2[(X^T X) \\beta - (X^T Y)]  = 0 \\iff \\beta = (X^T X)^{-1} (X^T Y)\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "assuming $(X^T X)$ is invertible.\n",
        "\n",
        "Now we calculate the standard error of $\\beta$\n",
        "\n",
        "From above, we have\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\hat{\\beta} &= (X^T X)^{-1} (X^T Y) \\\\\n",
        "&= (X^T X)^{-1} X^T (X \\beta + \\epsilon)\\\\\n",
        "&= (X^T X)^{-1} X^T X \\beta + (X^T X)^{-1} X^T \\epsilon\\\\\n",
        "&= In \\beta + (X^T X)^{-1} X^T \\epsilon\\\\\n",
        "\\implies \\beta - \\hat{\\beta} &= (X^T X)^{-1} X^T \\epsilon\\\\\n",
        "\\implies Var(\\hat{\\beta}) &= Var[(X^T X)^{-1} X^T \\epsilon]\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Apply variances rule for a matrix-vector mulilplication $Var(Ax) = AVar(X)A^T$, we have\n",
        "$$\n",
        "\\begin{align*}\n",
        "Var(\\hat{\\beta}) &= [(X^T X)^{-1} X^T] Var(\\epsilon) [(X^T X)^{-1} X^T]^T\\\\\n",
        "&= [(X^T X)^{-1} X^T] Var(\\epsilon) X [(X^T X)^{-1}]^T\\\\\n",
        "\\end{align*}\n",
        "$$\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}