{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhquan-truong/PM520/blob/main/Lab_5_ExpFam_AnhQuanTRUONG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# It's a Family Affair, or: Exponential Families\n",
        "\n",
        "[Exponential Families](https://en.wikipedia.org/wiki/Exponential_family) (sometimes abbreviated as ExpFam) provide a [succinct characterization](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions) of many distributions (e.g., [Normal](https://en.wikipedia.org/wiki/Normal_distribution), [Gamma](https://en.wikipedia.org/wiki/Gamma_distribution), [Poisson](https://en.wikipedia.org/wiki/Poisson_distribution), [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution), [Wishart](https://en.wikipedia.org/wiki/Wishart_distribution), etc.). We'll take an informal look at their properties and how to perform inference."
      ],
      "metadata": {
        "id": "4R7VI43gT937"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exponential Families\n",
        "Let $\\eta = [\\eta_1, \\dotsc, \\eta_k]$ be a $k$-vector of parameters, and $x$ be an observation such that $x \\sim f(\\eta)$. We can define its [PDF](https://en.wikipedia.org/wiki/Probability_density_function) (or [PMF](https://en.wikipedia.org/wiki/Probability_mass_function) in case of discrete $x$) as $$f(x | \\eta) = h(x)\\exp(\\eta \\cdot T(x) - A(\\eta)),$$ where $h(x)$ is a *base measure*, $\\eta$ are the *natural parameters*, $T(x)$ are the [*sufficient statistics*](https://en.wikipedia.org/wiki/Sufficient_statistic), and $A(\\eta)$ is the [*log-partition function*](https://en.wikipedia.org/wiki/Partition_function_%28mathematics%29). If $\\eta$ is *finite*, and the [*support*](https://en.wikipedia.org/wiki/Support_%28mathematics%29%23In_probability_and_measure_theory) of $f$ does not depend on the value of $\\eta$, then $f$ can be said to be a member of the [Exponential Families](https://en.wikipedia.org/wiki/Exponential_family).\n",
        "\n",
        "### Example: Normal Distribution\n",
        "Recall if $x \\sim N(\\mu, \\sigma^2)$, then the PDF of $x$ is given by,\n",
        "$$f(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right).$$ To see that the two-parameter Normal distribution is a member of the Exponential Families, define $\\eta = [\\frac{\\mu}{\\sigma^2}, -\\frac{1}{2\\sigma^2}]$, $h(x) = \\frac{1}{\\sqrt{2\\pi}}$, $T(x) = [x, x^2]^T$, and $A(\\eta) = \\frac{\\mu^2}{2\\sigma^2} + \\log |\\sigma| = -\\frac{\\eta_1^2}{4\\eta_2} + \\frac{1}{2}\\log|\\frac{1}{2\\eta_2}|$. Placing this all together we have,\n",
        "$$\\begin{align*}\n",
        "f(x | \\eta) &= h(x)\\exp(\\eta \\cdot T(x) - A(\\eta)) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp(\\eta \\cdot T(x) - A(\\eta)) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left([\\frac{\\mu}{\\sigma^2}, -\\frac{1}{2\\sigma^2}] \\cdot T(x) - A(\\eta)\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left([\\frac{\\mu}{\\sigma^2}, -\\frac{1}{2\\sigma^2}] \\cdot [x, x^2]^T - A(\\eta)\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(\\frac{\\mu x}{\\sigma^2} -\\frac{x^2}{2\\sigma^2} - A(\\eta)\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(\\frac{\\mu x}{\\sigma^2} -\\frac{x^2}{2\\sigma^2} + \\frac{\\eta_1^2}{4\\eta_2} - \\frac{1}{2}\\log\\left|\\frac{1}{2\\eta_2}\\right|\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(\\frac{\\mu x}{\\sigma^2} -\\frac{x^2}{2\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2} - \\log |\\sigma|\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2} - \\log |\\sigma|\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\\\\n",
        "  &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\\\\n",
        "\\end{align*}$$\n",
        "\n",
        "## Expectations\n",
        "A key property of ExpFam distributions is that one can define the [moments](https://en.wikipedia.org/wiki/Moment_%28mathematics%29) of $T(x)$ from the log-partition function $A(\\eta)$. Two key moments to remember are, $\\mathbb{E}[T(x)] = \\frac{\\partial}{\\partial \\eta} A(\\eta)$ and $\\mathbb{V}[T(x)] = \\frac{\\partial^2}{\\partial \\eta \\partial \\eta^T} A(\\eta)$.\n",
        "\n",
        "In the case of scalar Normal variables, we have $\\mathbb{E}[T(x)] = \\mathbb{E}\\left[[x, x^2]^T\\right] = [\\mu, \\sigma^2 + \\mu^2]^T$ and $\\mathbb{V}[T(x)] = \\begin{bmatrix} \\sigma^2 & 0 \\\\ 0 & 2\\sigma^4 \\end{bmatrix}$.\n",
        "$$\\begin{align*}\n",
        "\\mathbb{E}[T(x)] &= \\frac{\\partial}{\\partial \\eta} A(\\eta) \\\\\n",
        "  &= \\frac{\\partial}{\\partial \\eta} \\left[-\\frac{\\eta_1^2}{4\\eta_2} + \\frac{1}{2}\\log|\\frac{1}{2\\eta_2}|\\right] \\\\\n",
        "  &= -\\frac{\\partial}{\\partial \\eta} \\frac{\\eta_1^2}{4\\eta_2} + \\frac{\\partial}{\\partial \\eta}\\frac{1}{2}\\log|\\frac{1}{2\\eta_2}| \\\\\n",
        "  &= -\\frac{\\partial}{\\partial \\eta} \\frac{\\eta_1^2}{4\\eta_2} - \\frac{\\partial}{\\partial \\eta}\\frac{1}{2}\\log|2\\eta_2| \\\\\n",
        "\\mathbb{E}[T(x)]_1 &= -\\frac{\\partial}{\\partial \\eta_1} \\frac{\\eta_1^2}{4\\eta_2} - \\frac{\\partial}{\\partial \\eta_1}\\frac{1}{2}\\log|2\\eta_2| \\\\\n",
        "  &= - \\frac{2 \\eta_1}{4 \\eta_2} = -\\frac{\\eta_1}{2 \\eta_2} = -\\frac{\\mu / \\sigma^2}{-2 / (2\\sigma^2)} = \\mu \\\\\n",
        "\\mathbb{E}[T(x)]_2 &= -\\frac{\\partial}{\\partial \\eta_2} \\frac{\\eta_1^2}{4\\eta_2} - \\frac{\\partial}{\\partial \\eta_2}\\frac{1}{2}\\log|2\\eta_2| \\\\\n",
        "  &= \\frac{\\eta^2_1}{4\\eta_2^2} - \\frac{1}{2\\eta_2} = \\frac{\\mu^2/\\sigma^4}{1/\\sigma^4} + \\frac{1}{1 / \\sigma^2} = \\mu^2 + \\sigma^2.\n",
        "\\end{align*}$$\n",
        "\n",
        "## MLE Inference for ExpFam\n",
        "Given $x_1, \\dotsc, x_n$ we assume that $x_i \\sim f(\\eta)$ independently and identically distributed under some ExpFam dist $f$ with natural parameters $\\eta$. Our log-likelihood is then given by,\n",
        "$$ \\ell(\\eta) = \\sum_{i=1}^n \\log f(x_i | \\eta).$$ To identify the\n",
        "maximum likelihood estimates we first compute the gradient of $\\ell$ wrt $\\eta$, which is given by\n",
        "$$\\begin{align*}\n",
        "\\nabla \\ell(\\eta) &= \\sum_{i=1}^n \\nabla \\log f(x_i | \\eta) \\\\\n",
        "  &= \\sum_{i=1}^n \\nabla \\log \\left[ h(x_i)\\exp(\\eta \\cdot T(x_i) - A(\\eta))\\right] \\\\\n",
        "  &= \\sum_{i=1}^n \\nabla \\left[\\log h(x_i) + \\log \\exp(\\eta \\cdot T(x_i) - A(\\eta))\\right] \\\\\n",
        "  &= \\sum_{i=1}^n \\nabla \\log h(x_i) + \\nabla \\log \\exp(\\eta \\cdot T(x_i) - A(\\eta)) \\\\\n",
        "  &= \\sum_{i=1}^n \\nabla [\\eta \\cdot T(x_i)] - \\nabla A(\\eta) \\\\\n",
        "  &= \\sum_{i=1}^n T(x_i) - \\mathbb{E}[T(x_i)],\n",
        "\\end{align*}$$\n",
        "where we used the fact that  $\\nabla A(\\eta) = \\mathbb{E}[T(x)]$. Setting this to zero and solving implies, we would like to find values $\\eta$ such that $\\sum_i T(x_i) = \\sum_i \\mathbb{E}[T(x_i)] = n \\mathbb{E}[T(X)] ⇒ \\frac{1}{n} \\sum_i T(x_i) = \\mathbb{E}[T(X)]$! In other words, MLE under ExpFam seeks values of $\\eta$ that *match the empirical mean of the observed sufficient statistics to their expectation*!"
      ],
      "metadata": {
        "id": "tz3bA7fUcApH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## It's an Equinox for all seasons\n",
        "TBD: Information on `equinox`, (functional) objective oriented programming, abstract base classes. More documentation on `Normal` class."
      ],
      "metadata": {
        "id": "4cm1S74tRxqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install equinox"
      ],
      "metadata": {
        "id": "ykn-OzCv3xjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c685a04-967b-43df-fdb3-0f056e20cdff",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting equinox\n",
            "  Downloading equinox-0.13.4-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: jax!=0.7.0,!=0.7.1,>=0.4.38 in /usr/local/lib/python3.12/dist-packages (from equinox) (0.7.2)\n",
            "Collecting jaxtyping>=0.2.20 (from equinox)\n",
            "  Downloading jaxtyping-0.3.7-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from equinox) (4.15.0)\n",
            "Collecting wadler-lindig>=0.1.0 (from equinox)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jaxlib<=0.7.2,>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from jax!=0.7.0,!=0.7.1,>=0.4.38->equinox) (0.7.2)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax!=0.7.0,!=0.7.1,>=0.4.38->equinox) (0.5.4)\n",
            "Requirement already satisfied: numpy>=2.0 in /usr/local/lib/python3.12/dist-packages (from jax!=0.7.0,!=0.7.1,>=0.4.38->equinox) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax!=0.7.0,!=0.7.1,>=0.4.38->equinox) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax!=0.7.0,!=0.7.1,>=0.4.38->equinox) (1.16.3)\n",
            "Downloading equinox-0.13.4-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, jaxtyping, equinox\n",
            "Successfully installed equinox-0.13.4 jaxtyping-0.3.7 wadler-lindig-0.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "\n",
        "import equinox as eqx\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jaxtyping import Array, ArrayLike\n",
        "\n",
        "# Class in python, encapsulate some states, expose some functionality that can\n",
        "# operate on the states\n",
        "# A class can inherit from another class (parent class)\n",
        "# Like a blue print\n",
        "\n",
        "\n",
        "class AbstractExpFam(eqx.Module):\n",
        "  \"\"\"\n",
        "  Simple base class for Exponential Families. Will provide means to compute\n",
        "  sufficient statistics and evaluat the loglikelihood of downstream\n",
        "  implementations.\n",
        "  \"\"\"\n",
        "\n",
        "# Here you can see that base_measure is indented, meaning it is an method of\n",
        "# the class\n",
        "\n",
        "# A method is a function attached to a instance of the class\n",
        "\n",
        "  @abstractmethod # this is abstract, no implementation\n",
        "  def base_measure(self, x: ArrayLike) -> Array:\n",
        "    \"\"\"\n",
        "    Computes the base measure for an implementation of ExpFam.\n",
        "\n",
        "    x: ArrayLike, the observations.\n",
        "\n",
        "    Returns:\n",
        "      The base measure for an implementation of ExpFam.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  @abstractmethod\n",
        "  def sufficient_statistics(self, x: ArrayLike) -> Array:\n",
        "    \"\"\"\n",
        "    Computes the sufficient statistics (i.e. $T(x)$) for an implementation\n",
        "    of ExpFam.\n",
        "\n",
        "    x: ArrayLike, the observations.\n",
        "\n",
        "    Returns:\n",
        "      The sufficient statistics for each observation under an implementation\n",
        "      of ExpFam.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  @abstractmethod\n",
        "  def log_partition(self, eta: ArrayLike) -> Array:\n",
        "    \"\"\"\n",
        "    Computes the log partition function (i.e. $A(\\eta)$) for an implementation\n",
        "    of ExpFam with natural parameters $\\eta$.\n",
        "\n",
        "    eta: ArrayLike, the natural parameters to evaluate under the log partition.\n",
        "\n",
        "    Returns:\n",
        "      The value of the log partition function for each observation under an\n",
        "      implementation of ExpFam.\n",
        "    \"\"\"\n",
        "    ...\n",
        "  # Here we have implementation\n",
        "  def loglikelihood(self, eta: ArrayLike, x: ArrayLike) -> Array:\n",
        "    \"\"\"\n",
        "    Computes the log likelihood for each observation $x$ under an implementation\n",
        "    of ExpFam with natural parameters $\\eta$.\n",
        "\n",
        "    eta: ArrayLike, the natural parameters to evaluate under the log likelihood.\n",
        "    x: ArrayLike, the observations.\n",
        "\n",
        "    Returns:\n",
        "      The log likelihood for each observation under an implementation of ExpFam.\n",
        "    \"\"\"\n",
        "    t_x = self.sufficient_statistics(x)\n",
        "    log_h_x = jnp.log(self.base_measure(x))\n",
        "    log_eta = self.log_partition(eta)\n",
        "    if eta.ndim == 0:\n",
        "      inner = t_x * eta\n",
        "    else:\n",
        "      inner = t_x @ eta\n",
        "\n",
        "    return inner - log_eta + log_h_x\n",
        "  # special method __call__: you pass eta and x, and you return llh\n",
        "  def __call__(self, eta: ArrayLike, x: ArrayLike) -> Array:\n",
        "    return self.loglikelihood(eta, x)\n",
        "\n",
        "\n",
        "  def fit_mle(self, x: ArrayLike) -> Array:\n",
        "    \"\"\"\n",
        "    Maximizes the log likelihood for each observation under an implementation\n",
        "    of ExpFam with natural parameters.\n",
        "\n",
        "    x: ArrayLike, the observations.\n",
        "\n",
        "    Returns:\n",
        "      The moment parameters that maximize log likelihood.\n",
        "    \"\"\"\n",
        "    t_x = self.sufficient_statistics(x)\n",
        "    return jnp.mean(t_x, axis=0) # returns two values across rows\n",
        "\n",
        "\n",
        "class Normal(AbstractExpFam):\n",
        "\n",
        "  def base_measure(self, x: ArrayLike) -> Array:\n",
        "    return 1. / jnp.sqrt(2 * jnp.pi)\n",
        "\n",
        "  def sufficient_statistics(self, x: ArrayLike) -> Array:\n",
        "    x_sq = x ** 2\n",
        "    return jnp.concatenate((x[:, jnp.newaxis], x_sq[:, jnp.newaxis]), axis=1)\n",
        "\n",
        "  def log_partition(self, eta: ArrayLike) -> Array:\n",
        "    eta_1, eta_2 = eta\n",
        "    term1 = -(eta_1**2 / (4 * eta_2))\n",
        "    term2 = -0.5 * jnp.log(- 2 * eta_2)\n",
        "    return term1 + term2"
      ],
      "metadata": {
        "id": "ppu0j4ydwTnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "08755d1b-ade3-4c07-b8b3-9000d7d2a02a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:55: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:69: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:55: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:69: SyntaxWarning: invalid escape sequence '\\e'\n",
            "/tmp/ipython-input-2412552810.py:55: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  Computes the log partition function (i.e. $A(\\eta)$) for an implementation\n",
            "/tmp/ipython-input-2412552810.py:69: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  of ExpFam with natural parameters $\\eta$.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Having implemented our objects to perform MLE in a general setting, with a specific example of normal distributions, let's perform some sanity checks before proceeding with inference."
      ],
      "metadata": {
        "id": "If1EgDTJSFjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.random as rdm\n",
        "\n",
        "# initialize our PRNG state\n",
        "seed = 0\n",
        "key = rdm.PRNGKey(seed)\n",
        "\n",
        "# define some parameter values for N(mu, sigma2)\n",
        "mu = 5.0\n",
        "sigma_sq = 10.0\n",
        "\n",
        "# generate random data\n",
        "N = 100\n",
        "key, x_key = rdm.split(key)\n",
        "obs = mu + jnp.sqrt(sigma_sq) * rdm.normal(x_key, shape=(N,))\n",
        "\n",
        "# transform parameters to natural parameters and create some other guess at eta\n",
        "eta_true = jnp.array([mu / sigma_sq, - 1. / (2 * sigma_sq)])\n",
        "eta_guess = jnp.array([-9, -100.])\n",
        "\n",
        "# create an instance of Normal distribution using our implementation above\n",
        "model = Normal() # this is instance of the class, and we have __call__ to make\n",
        "# it like a function\n",
        "\n",
        "# calculate the sum of log likelihood\n",
        "# equivalent to `jnp.sum(model.loglikelihood(eta_true))`\n",
        "sum_ll_true = jnp.sum(model(eta_true, obs))\n",
        "sum_ll_guess = jnp.sum(model(eta_guess, obs))\n",
        "print(f\"true logl $\\ell$({eta_true}) = {sum_ll_true} | guess logl $\\ell$({eta_guess}) = {sum_ll_guess}\")\n",
        "\n",
        "# sanity check against jax loglikelihood\n",
        "import jax.scipy.stats as stats\n",
        "jax_ll_true = jnp.sum(stats.norm.logpdf(obs, mu, jnp.sqrt(sigma_sq)))\n",
        "print(f\"Our $\\ell$({eta_true}) = {sum_ll_true} | JAX logl $\\ell$({eta_true}) = {jax_ll_true}\")\n",
        "\n",
        "fitted = model.fit_mle(obs)\n",
        "print(f\"Our MLE moment-based parameters are {fitted}\")\n",
        "fitted_con = jnp.array([fitted[0], fitted[1] - fitted[0]**2])\n",
        "print(f\"Our MLE canonical parameters are {fitted_con}\")\n",
        "print(f\"Our true canonical parameters are {jnp.array([mu, sigma_sq])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOmELPydTaY-",
        "outputId": "bccdfdbd-eb23-46f1-8bff-1b07017690da"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:28: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:28: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:33: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:33: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:28: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:28: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:33: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:33: SyntaxWarning: invalid escape sequence '\\e'\n",
            "/tmp/ipython-input-3865825455.py:28: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  print(f\"true logl $\\ell$({eta_true}) = {sum_ll_true} | guess logl $\\ell$({eta_guess}) = {sum_ll_guess}\")\n",
            "/tmp/ipython-input-3865825455.py:28: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  print(f\"true logl $\\ell$({eta_true}) = {sum_ll_true} | guess logl $\\ell$({eta_guess}) = {sum_ll_guess}\")\n",
            "/tmp/ipython-input-3865825455.py:33: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  print(f\"Our $\\ell$({eta_true}) = {sum_ll_true} | JAX logl $\\ell$({eta_true}) = {jax_ll_true}\")\n",
            "/tmp/ipython-input-3865825455.py:33: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  print(f\"Our $\\ell$({eta_true}) = {sum_ll_true} | JAX logl $\\ell$({eta_true}) = {jax_ll_true}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true logl $\\ell$([ 0.5  -0.05]) = -261.07476806640625 | guess logl $\\ell$([  -9. -100.]) = -295570.1875\n",
            "Our $\\ell$([ 0.5  -0.05]) = -261.07476806640625 | JAX logl $\\ell$([ 0.5  -0.05]) = -261.07476806640625\n",
            "Our MLE moment-based parameters are [ 4.3371615 29.181948 ]\n",
            "Our MLE canonical parameters are [ 4.3371615 10.370977 ]\n",
            "Our true canonical parameters are [ 5. 10.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possion distribution"
      ],
      "metadata": {
        "id": "lzqKyIZ2czpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.scipy.special import gamma\n",
        "\n",
        "class Poisson(AbstractExpFam):\n",
        "\n",
        "  def base_measure(self, x: ArrayLike) -> Array:\n",
        "    return 1. /gamma(x + 1.)\n",
        "\n",
        "  def sufficient_statistics(self, x: ArrayLike) -> Array:\n",
        "    return x\n",
        "\n",
        "  def log_partition(self, eta: ArrayLike) -> Array:\n",
        "    return jnp.exp(eta)"
      ],
      "metadata": {
        "id": "k4gT7TI-zKF4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize our PRNG state\n",
        "seed = 0\n",
        "key = rdm.PRNGKey(seed)\n",
        "\n",
        "# define some parameter values for N(mu, sigma2)\n",
        "rate = 5.0\n",
        "\n",
        "# generate random data\n",
        "N = 1000\n",
        "key, x_key = rdm.split(key)\n",
        "obs = rdm.poisson(x_key, rate, shape=(N,))\n",
        "\n",
        "# transform parameters to natural parameters and create some other guess at eta\n",
        "eta_true = jnp.log(rate)\n",
        "eta_guess = jnp.log(100.)\n",
        "\n",
        "# create an instance of Poisson distribution using our implementation above\n",
        "model = Poisson()\n",
        "\n",
        "# calculate the sum of log likelihood\n",
        "# equivalent to `jnp.sum(model.loglikelihood(eta_true))`\n",
        "sum_ll_true = jnp.sum(model(eta_true, obs))\n",
        "sum_ll_guess = jnp.sum(model(eta_guess, obs))\n",
        "print(f\"true logl({eta_true}) = {sum_ll_true} | guess logl({eta_guess}) = {sum_ll_guess}\")\n",
        "\n",
        "# sanity check against jax loglikelihood\n",
        "import jax.scipy.stats as stats\n",
        "jax_ll_true = jnp.sum(stats.poisson.logpmf(obs, rate))\n",
        "print(f\"Our logl({eta_true}) = {sum_ll_true} | JAX logl({eta_true}) = {jax_ll_true}\")\n",
        "\n",
        "fitted = model.fit_mle(obs)\n",
        "print(f\"Our MLE moment-based parameters are {fitted}\")\n",
        "print(f\"Our true canonical parameters are {rate}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Unr2ullt3pbe",
        "outputId": "9080bf83-c3d4-4c8f-93c9-05ebd48cbcdb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true logl(1.6094379425048828) = -2215.308349609375 | guess logl(4.605170249938965) = -82173.734375\n",
            "Our logl(1.6094379425048828) = -2215.308349609375 | JAX logl(1.6094379425048828) = -2215.307861328125\n",
            "Our MLE moment-based parameters are 5.021000385284424\n",
            "Our true canonical parameters are 5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gamma Distribution"
      ],
      "metadata": {
        "id": "CWL8OHeEcwYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.scipy.special import gamma, gammaln\n",
        "\n",
        "class Gamma(AbstractExpFam):\n",
        "\n",
        "  def base_measure(self, x: ArrayLike) -> Array:\n",
        "    # For the exponential family form f(x; alpha, beta) = (beta^alpha / Gamma(alpha)) * x^(alpha-1) * exp(-beta*x),\n",
        "    # if T(x) = [log(x), x], then eta = [alpha-1, -beta].\n",
        "    # The base measure h(x) is typically 1 in this formulation, as x^(alpha-1) is part of eta * T(x).\n",
        "    return jnp.ones_like(x)\n",
        "\n",
        "  def sufficient_statistics(self, x: ArrayLike) -> Array:\n",
        "    # T(x) for Gamma is [log(x), x]\n",
        "    return jnp.concatenate((jnp.log(x)[:, jnp.newaxis], x[:, jnp.newaxis]), axis=1)\n",
        "\n",
        "  def log_partition(self, eta: ArrayLike) -> Array:\n",
        "    # For eta = [alpha-1, -beta]\n",
        "    # alpha = eta_1 + 1\n",
        "    # beta = -eta_2\n",
        "    eta_1, eta_2 = eta\n",
        "    alpha = eta_1 + 1\n",
        "    beta = -eta_2\n",
        "    # A(eta) = log(Gamma(alpha)) - alpha * log(beta)\n",
        "    # Using gammaln for log(Gamma(alpha)) for numerical stability\n",
        "    return gammaln(alpha) - alpha * jnp.log(beta)"
      ],
      "metadata": {
        "id": "31bHR07Fc83H"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize our PRNG state\n",
        "seed = 0\n",
        "key = rdm.PRNGKey(seed)\n",
        "\n",
        "# define some parameter values for N(mu, sigma2)\n",
        "shape = 5.0\n",
        "scale = 8\n",
        "\n",
        "# generate random data\n",
        "N = 100 # Increased N for better estimation\n",
        "key, x_key = rdm.split(key)\n",
        "obs = scale * rdm.gamma(x_key, shape, shape=(N,))\n",
        "\n",
        "# transform parameters to natural parameters and create some other guess at eta\n",
        "eta_true = jnp.array([shape - 1., -1. / scale])\n",
        "eta_guess = jnp.array([100.,-50.])\n",
        "\n",
        "# create an instance of Poisson distribution using our implementation above\n",
        "model = Gamma()\n",
        "\n",
        "# calculate the sum of log likelihood\n",
        "# equivalent to `jnp.sum(model.loglikelihood(eta_true))`\n",
        "sum_ll_true = jnp.sum(model(eta_true, obs))\n",
        "sum_ll_guess = jnp.sum(model(eta_guess, obs))\n",
        "print(f\"true logl({eta_true}) = {sum_ll_true} | guess logl({eta_guess}) = {sum_ll_guess}\")\n",
        "\n",
        "# sanity check against jax loglikelihood\n",
        "import jax.scipy.stats as stats\n",
        "jax_ll_true = jnp.sum(stats.gamma.logpdf(obs, shape, scale=scale))\n",
        "print(f\"Our logl({eta_true}) = {sum_ll_true} | JAX logl({eta_true}) = {jax_ll_true}\")\n",
        "\n",
        "fitted = model.fit_mle(obs)\n",
        "print(f\"Our MLE moment-based parameters are {fitted}\")\n",
        "print(f\"Our true canonical parameters are  {jnp.array([shape - 1., -1. / scale])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR6G8zZudGi4",
        "outputId": "247d9368-470a-4098-d99b-67e9ab05feb4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true logl([ 4.    -0.125]) = -433.49774169921875 | guess logl([100. -50.]) = -162773.03125\n",
            "Our logl([ 4.    -0.125]) = -433.49774169921875 | JAX logl([ 4.    -0.125]) = -433.497802734375\n",
            "Our MLE moment-based parameters are [ 3.5701451 40.322403 ]\n",
            "Our true canonical parameters are  [ 4.    -0.125]\n"
          ]
        }
      ]
    }
  ]
}